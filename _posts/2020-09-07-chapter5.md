---
title: تنظيف البيانات
show_title: true
chapter_number: 5
chapter_text: الفصل الخامس
chapter_lessons: [[0, 'مقدمة'], [1, 'نظره على بيانات شرطة مدينة بيركلي'],[2, 'تنظيف بيانات المكالمات'], [3, 'تنظيف بيانات الإيقافات'], [4, 'شكل وتجميع البيانات'], [5, 'تقسيم البيانات'], [6, 'مدى البيانات'], [7, 'زمانية البيانات'], [8 , 'مدى ثقتنا بالبيانات']]
chapter_sublessons: [
    [],
    [],
    ['فهم توليد البيانات', 'تنظيف البيانات', 'هل توجد بيانات مفقوده؟', 'هل توجد أي بيانات مفقوده تم تعبئتها؟', 'اي جزء من البيانات ادخلت بواسطة اشخاص حقيقين؟', 'لمسات اخيره'],
    ['هل توجد بيانات مفقوده ؟', 'هل توجد أي بيانات مفقوده تم تعبئتها ؟', 'اي جزء من البيانات ادخلت بواسطة اشخاص حقيقين ؟'],
    ['شكل البيانات', 'التجميع', 'قائمة المراجعه لشكل البيانات'],
    ['تقسيم البيانات', 'قائمة المراجعه لتقسيم البيانات'],
    [],
    [],
    []
]
layout: default
---

## مقدمة

تأتي البيانات بعدة اشكال وتتنوع من حيث فائدتها في التحليل. رغم اننا نسعى ان تكون جميع البيانات على شكل جدول وكل قيمه ادخل بشكل مستمر ودقيق، في الحقيقه يجب علينا التحقق بشكل دقيقه عن المشاكل التي قد تقودنا لنتائج غير صحيحه.

المصطلح "تنظيف البيانات" يطلق على خطوة التحقق من البيانات واتخاذ قرارات عن كيفية إصلاح الاخطاء والبيانات المفقوده. سنناقش اكثر المشاكل الشائعه في البيانات ونشرحها بشكل اوضح.

تنظيف البيانات له بعض القيود. مثلاً، لا يوجد تنظيف بأمكانه تحسين عينات تم اخذها بشكل متحيز. قبل البدء بمشوار تنظيف البيانات الذي عاده ما يكون طويلاً، يجب ان نتأكد ان بياناتنا تم جمعها بشكل دقيق وبدون تحيز. في ذلك الحين يمكننا بدأ التحقق من البيانات وتنظيفها للتخلص من المشاكل في انواع البيانات او طرق الادخال.

سنقوم بشرح طٌرق لتنظيف البيانات وسنستخدم بيانات شرطة مدينة بيركلي.


## نظره على بيانات شرطة مدينة بيركلي

سنستخدم بيانات شرطة مدينة بيركلي المنشوره للعامه لشرح طرق تنظيف البيانات. يمكن تحميل بيانات المكالمات من [هنا](https://data.cityofberkeley.info/Public-Safety/Berkeley-PD-Calls-for-Service/k2nh-s5h5)، وبيانات الإيقافات من [هنا](https://data.cityofberkeley.info/Public-Safety/Berkeley-PD-Stop-Data/6e9j-pj9p).

> روابط اخرى لتحميل البيانات:
> - [بيانات المكالمات]({{ site.baseurl }}/files/chapter5/Berkeley_PD_-_Calls_for_Service.csv).
> - [بيانات الإيقافات]({{ site.baseurl }}/files/chapter5/stops.json).

سنستخدم الامر `ls` مع `-lh` لعرض معلومات أكثر عن الملفات:

```python
!ls -lh data/
```

```ruby
total 13936
-rw-r--r--@ 1 sam  staff   979K Aug 29 14:41 Berkeley_PD_-_Calls_for_Service.csv
-rw-r--r--@ 1 sam  staff    81B Aug 29 14:28 cvdow.csv
-rw-r--r--@ 1 sam  staff   5.8M Aug 29 14:41 stops.json
```

يظهر لنا الأمر السابق الملفات في المجلد واحجامها. الأمر مهم لأنه يبين لنا أن الملفات صغيرة الحجم ويمكن تحميلها على ذاكرة حاسبنا. قاعده عامه، يمكن تحميل الملفات بشكل آمن عندما يكون حجمها حوالي ربع حجم الذاكرة في الحاسب. مثلاً، اذا كان جهازنا يحتوي على 4 ج.ب. من الذاكرة العشوائيه، يمكننا تحميل ملف CSV بحجم 1 ج.ب في بانداز. ليتحمل حاسبنا ملفات بحجم أكبر يجب علينا استخدام ادوات خاصه وسنتحدث لاحقاً عنها في هذا الكتاب.

لاحظ استخدامنا لعلامة التعجب قبل `ls`. هذا يخبر جوبتر ان الكود البرمجي التالي خاص بمترجم الشيل، وليس بايثون. مثال آخر:

```python
# الامر `wc` يظهر عدد الاسطر في كل ملف.
# يمكن ان نلاحظ ان ملف `stops.json` يحتوي على عدد (29852) سطر.
!wc -l data/*
```

```ruby
   16497 data/Berkeley_PD_-_Calls_for_Service.csv
       8 data/cvdow.csv
   29852 data/stops.json
   46357 total
```

## تنظيف بيانات المكالمات

### فهم توليد البيانات

سنقوم بذكر بعض الأسئله التي يجب أن تتحقق منها في جميع بياناتك قبل البدء بتنظيفها ومعالجتها. هذه الأسئله عن كيف تم توليد وإنشاء هذه البيانات، في هذه الخطوه تنظيف البيانات **لن** يساعدنا على حل المشاكل التي حصلت اثناء إنشاء البيانات.

**على ماذا تحتوي البيانات؟** الموقع الذي تم اخذ بيانات المكالمات منه يذكر ان "الجرائم/الحوادث (وليس تقارير الجرائم) التي حدثت في ال180 يوم السابقه". مزيد من القراءه في الموقع اوضحت التالي "ليس جميع المكالمات التي طلبت خدمات الشرطه تم اضافتها (مثل عضة حيوان)".

موقع بيانات الإيقافات يوضح ان البيانات تحتوي على جميع "السيارات المحتجزه (بما فيها الدراجات الهوائية) وإعتقالات المشاه (حتى خمس اشخاص)" منذ 26 يناير 2015.

**هل البيانات تعداد / احصاء للسكان؟** هذا يعتمد على عدد مجتمعنا الإحصائي. مثلاً، اذا كنا مهتمين بمكالمات طلب الخدمات لآخر 180 يوم للحوادث والجرائم، فأن بيانات المكالمات تعتبر كذلك. ولكن اذا اردنا المكالمات لطلب خدمات الشرطة للعشر سنوات السابقة، فأن البيانات ليست تعداد مناسب. يمكننا قول نفس الكلام على بيانات الإيقافات لما ان البيانات تم جمعها إبتداءًا من 26 يناير 2015.

**اذا كانت البيانات تُشكل عينه، هل هي عينة الإحتمالات؟** البيانات لا تمثل عينة الإحتمالات لأنها لا تظهر اي عشوائيه في طريقة جمع البيانات. لدينا بيانات كامله لفتره معينه فقط وليس لفترات أخرى.

**هل هناك قيود ستجبرنا عليه البيانات في نتائجنا؟** رغم اننا سنسأل هذا السؤال بعد كل خطوه من خطواتنا، يمكننا ملاحظة ان بياناتنا تظهر لنا بعض القيود. أهم القيود التي تظهرها لنا هي اننا لا يمكننا القيام بأي تقديرات غير متحيزه للفترات التي لم يتم تسجيلها في البيانات.

### تنظيف البيانات

لنبدأ الآن بتنظيف بيانات المكالمات. الأمر `head` يظهر لنا أول خمس اسطر في الملف:

```python
!head data/Berkeley_PD_-_Calls_for_Service.csv
```

```ruby
CASENO,OFFENSE,EVENTDT,EVENTTM,CVLEGEND,CVDOW,InDbDate,Block_Location,BLKADDR,City,State
17091420,BURGLARY AUTO,07/23/2017 12:00:00 AM,06:00,BURGLARY - VEHICLE,0,08/29/2017 08:28:05 AM,"2500 LE CONTE AVE
Berkeley, CA
(37.876965, -122.260544)",2500 LE CONTE AVE,Berkeley,CA
17020462,THEFT FROM PERSON,04/13/2017 12:00:00 AM,08:45,LARCENY,4,08/29/2017 08:28:00 AM,"2200 SHATTUCK AVE
Berkeley, CA
(37.869363, -122.268028)",2200 SHATTUCK AVE,Berkeley,CA
17050275,BURGLARY AUTO,08/24/2017 12:00:00 AM,18:30,BURGLARY - VEHICLE,4,08/29/2017 08:28:06 AM,"200 UNIVERSITY AVE
Berkeley, CA
(37.865491, -122.310065)",200 UNIVERSITY AVE,Berkeley,CA
```

يظهر أن الملف من النوع CSV، ولكن من الصعب معرفه ما اذا كان جميع محتوى الملف منسق بطريقه صحيحه. يمكننا استخدام الداله `pd.read_csv` لقراءة الملف ك DataFrame. إذا اظهر الأمر `pd.read_csv` اخطاء، فيجب علينا البحث بشكل اعمق وحل المشكله يدوياً. لحسن الحظ، قامت الداله بقراءة الملف بشكل صحيح على شكل DataFrame:

```python
calls = pd.read_csv('data/Berkeley_PD_-_Calls_for_Service.csv')
calls
```

**State**|**City**|**BLKADDR**|**Block\_Location**|**...**|**EVENTTM**|**EVENTDT**|**OFFENSE**|**CASENO**||
:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:
CA|Berkeley|2500 LE CONTE AVE|2500 LE CONTE AVE\nBerkeley, CA\n(37.876965, -...|...|06:00|07/23/2017 12:00:00 AM|BURGLARY AUTO|17091420|0
CA|Berkeley|2200 SHATTUCK AVE|2200 SHATTUCK AVE\nBerkeley, CA\n(37.869363, -...|...|08:45|04/13/2017 12:00:00 AM|THEFT FROM PERSON|17020462|1
CA|Berkeley|200 UNIVERSITY AVE|200 UNIVERSITY AVE\nBerkeley, CA\n(37.865491, ...|...|18:30|08/24/2017 12:00:00 AM|BURGLARY AUTO|17050275|2
...|...|...|...|...|...|...|...|...|...
CA|Berkeley|1600 FAIRVIEW ST|1600 FAIRVIEW ST\nBerkeley, CA\n(37.850001, -1...|...|12:22|04/01/2017 12:00:00|DISTURBANCE|17018126|5505
CA|Berkeley|2000 DELAWARE ST|2000 DELAWARE ST\nBerkeley, CA\n(37.874489, -1...|...|12:00|04/01/2017 12:00:00|THEFT MISD. (UNDER $950)|17090665|5506
CA|Berkeley|2400 TELEGRAPH AVE|2400 TELEGRAPH AVE\nBerkeley, CA\n(37.866761, ...|...|20:02|08/22/2017 12:00:00 AM|SEXUAL ASSAULT MISD.|17049700|5507

```center-result
5508 rows × 11 columns
```

يمكننا كتابة داله تقوم بإظهار اجزاء من البيانات:

```python
def df_interact(df):
    def peek(row=0, col=0):
        return df.iloc[row:row + 5, col:col + 6]
    interact(peek, row=(0, len(df), 5), col=(0, len(df.columns) - 6))
    print('({} rows, {} columns) total'.format(df.shape[0], df.shape[1]))

df_interact(calls)
```

> الكود البرمجي السابق يُنتِج لنا جدول تفاعلي كما في الصوره أدناه، يحتوي بشكل تلقائي على ال6 اسطر الأولى و أول 6 عواميد.
> عند تحريك القيم بالأعلى Row ستظهر لنا الخمس اسطر بعد القيمه التي اخترناها، مثلاً في الصوره ادناه اخترت القيمه 815 فأظهر لنا الأسطر من 815 حتى 819.
> بنفس الطريقة للخيار Col، تظهر لنا الست عواميد بعد العامود الذي نختاره، هنا اخترت العامود 2 فأظهر لنا العامود الثالث حتى التاسع.
> تذكر دائماً انه الحساب في الجداول / بايثون يبدأ من 0. 


<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter5/df_interact.png'>
</p>

```ruby
(5508 rows, 11 columns) total
```

بناءًا على النتائج في الأعلى، تبدو البيانات مرتبه بشكل مناسب بما ان العواميد مٌسماه بشكل صحيح والبيانات في كل عامود مدخله بشكل متناسق. ماذا يحتوي كل عامود؟ يمكننا التحقق من ذلك بموقع البيانات:

**النوع**|**الوصف**|**العامود**
:-----:|:-----:|:-----:
رقم|رقم القضيه|CASENO
نص|نوع المخالفه|OFFENSE
تاريخ + وقت|تاريخ الحدث|EVENTDT
نص|وقت الحدث|EVENTTM
نص|تفاصيل الحدث|CVLEGEND
رقم|أي يوم بالاسبوع حدثت فيه المخالفه|CVDOW
تاريخ + وقت|تاريخ ووقت تحديث المخالفه في قاعدة البيانات|InDbDate
Location|عنوان المخالفه|Block\_Location
نص| |BLKADDR
نص| |City
نص| |State

قد تبدو البيانات سهلة التحليل. ولكن قبل البدء بذلك، يجب أن نجيب عن الأسئله التاليه:
1- **هل توجد بيانات مفقوده؟** السؤال مهم لأن البيانات المفقوده قد تكون لعدة اسباب. مثلاً، عناوين مفقوده قد يكون حذفها بسبب حماية خصوصية الأشخاص، أو أن احد المباشرين لهذه المخالفه رفض الإجابه على هذا السؤال، او بسبب حدوث مشاكل في جهاز التسجيل.
2- **هل توجد أي بيانات مفقوده تم تعبئتها؟ ( مثلاً كتابة رقم 999 لعمر مجهول أو 12:00 صباحاً لتاريخ مجهول)؟** بالتأكيد ستأثر هذه على تحليلنا إذا تجاهلناها.
3- **اي جزء من البيانات ادخلت بواسطة اشخاص حقيقين؟** كما سنرى بعد قليل، البيانات التي يتم ادخالها من البشر تحتوي على الكثير من التناقضات والأخطاء الإملائيه.

رغم ان هناك الكثير من المشاكل التي يجب ان نتحقق منها، هذه المشاكل الثلاثه هي الأكثر تكراراً في جميع البيانات. يمكنك مراجعة [Quartz](https://github.com/Quartz/bad-data-guide) لإستعراض قائمة من المشاكل التي يحتاج للتحقق منها في البيانات قبل البدء بالتحليل.

### هل توجد بيانات مفقوده؟

هذه الخطوه سهله التحقق في بانداز عن طريق الكود البرمجي التالي:

```python
# ستظهر لنا الأسطر التي تحتوي على الاقل على قيمه واحده مفقوده
null_rows = calls.isnull().any(axis=1)
calls[null_rows]
```

**State**|**City**|**BLKADDR**|**Block\_Location**|**...**|**EVENTTM**|**EVENTDT**|**OFFENSE**|**CASENO**| |
:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:
CA|Berkeley|NaN|Berkeley, CA\n(37.869058, -122.270455)|...|22:00|03/16/2017 12:00:00 AM|BURGLARY AUTO|17014831|116
CA|Berkeley|NaN|Berkeley, CA\n(37.869058, -122.270455)|...|16:00|07/20/2017 12:00:00 AM|BURGLARY AUTO|17042511|478
CA|Berkeley|NaN|Berkeley, CA\n(37.869058, -122.270455)|...|21:00|04/22/2017 12:00:00 AM|VEHICLE STOLEN|17022572|486
...|...|...|...|...|...|...|...|...|...
CA|Berkeley|NaN|Berkeley, CA\n(37.869058, -122.270455)|...|08:00|07/01/2017 12:00:00|VANDALISM|17091287|4945
CA|Berkeley|NaN|Berkeley, CA\n(37.869058, -122.270455)|...|15:00|06/30/2017 12:00:00 AM|BURGLARY RESIDENTIAL|17038382|4947
CA|Berkeley|NaN|Berkeley, CA\n(37.869058, -122.270455)|...|23:30|08/15/2017 12:00:00 AM|VANDALISM|17091632|5167

```center-result
27 rows × 11 columns
```

يظهر لنا 27 لا تحتوي على عناوين في العامود `BLKADDR`. لسوء الحظ، لا يظهر لنا في شرح البيانات اي معلومه عن طريقة حفظ معلومات العنوان. نعرف أن جميع البيانات لأحداث تمت في مدينة بيركلي، لذا يمكننا الافتراض ان جميع المكالمات كانت لعناوين في مكان ما في بيركلي.

### هل توجد أي بيانات مفقوده تم تعبئتها؟

من النتيجه السابقه نلاحظ ان العامود `Block_Location` يحتوي على القيمه `Berkeley, CA` اذا كان القيمه في العامود `BLKADDR` مفقوده.

أيضاً، التحقق من الجدول اظهر لنا ان العامود `EVENTDT` يحتوي على التاريخ بشكل صحيح ولكن في كل الأسطر تم تسجيل الوقت `12am`، الوقت الحقيقي في العامود `EVENTTM`. لنتحقق من اكتشفناه:

```python
# اظهار اول سبع اسطر
calls.head(7)
```

**State**|**City**|**BLKADDR**|**Block\_Location**|**...**|**EVENTTM**|**EVENTDT**|**OFFENSE**|**CASENO**||
:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:
CA|Berkeley|2500 LE CONTE AVE|2500 LE CONTE AVE\nBerkeley, CA\n(37.876965, -...|...|06:00|07/23/2017 12:00:00 AM|BURGLARY AUTO|17091420|0
CA|Berkeley|2200 SHATTUCK AVE|2200 SHATTUCK AVE\nBerkeley, CA\n(37.869363, -...|...|08:45|04/13/2017 12:00:00 AM|THEFT FROM PERSON|17020462|1
CA|Berkeley|200 UNIVERSITY AVE|200 UNIVERSITY AVE\nBerkeley, CA\n(37.865491, ...|...|18:30|08/24/2017 12:00:00 AM|BURGLARY AUTO|17050275|2
CA|Berkeley|1900 SEVENTH ST|1900 SEVENTH ST\nBerkeley, CA\n(37.869318, -12...|...|17:30|04/06/2017 0:00|GUN/WEAPON|17019145|3
CA|Berkeley|100 PARKSIDE DR|100 PARKSIDE DR\nBerkeley, CA\n(37.854247, -12...|...|18:00|08/01/2017 0:00|VEHICLE STOLEN|17044993|4
CA|Berkeley|1500 PRINCE ST|1500 PRINCE ST\nBerkeley, CA\n(37.851503, -122...|...|12:00|06/28/2017 12:00:00 AM|BURGLARY RESIDENTIAL|17037319|5
CA|Berkeley|300 MENLO PL|300 MENLO PL\nBerkeley, CA\n|...|08:45|05/30/2017 12:00:00 AM|BURGLARY RESIDENTIAL|17030791|6

```center-result
7 rows × 11 columns
```

كخطوة تنظيف، نريد ان نجمع العامودين `EVENTDT` و `EVENTTM` ليحتوي على التاريخ والوقت في عامود واحد. اذا قمنا بكتابة داله تستقبل DataFrame وتنشأ أخرى، فيمكننا لاحقنا إستخدام `pd.pipe` لتطبيق ذلك على جميع القيم: [📝][pd_pipe]

```python
def combine_event_datetimes(calls):
    combined = pd.to_datetime(
        # جمع التاريخ والوقت على شكل نص
        calls['EVENTDT'].str[:10] + ' ' + calls['EVENTTM'],
        infer_datetime_format=True,
    )
    return calls.assign(EVENTDTTM=combined)

# لعرض النتائج قبل التعديل على ال DataFrame
calls.pipe(combine_event_datetimes).head(2)
```

**EVENTDTTM**|**State**|**City**|**BLKADDR**|**...**|**EVENTTM**|**EVENTDT**|**OFFENSE**|**CASENO**| |
:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:
23/07/2017 06:00:00|CA|Berkeley|2500 LE CONTE AVE|...|06:00|07/23/2017 12:00:00 AM|BURGLARY AUTO|17091420|0
2017-04-13 08:45:00|CA|Berkeley|2200 SHATTUCK AVE|...|08:45|04/13/2017 12:00:00 AM|THEFT FROM PERSON|17020462|1

```center-result
2 rows × 12 columns
```

### اي جزء من البيانات ادخلت بواسطة اشخاص حقيقين؟

يبدو أن الكثير من العواميد تم ادخالها بشكل تلقائي بواسطة الآله، بما في ذلك التاريخ، الوقت، اليوم في الأسبوع، وعنوان الحادثه.

ايضاً، العامود `OFFENSE` و `CVLEGEND` يبدو انها تحتوي على بيانات ثابته. يمكننا التحقق من القيم المدخله في كل عامود لنتحقق ان كان هناك اي قيم تحتوي على اخطاء املائيه:

```python
calls['OFFENSE'].unique()
```

```ruby
array(['BURGLARY AUTO', 'THEFT FROM PERSON', 'GUN/WEAPON',
       'VEHICLE STOLEN', 'BURGLARY RESIDENTIAL', 'VANDALISM',
       'DISTURBANCE', 'THEFT MISD. (UNDER $950)', 'THEFT FROM AUTO',
       'DOMESTIC VIOLENCE', 'THEFT FELONY (OVER $950)', 'ALCOHOL OFFENSE',
       'MISSING JUVENILE', 'ROBBERY', 'IDENTITY THEFT',
       'ASSAULT/BATTERY MISD.', '2ND RESPONSE', 'BRANDISHING',
       'MISSING ADULT', 'NARCOTICS', 'FRAUD/FORGERY',
       'ASSAULT/BATTERY FEL.', 'BURGLARY COMMERCIAL', 'MUNICIPAL CODE',
       'ARSON', 'SEXUAL ASSAULT FEL.', 'VEHICLE RECOVERED',
       'SEXUAL ASSAULT MISD.', 'KIDNAPPING', 'VICE', 'HOMICIDE'], dtype=object)
```

```python
calls['CVLEGEND'].unique()
```

```ruby
array(['BURGLARY - VEHICLE', 'LARCENY', 'WEAPONS OFFENSE',
       'MOTOR VEHICLE THEFT', 'BURGLARY - RESIDENTIAL', 'VANDALISM',
       'DISORDERLY CONDUCT', 'LARCENY - FROM VEHICLE', 'FAMILY OFFENSE',
       'LIQUOR LAW VIOLATION', 'MISSING PERSON', 'ROBBERY', 'FRAUD',
       'ASSAULT', 'NOISE VIOLATION', 'DRUG VIOLATION',
       'BURGLARY - COMMERCIAL', 'ALL OTHER OFFENSES', 'ARSON', 'SEX CRIME',
       'RECOVERED VEHICLE', 'KIDNAPPING', 'HOMICIDE'], dtype=object)
```

بما ان كل قيمه يبدو انها ادخلت بشكل صحيح، لن نحتاج للقيام بأي تعديلات على العامودان.

تحققنا ايضاً من العامود `BLKADDR` ان كان يحتوي على اي تناقضات ووجدنا ان العناوين ادخلت في بعض المرات على الشكل التالي `2500 LE CONTE AVE` كعنوان كامل وفي بعض مرات اخرى سجل العنوان كتقاطع شارعين مثل `ALLSTON WAY & FIFTH ST`. يشير ذلك ان البيانات ادخلت يدوياً بواسطة اشخاص حقيقين وليس بشكل آلي، هذا العامود سيكون صعب تحليله. لحسن الظهر يمكننا استخدام بيانات خطوط الطول والعرض `Latitude` و `Longitude` بدلاً من العنوان.


```python
calls['BLKADDR'][[0, 5001]]
```

```ruby
0            2500 LE CONTE AVE
5001    ALLSTON WAY & FIFTH ST
Name: BLKADDR, dtype: object
```

### لمسات اخيره

تبدو البيانات جاهزه للتحليل الآن. العامود `Block_Location` يحتوي على نص بالعنوان، خط الطول والعرض. نحتاج لفصل قيمة خط الطول والعرض ليسهل استخدامها: 

```python
def split_lat_lon(calls):
    return calls.join(
        calls['Block_Location']
        # الحصول على الإحداثات من النص ( بيانات خط الطول والعرض)
        .str.split('\n').str[2]
        # حذف الأقواس من النص
        .str[1:-1]
        # فصل قيم خط الطول والعرض إلى عامودان
        .str.split(', ', expand=True)
        .rename(columns={0: 'Latitude', 1: 'Longitude'})
    )

calls.pipe(split_lat_lon).head(2)
```

**Longitude**|**Latitude**|**State**|**City**|**...**|**EVENTTM**|**EVENTDT**|**OFFENSE**|**CASENO**| 
:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:
-122.260544|37.876965|CA|Berkeley|...|6:00|07/23/2017 12:00:00 AM|BURGLARY AUTO|17091420|0
-122.268028|37.869363|CA|Berkeley|...|8:45|04/13/2017 12:00:00 AM|THEFT FROM PERSON|17020462|1

```center-result
2 rows × 13 columns
```

ثم نربط كل رقم في العامود `CVDOW` مع نص اليوم، سنستخدم الملف `cvdow.csv` الذي تم تجهيزه مسبقاً، يحتوي الملف على رقم اليوم واليوم كنص مكتوب:

> لتحميل الملف: [cvdow.csv]({{ site.baseurl }}/files/chapter5/cvdow.csv)

```python
day_of_week = pd.read_csv('data/cvdow.csv')
day_of_week
```

**Day**|**CVDOW**||
:-----:|:-----:|:-----:
Sunday|0|0
Monday|1|1
Tuesday|2|2
Wednesday|3|3
Thursday|4|4
Friday|5|5
Saturday|6|6

نقوم بترجمة العامود `CVDOW` إلى نص:

```python
def match_weekday(calls):
    return calls.merge(day_of_week, on='CVDOW')

calls.pipe(match_weekday).head(2)
```

**Day**|**State**|**City**|**BLKADDR**|**...**|**EVENTTM**|**EVENTDT**|**OFFENSE**|**CASENO**| 
:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:
Sunday|CA|Berkeley|2500 LE CONTE AVE|...|6:00|07/23/2017 12:00:00 AM|BURGLARY AUTO|17091420|0
Sunday|CA|Berkeley|BOWDITCH STREET & CHANNING WAY|...|22:00|07/02/2017 0:00|BURGLARY AUTO|17038302|1

```center-result
2 rows × 12 columns
```

سنحذف العواميد التي لا نحتاجها:

```python
def drop_unneeded_cols(calls):
    return calls.drop(columns=['CVDOW', 'InDbDate', 'Block_Location', 'City',
                               'State', 'EVENTDT', 'EVENTTM'])
```

الآن نقوم بتطبيق جميع الدوال التي عرفناها سابقاً على البيانات:

```python
calls_final = (calls.pipe(combine_event_datetimes)
               .pipe(split_lat_lon)
               .pipe(match_weekday)
               .pipe(drop_unneeded_cols))
df_interact(calls_final)
```

> بنفس الطريقة السابقة سيظهر لنا جدول تفاعلي.

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter5/df_interact_final.png'>
</p>

```center-result
(5508 rows, 8 columns) total
```

الآن، بيانات المكالمات جاهزة للتحليل. في الجزء التالي، سنقوم بتنظيف بيانات الإيقافات.











[pd_pipe]: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.pipe.html