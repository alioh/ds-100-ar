---
title: تنظيف البيانات
show_title: true
chapter_number: 5
chapter_text: الفصل الخامس
chapter_lessons: [[0, 'مقدمة'], [1, 'نظره على بيانات شرطة مدينة بيركلي'],[2, 'تنظيف بيانات المكالمات'], [3, 'تنظيف بيانات الإيقافات'], [4, 'شكل وتجميع البيانات'], [5, 'تقسيم البيانات'], [6, 'مدى البيانات'], [7, 'زمانية البيانات'], [8 , 'مدى ثقتنا بالبيانات']]
chapter_sublessons: [
    [],
    [],
    ['فهم توليد البيانات', 'تنظيف البيانات', 'هل توجد بيانات مفقوده؟', 'هل توجد أي بيانات مفقوده تم تعبئتها؟', 'اي جزء من البيانات ادخلت بواسطة اشخاص حقيقين؟', 'لمسات اخيره'],
    ['هل توجد بيانات مفقوده ؟', 'هل توجد أي بيانات مفقوده تم تعبئتها ؟', 'اي جزء من البيانات ادخلت بواسطة اشخاص حقيقين ؟', 'الملخص'],
    ['شكل البيانات', 'التجميع', 'قائمة المراجعه لشكل البيانات'],
    ['تقسيم البيانات', 'قائمة المراجعه لتقسيم البيانات'],
    [],
    [],
    []
]
layout: default
---

## مقدمة

تأتي البيانات بعدة اشكال وتتنوع من حيث فائدتها في التحليل. رغم اننا نسعى ان تكون جميع البيانات على شكل جدول وكل قيمه ادخل بشكل مستمر ودقيق، في الحقيقه يجب علينا التحقق بشكل دقيقه عن المشاكل التي قد تقودنا لنتائج غير صحيحه.

المصطلح "تنظيف البيانات" يطلق على خطوة التحقق من البيانات واتخاذ قرارات عن كيفية إصلاح الاخطاء والبيانات المفقوده. سنناقش اكثر المشاكل الشائعه في البيانات ونشرحها بشكل اوضح.

تنظيف البيانات له بعض القيود. مثلاً، لا يوجد تنظيف بأمكانه تحسين عينات تم اخذها بشكل متحيز. قبل البدء بمشوار تنظيف البيانات الذي عاده ما يكون طويلاً، يجب ان نتأكد ان بياناتنا تم جمعها بشكل دقيق وبدون تحيز. في ذلك الحين يمكننا بدأ التحقق من البيانات وتنظيفها للتخلص من المشاكل في انواع البيانات او طرق الادخال.

سنقوم بشرح طٌرق لتنظيف البيانات وسنستخدم بيانات شرطة مدينة بيركلي.


## نظره على بيانات شرطة مدينة بيركلي

سنستخدم بيانات شرطة مدينة بيركلي المنشوره للعامه لشرح طرق تنظيف البيانات. يمكن تحميل بيانات المكالمات من [هنا](https://data.cityofberkeley.info/Public-Safety/Berkeley-PD-Calls-for-Service/k2nh-s5h5)، وبيانات الإيقافات من [هنا](https://data.cityofberkeley.info/Public-Safety/Berkeley-PD-Stop-Data/6e9j-pj9p).

> روابط اخرى لتحميل البيانات:
> - [بيانات المكالمات]({{ site.baseurl }}/files/chapter5/Berkeley_PD_-_Calls_for_Service.csv).
> - [بيانات الإيقافات]({{ site.baseurl }}/files/chapter5/stops.json).

سنستخدم الامر `ls` مع `-lh` لعرض معلومات أكثر عن الملفات:

```python
!ls -lh data/
```

```ruby
total 13936
-rw-r--r--@ 1 sam  staff   979K Aug 29 14:41 Berkeley_PD_-_Calls_for_Service.csv
-rw-r--r--@ 1 sam  staff    81B Aug 29 14:28 cvdow.csv
-rw-r--r--@ 1 sam  staff   5.8M Aug 29 14:41 stops.json
```

يظهر لنا الأمر السابق الملفات في المجلد واحجامها. الأمر مهم لأنه يبين لنا أن الملفات صغيرة الحجم ويمكن تحميلها على ذاكرة حاسبنا. قاعده عامه، يمكن تحميل الملفات بشكل آمن عندما يكون حجمها حوالي ربع حجم الذاكرة في الحاسب. مثلاً، اذا كان جهازنا يحتوي على 4 ج.ب. من الذاكرة العشوائيه، يمكننا تحميل ملف CSV بحجم 1 ج.ب في بانداز. ليتحمل حاسبنا ملفات بحجم أكبر يجب علينا استخدام ادوات خاصه وسنتحدث لاحقاً عنها في هذا الكتاب.

لاحظ استخدامنا لعلامة التعجب قبل `ls`. هذا يخبر جوبتر ان الكود البرمجي التالي خاص بمترجم الشيل، وليس بايثون. مثال آخر:

```python
# الامر `wc` يظهر عدد الاسطر في كل ملف.
# يمكن ان نلاحظ ان ملف `stops.json` يحتوي على عدد (29852) سطر.
!wc -l data/*
```

```ruby
   16497 data/Berkeley_PD_-_Calls_for_Service.csv
       8 data/cvdow.csv
   29852 data/stops.json
   46357 total
```

## تنظيف بيانات المكالمات

### فهم توليد البيانات

سنقوم بذكر بعض الأسئله التي يجب أن تتحقق منها في جميع بياناتك قبل البدء بتنظيفها ومعالجتها. هذه الأسئله عن كيف تم توليد وإنشاء هذه البيانات، في هذه الخطوه تنظيف البيانات **لن** يساعدنا على حل المشاكل التي حصلت اثناء إنشاء البيانات.

**على ماذا تحتوي البيانات؟** الموقع الذي تم اخذ بيانات المكالمات منه يذكر ان "الجرائم/الحوادث (وليس تقارير الجرائم) التي حدثت في ال180 يوم السابقه". مزيد من القراءه في الموقع اوضحت التالي "ليس جميع المكالمات التي طلبت خدمات الشرطه تم اضافتها (مثل عضة حيوان)".

موقع بيانات الإيقافات يوضح ان البيانات تحتوي على جميع "السيارات المحتجزه (بما فيها الدراجات الهوائية) وإعتقالات المشاه (حتى خمس اشخاص)" منذ 26 يناير 2015.

**هل البيانات تعداد / احصاء للسكان؟** هذا يعتمد على عدد مجتمعنا الإحصائي. مثلاً، اذا كنا مهتمين بمكالمات طلب الخدمات لآخر 180 يوم للحوادث والجرائم، فأن بيانات المكالمات تعتبر كذلك. ولكن اذا اردنا المكالمات لطلب خدمات الشرطة للعشر سنوات السابقة، فأن البيانات ليست تعداد مناسب. يمكننا قول نفس الكلام على بيانات الإيقافات لما ان البيانات تم جمعها إبتداءًا من 26 يناير 2015.

**اذا كانت البيانات تُشكل عينه، هل هي عينة الإحتمالات؟** البيانات لا تمثل عينة الإحتمالات لأنها لا تظهر اي عشوائيه في طريقة جمع البيانات. لدينا بيانات كامله لفتره معينه فقط وليس لفترات أخرى.

**هل هناك قيود ستجبرنا عليه البيانات في نتائجنا؟** رغم اننا سنسأل هذا السؤال بعد كل خطوه من خطواتنا، يمكننا ملاحظة ان بياناتنا تظهر لنا بعض القيود. أهم القيود التي تظهرها لنا هي اننا لا يمكننا القيام بأي تقديرات غير متحيزه للفترات التي لم يتم تسجيلها في البيانات.

### تنظيف البيانات

لنبدأ الآن بتنظيف بيانات المكالمات. الأمر `head` يظهر لنا أول خمس اسطر في الملف:

```python
!head data/Berkeley_PD_-_Calls_for_Service.csv
```

```ruby
CASENO,OFFENSE,EVENTDT,EVENTTM,CVLEGEND,CVDOW,InDbDate,Block_Location,BLKADDR,City,State
17091420,BURGLARY AUTO,07/23/2017 12:00:00 AM,06:00,BURGLARY - VEHICLE,0,08/29/2017 08:28:05 AM,"2500 LE CONTE AVE
Berkeley, CA
(37.876965, -122.260544)",2500 LE CONTE AVE,Berkeley,CA
17020462,THEFT FROM PERSON,04/13/2017 12:00:00 AM,08:45,LARCENY,4,08/29/2017 08:28:00 AM,"2200 SHATTUCK AVE
Berkeley, CA
(37.869363, -122.268028)",2200 SHATTUCK AVE,Berkeley,CA
17050275,BURGLARY AUTO,08/24/2017 12:00:00 AM,18:30,BURGLARY - VEHICLE,4,08/29/2017 08:28:06 AM,"200 UNIVERSITY AVE
Berkeley, CA
(37.865491, -122.310065)",200 UNIVERSITY AVE,Berkeley,CA
```

يظهر أن الملف من النوع CSV، ولكن من الصعب معرفه ما اذا كان جميع محتوى الملف منسق بطريقه صحيحه. يمكننا استخدام الداله `pd.read_csv` لقراءة الملف ك DataFrame. إذا اظهر الأمر `pd.read_csv` اخطاء، فيجب علينا البحث بشكل اعمق وحل المشكله يدوياً. لحسن الحظ، قامت الداله بقراءة الملف بشكل صحيح على شكل DataFrame:

```python
calls = pd.read_csv('data/Berkeley_PD_-_Calls_for_Service.csv')
calls
```

<div class="table-wrapper" markdown="block">

**State**|**City**|**BLKADDR**|**Block\_Location**|**...**|**EVENTTM**|**EVENTDT**|**OFFENSE**|**CASENO**||
:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:
CA|Berkeley|2500 LE CONTE AVE|2500 LE CONTE AVE\nBerkeley, CA\n(37.876965, -...|...|06:00|07/23/2017 12:00:00 AM|BURGLARY AUTO|17091420|0
CA|Berkeley|2200 SHATTUCK AVE|2200 SHATTUCK AVE\nBerkeley, CA\n(37.869363, -...|...|08:45|04/13/2017 12:00:00 AM|THEFT FROM PERSON|17020462|1
CA|Berkeley|200 UNIVERSITY AVE|200 UNIVERSITY AVE\nBerkeley, CA\n(37.865491, ...|...|18:30|08/24/2017 12:00:00 AM|BURGLARY AUTO|17050275|2
...|...|...|...|...|...|...|...|...|...
CA|Berkeley|1600 FAIRVIEW ST|1600 FAIRVIEW ST\nBerkeley, CA\n(37.850001, -1...|...|12:22|04/01/2017 12:00:00|DISTURBANCE|17018126|5505
CA|Berkeley|2000 DELAWARE ST|2000 DELAWARE ST\nBerkeley, CA\n(37.874489, -1...|...|12:00|04/01/2017 12:00:00|THEFT MISD. (UNDER $950)|17090665|5506
CA|Berkeley|2400 TELEGRAPH AVE|2400 TELEGRAPH AVE\nBerkeley, CA\n(37.866761, ...|...|20:02|08/22/2017 12:00:00 AM|SEXUAL ASSAULT MISD.|17049700|5507

</div>

```center-result
5508 rows × 11 columns
```

يمكننا كتابة داله تقوم بإظهار اجزاء من البيانات:

```python
def df_interact(df):
    def peek(row=0, col=0):
        return df.iloc[row:row + 5, col:col + 6]
    interact(peek, row=(0, len(df), 5), col=(0, len(df.columns) - 6))
    print('({} rows, {} columns) total'.format(df.shape[0], df.shape[1]))

df_interact(calls)
```

> الكود البرمجي السابق يُنتِج لنا جدول تفاعلي كما في الصوره أدناه، يحتوي بشكل تلقائي على ال6 اسطر الأولى و أول 6 عواميد.
> عند تحريك القيم بالأعلى Row ستظهر لنا الخمس اسطر بعد القيمه التي اخترناها، مثلاً في الصوره ادناه اخترت القيمه 815 فأظهر لنا الأسطر من 815 حتى 819.
> بنفس الطريقة للخيار Col، تظهر لنا الست عواميد بعد العامود الذي نختاره، هنا اخترت العامود 2 فأظهر لنا العامود الثالث حتى التاسع.
> تذكر دائماً انه الحساب في الجداول / بايثون يبدأ من 0. 


<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter5/df_interact_calls.png'>
</p>

```ruby
(5508 rows, 11 columns) total
```

بناءًا على النتائج في الأعلى، تبدو البيانات مرتبه بشكل مناسب بما ان العواميد مٌسماه بشكل صحيح والبيانات في كل عامود مدخله بشكل متناسق. ماذا يحتوي كل عامود؟ يمكننا التحقق من ذلك بموقع البيانات:

**النوع**|**الوصف**|**العامود**
:-----:|:-----:|:-----:
رقم|رقم القضيه|CASENO
نص|نوع المخالفه|OFFENSE
تاريخ + وقت|تاريخ الحدث|EVENTDT
نص|وقت الحدث|EVENTTM
نص|تفاصيل الحدث|CVLEGEND
رقم|أي يوم بالاسبوع حدثت فيه المخالفه|CVDOW
تاريخ + وقت|تاريخ ووقت تحديث المخالفه في قاعدة البيانات|InDbDate
عنوان|عنوان المخالفه|Block\_Location
نص| |BLKADDR
نص| |City
نص| |State

قد تبدو البيانات سهلة التحليل. ولكن قبل البدء بذلك، يجب أن نجيب عن الأسئله التاليه:
- **هل توجد بيانات مفقوده؟** السؤال مهم لأن البيانات المفقوده قد تكون لعدة اسباب. مثلاً، عناوين مفقوده قد يكون حذفها بسبب حماية خصوصية الأشخاص، أو أن احد المباشرين لهذه المخالفه رفض الإجابه على هذا السؤال، او بسبب حدوث مشاكل في جهاز التسجيل.
- **هل توجد أي بيانات مفقوده تم تعبئتها؟ ( مثلاً كتابة رقم 999 لعمر مجهول أو 12:00 صباحاً لتاريخ مجهول)؟** بالتأكيد ستأثر هذه على تحليلنا إذا تجاهلناها.
- **اي جزء من البيانات ادخلت بواسطة اشخاص حقيقين؟** كما سنرى بعد قليل، البيانات التي يتم ادخالها من البشر تحتوي على الكثير من التناقضات والأخطاء الإملائيه.

رغم ان هناك الكثير من المشاكل التي يجب ان نتحقق منها، هذه المشاكل الثلاثه هي الأكثر تكراراً في جميع البيانات. يمكنك مراجعة [Quartz](https://github.com/Quartz/bad-data-guide) لإستعراض قائمة من المشاكل التي يحتاج للتحقق منها في البيانات قبل البدء بالتحليل.

### هل توجد بيانات مفقوده؟

هذه الخطوه سهله التحقق في بانداز عن طريق الكود البرمجي التالي:

```python
# ستظهر لنا الأسطر التي تحتوي على الاقل على قيمه واحده مفقوده
null_rows = calls.isnull().any(axis=1)
calls[null_rows]
```

<div class="table-wrapper" markdown="block">

**State**|**City**|**BLKADDR**|**Block\_Location**|**...**|**EVENTTM**|**EVENTDT**|**OFFENSE**|**CASENO**| |
:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:
CA|Berkeley|NaN|Berkeley, CA\n(37.869058, -122.270455)|...|22:00|03/16/2017 12:00:00 AM|BURGLARY AUTO|17014831|116
CA|Berkeley|NaN|Berkeley, CA\n(37.869058, -122.270455)|...|16:00|07/20/2017 12:00:00 AM|BURGLARY AUTO|17042511|478
CA|Berkeley|NaN|Berkeley, CA\n(37.869058, -122.270455)|...|21:00|04/22/2017 12:00:00 AM|VEHICLE STOLEN|17022572|486
...|...|...|...|...|...|...|...|...|...
CA|Berkeley|NaN|Berkeley, CA\n(37.869058, -122.270455)|...|08:00|07/01/2017 12:00:00|VANDALISM|17091287|4945
CA|Berkeley|NaN|Berkeley, CA\n(37.869058, -122.270455)|...|15:00|06/30/2017 12:00:00 AM|BURGLARY RESIDENTIAL|17038382|4947
CA|Berkeley|NaN|Berkeley, CA\n(37.869058, -122.270455)|...|23:30|08/15/2017 12:00:00 AM|VANDALISM|17091632|5167

</div>

```center-result
27 rows × 11 columns
```

يظهر لنا 27 لا تحتوي على عناوين في العامود `BLKADDR`. لسوء الحظ، لا يظهر لنا في شرح البيانات اي معلومه عن طريقة حفظ معلومات العنوان. نعرف أن جميع البيانات لأحداث تمت في مدينة بيركلي، لذا يمكننا الافتراض ان جميع المكالمات كانت لعناوين في مكان ما في بيركلي.

### هل توجد أي بيانات مفقوده تم تعبئتها؟

من النتيجه السابقه نلاحظ ان العامود `Block_Location` يحتوي على القيمه `Berkeley, CA` اذا كان القيمه في العامود `BLKADDR` مفقوده.

أيضاً، التحقق من الجدول اظهر لنا ان العامود `EVENTDT` يحتوي على التاريخ بشكل صحيح ولكن في كل الأسطر تم تسجيل الوقت `12am`، الوقت الحقيقي في العامود `EVENTTM`. لنتحقق من اكتشفناه:

```python
# اظهار اول سبع اسطر
calls.head(7)
```

<div class="table-wrapper" markdown="block">

**State**|**City**|**BLKADDR**|**Block\_Location**|**...**|**EVENTTM**|**EVENTDT**|**OFFENSE**|**CASENO**||
:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:
CA|Berkeley|2500 LE CONTE AVE|2500 LE CONTE AVE\nBerkeley, CA\n(37.876965, -...|...|06:00|07/23/2017 12:00:00 AM|BURGLARY AUTO|17091420|0
CA|Berkeley|2200 SHATTUCK AVE|2200 SHATTUCK AVE\nBerkeley, CA\n(37.869363, -...|...|08:45|04/13/2017 12:00:00 AM|THEFT FROM PERSON|17020462|1
CA|Berkeley|200 UNIVERSITY AVE|200 UNIVERSITY AVE\nBerkeley, CA\n(37.865491, ...|...|18:30|08/24/2017 12:00:00 AM|BURGLARY AUTO|17050275|2
CA|Berkeley|1900 SEVENTH ST|1900 SEVENTH ST\nBerkeley, CA\n(37.869318, -12...|...|17:30|04/06/2017 0:00|GUN/WEAPON|17019145|3
CA|Berkeley|100 PARKSIDE DR|100 PARKSIDE DR\nBerkeley, CA\n(37.854247, -12...|...|18:00|08/01/2017 0:00|VEHICLE STOLEN|17044993|4
CA|Berkeley|1500 PRINCE ST|1500 PRINCE ST\nBerkeley, CA\n(37.851503, -122...|...|12:00|06/28/2017 12:00:00 AM|BURGLARY RESIDENTIAL|17037319|5
CA|Berkeley|300 MENLO PL|300 MENLO PL\nBerkeley, CA\n|...|08:45|05/30/2017 12:00:00 AM|BURGLARY RESIDENTIAL|17030791|6

</div>

```center-result
7 rows × 11 columns
```

كخطوة تنظيف، نريد ان نجمع العامودين `EVENTDT` و `EVENTTM` ليحتوي على التاريخ والوقت في عامود واحد. اذا قمنا بكتابة داله تستقبل DataFrame وتنشأ أخرى، فيمكننا لاحقنا إستخدام `pd.pipe` لتطبيق ذلك على جميع القيم: [📝][pd_pipe]

```python
def combine_event_datetimes(calls):
    combined = pd.to_datetime(
        # جمع التاريخ والوقت على شكل نص
        calls['EVENTDT'].str[:10] + ' ' + calls['EVENTTM'],
        infer_datetime_format=True,
    )
    return calls.assign(EVENTDTTM=combined)

# لعرض النتائج قبل التعديل على ال DataFrame
calls.pipe(combine_event_datetimes).head(2)
```

<div class="table-wrapper" markdown="block">

**EVENTDTTM**|**State**|**City**|**BLKADDR**|**...**|**EVENTTM**|**EVENTDT**|**OFFENSE**|**CASENO**| |
:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:
23/07/2017 06:00:00|CA|Berkeley|2500 LE CONTE AVE|...|06:00|07/23/2017 12:00:00 AM|BURGLARY AUTO|17091420|0
2017-04-13 08:45:00|CA|Berkeley|2200 SHATTUCK AVE|...|08:45|04/13/2017 12:00:00 AM|THEFT FROM PERSON|17020462|1

</div>

```center-result
2 rows × 12 columns
```

### اي جزء من البيانات ادخلت بواسطة اشخاص حقيقين؟

يبدو أن الكثير من العواميد تم ادخالها بشكل تلقائي بواسطة الآله، بما في ذلك التاريخ، الوقت، اليوم في الأسبوع، وعنوان الحادثه.

ايضاً، العامود `OFFENSE` و `CVLEGEND` يبدو انها تحتوي على بيانات ثابته. يمكننا التحقق من القيم المدخله في كل عامود لنتحقق ان كان هناك اي قيم تحتوي على اخطاء املائيه:

```python
calls['OFFENSE'].unique()
```

```ruby
array(['BURGLARY AUTO', 'THEFT FROM PERSON', 'GUN/WEAPON',
       'VEHICLE STOLEN', 'BURGLARY RESIDENTIAL', 'VANDALISM',
       'DISTURBANCE', 'THEFT MISD. (UNDER $950)', 'THEFT FROM AUTO',
       'DOMESTIC VIOLENCE', 'THEFT FELONY (OVER $950)', 'ALCOHOL OFFENSE',
       'MISSING JUVENILE', 'ROBBERY', 'IDENTITY THEFT',
       'ASSAULT/BATTERY MISD.', '2ND RESPONSE', 'BRANDISHING',
       'MISSING ADULT', 'NARCOTICS', 'FRAUD/FORGERY',
       'ASSAULT/BATTERY FEL.', 'BURGLARY COMMERCIAL', 'MUNICIPAL CODE',
       'ARSON', 'SEXUAL ASSAULT FEL.', 'VEHICLE RECOVERED',
       'SEXUAL ASSAULT MISD.', 'KIDNAPPING', 'VICE', 'HOMICIDE'], dtype=object)
```

```python
calls['CVLEGEND'].unique()
```

```ruby
array(['BURGLARY - VEHICLE', 'LARCENY', 'WEAPONS OFFENSE',
       'MOTOR VEHICLE THEFT', 'BURGLARY - RESIDENTIAL', 'VANDALISM',
       'DISORDERLY CONDUCT', 'LARCENY - FROM VEHICLE', 'FAMILY OFFENSE',
       'LIQUOR LAW VIOLATION', 'MISSING PERSON', 'ROBBERY', 'FRAUD',
       'ASSAULT', 'NOISE VIOLATION', 'DRUG VIOLATION',
       'BURGLARY - COMMERCIAL', 'ALL OTHER OFFENSES', 'ARSON', 'SEX CRIME',
       'RECOVERED VEHICLE', 'KIDNAPPING', 'HOMICIDE'], dtype=object)
```

بما ان كل قيمه يبدو انها ادخلت بشكل صحيح، لن نحتاج للقيام بأي تعديلات على العامودان.

تحققنا ايضاً من العامود `BLKADDR` ان كان يحتوي على اي تناقضات ووجدنا ان العناوين ادخلت في بعض المرات على الشكل التالي `2500 LE CONTE AVE` كعنوان كامل وفي بعض مرات اخرى سجل العنوان كتقاطع شارعين مثل `ALLSTON WAY & FIFTH ST`. يشير ذلك ان البيانات ادخلت يدوياً بواسطة اشخاص حقيقين وليس بشكل آلي، هذا العامود سيكون صعب تحليله. لحسن الظهر يمكننا استخدام بيانات خطوط الطول والعرض `Latitude` و `Longitude` بدلاً من العنوان.


```python
calls['BLKADDR'][[0, 5001]]
```

```ruby
0            2500 LE CONTE AVE
5001    ALLSTON WAY & FIFTH ST
Name: BLKADDR, dtype: object
```

### لمسات اخيره

تبدو البيانات جاهزه للتحليل الآن. العامود `Block_Location` يحتوي على نص بالعنوان، خط الطول والعرض. نحتاج لفصل قيمة خط الطول والعرض ليسهل استخدامها: 

```python
def split_lat_lon(calls):
    return calls.join(
        calls['Block_Location']
        # الحصول على الإحداثات من النص ( بيانات خط الطول والعرض)
        .str.split('\n').str[2]
        # حذف الأقواس من النص
        .str[1:-1]
        # فصل قيم خط الطول والعرض إلى عامودان
        .str.split(', ', expand=True)
        .rename(columns={0: 'Latitude', 1: 'Longitude'})
    )

calls.pipe(split_lat_lon).head(2)
```
<div class="table-wrapper" markdown="block">

**Longitude**|**Latitude**|**State**|**City**|**...**|**EVENTTM**|**EVENTDT**|**OFFENSE**|**CASENO**| 
:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:
-122.260544|37.876965|CA|Berkeley|...|6:00|07/23/2017 12:00:00 AM|BURGLARY AUTO|17091420|0
-122.268028|37.869363|CA|Berkeley|...|8:45|04/13/2017 12:00:00 AM|THEFT FROM PERSON|17020462|1

</div>

```center-result
2 rows × 13 columns
```

ثم نربط كل رقم في العامود `CVDOW` مع نص اليوم، سنستخدم الملف `cvdow.csv` الذي تم تجهيزه مسبقاً، يحتوي الملف على رقم اليوم واليوم كنص مكتوب:

> لتحميل الملف: [cvdow.csv]({{ site.baseurl }}/files/chapter5/cvdow.csv)

```python
day_of_week = pd.read_csv('data/cvdow.csv')
day_of_week
```

**Day**|**CVDOW**||
:-----:|:-----:|:-----:
Sunday|0|0
Monday|1|1
Tuesday|2|2
Wednesday|3|3
Thursday|4|4
Friday|5|5
Saturday|6|6

نقوم بترجمة العامود `CVDOW` إلى نص:

```python
def match_weekday(calls):
    return calls.merge(day_of_week, on='CVDOW')

calls.pipe(match_weekday).head(2)
```

<div class="table-wrapper" markdown="block">

**Day**|**State**|**City**|**BLKADDR**|**...**|**EVENTTM**|**EVENTDT**|**OFFENSE**|**CASENO**| 
:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:
Sunday|CA|Berkeley|2500 LE CONTE AVE|...|6:00|07/23/2017 12:00:00 AM|BURGLARY AUTO|17091420|0
Sunday|CA|Berkeley|BOWDITCH STREET & CHANNING WAY|...|22:00|07/02/2017 0:00|BURGLARY AUTO|17038302|1

</div>

```center-result
2 rows × 12 columns
```

سنحذف العواميد التي لا نحتاجها:

```python
def drop_unneeded_cols(calls):
    return calls.drop(columns=['CVDOW', 'InDbDate', 'Block_Location', 'City',
                               'State', 'EVENTDT', 'EVENTTM'])
```

الآن نقوم بتطبيق جميع الدوال التي عرفناها سابقاً على البيانات:

```python
calls_final = (calls.pipe(combine_event_datetimes)
               .pipe(split_lat_lon)
               .pipe(match_weekday)
               .pipe(drop_unneeded_cols))
df_interact(calls_final)
```

> بنفس الطريقة السابقة سيظهر لنا جدول تفاعلي.

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter5/df_interact_calls_final.png'>
</p>

```center-result
(5508 rows, 8 columns) total
```

الآن، بيانات المكالمات جاهزة للتحليل. في الجزء التالي، سنقوم بتنظيف بيانات الإيقافات.


### تنظيف بيانات الإيقافات

لنبدأ تجهيز بيانات ملف الإيقافات للتحليل.

> لتحميل الملف: [stops.json]({{ site.baseurl }}/files/chapter5/stops.json)

نستخدم `head` لعرض الاسطر الاولى في الملف:

```python
!head data/stops.json
```

```ruby
{
  "meta" : {
    "view" : {
      "id" : "6e9j-pj9p",
      "name" : "Berkeley PD - Stop Data",
      "attribution" : "Berkeley Police Department",
      "averageRating" : 0,
      "category" : "Public Safety",
      "createdAt" : 1444171604,
      "description" : "This data was extracted from the Department’s Public Safety Server and covers the data beginning January 26, 2015.  On January 26, 2015 the department began collecting data pursuant to General Order B-4 (issued December 31, 2014).  Under that order, officers were required to provide certain data after making all vehicle detentions (including bicycles) and pedestrian detentions (up to five persons).  This data set lists stops by police in the categories of traffic, suspicious vehicle, pedestrian and bicycle stops.  Incident number, date and time, location and disposition codes are also listed in this data.\r\n\r\nAddress data has been changed from a specific address, where applicable, and listed as the block where the incident occurred.  Disposition codes were entered by officers who made the stop.  These codes included the person(s) race, gender, age (range), reason for the stop, enforcement action taken, and whether or not a search was conducted.\r\n\r\nThe officers of the Berkeley Police Department are prohibited from biased based policing, which is defined as any police-initiated action that relies on the race, ethnicity, or national origin rather than the behavior of an individual or information that leads the police to a particular individual who has been identified as being engaged in criminal activity.",
```

كما يبدو واضحاً ان الملف ليس من النوع CSV. الملف يحتوي على بيانات من النوع JSON (JavaScript Object Notation) "ترميز الكائنات باستعمال جافا سكريبت"، طريقه كثيرة الاستخدام تُحفظ فيها البيانات على في مصفوفه من النوع Dictionary. مكتبة [JSON](https://docs.python.org/3/library/json.html) في بايثون تسهل علينا قراءة هذا النوع من المصفوفات:

> تم شرح هذا النوع من المصفوفات Dictionary في الفصل الثالث. تحتوي على مفاتيح Keys وقيم لها Values.

```python
import json

# انتبه ان الملف قد يستهلك جميع ذاكرة الجهاز اذا كان حجمه كبير
# تحققنا من الحجم قبل تحميل الملف

with open('data/stops.json') as f:
    stops_dict = json.load(f)

stops_dict.keys()
```

```ruby
dict_keys(['meta', 'data'])
```

لاحظ اننا اظهرنا فقط المفاتيح في المصفوفه `stops_dict` كي لا نتسبب ببطئ المتصفح اثناء طباعة جميع محتوى المصفوفه. لإلقاء نظره على البيانات دون ان نتسبب بتعطيل المتصفح بسبب حجم البيانات الكبير، يمكننا تحويل المصفوفه إلى نص وطباعة بعض من محتواها:

```python
from pprint import pformat

def print_dict(dictionary, num_chars=1000):
    print(pformat(dictionary)[:num_chars])

print_dict(stops_dict['meta'])
```

> عرف الكاتب هنا داله لطباعة المصفوفه، تستقبل الداله المصفوفه + عدد الأسطر الم

```ruby
{'view': {'attribution': 'Berkeley Police Department',
          'averageRating': 0,
          'category': 'Public Safety',
          'columns': [{'dataTypeName': 'meta_data',
                       'fieldName': ':sid',
                       'flags': ['hidden'],
                       'format': {},
                       'id': -1,
                       'name': 'sid',
                       'position': 0,
                       'renderTypeName': 'meta_data'},
                      {'dataTypeName': 'meta_data',
                       'fieldName': ':id',
                       'flags': ['hidden'],
                       'format': {},
                       'id': -1,
                       'name': 'id',
                       'position': 0,
                       'renderTypeName': 'meta_data'},
                      {'dataTypeName': 'meta_data',
                       'fieldName': ':position',
                       'flags': ['hidden'],
                       'format': {},
```

```python
print_dict(stops_dict['data'], num_chars=300)
```

```ruby
[[1,
  '29A1B912-A0A9-4431-ADC9-FB375809C32E',
  1,
  1444146408,
  '932858',
  1444146408,
  '932858',
  None,
  '2015-00004825',
  '2015-01-26T00:10:00',
  'SAN PABLO AVE / MARIN AVE',
  'T',
  'M',
  None,
  None],
 [2,
  '1644D161-1113-4C4F-BB2E-BF780E7AE73E',
  2,
  1444146408,
  '932858',
  14
```

يمكن ان نلاحظ ان المفتاح `'meta'` يحتوي على وصف البيانات والعواميد. والمفتاح `'data'` يحتوي على البيانات. يمكننا استخدام هذه المعلومه لإنشاء ال DataFrame:

```python
# تحميل البيانات من JSON
# ثم تسمية العواميد
stops = pd.DataFrame(
    stops_dict['data'],
    columns=[c['name'] for c in stops_dict['meta']['view']['columns']])

stops
```

<div class="table-wrapper" markdown="block">

**Location - Longitude**|**Location - Latitude**|**Dispositions**|**Incident Type**|**...**|**created\_at**|**position**|**id**|**sid**| |
:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:
None|None|M|T|...|1444146408|1|29A1B912-A0A9-4431-ADC9-FB375809C32E|1|0
None|None|M|T|...|1444146408|2|1644D161-1113-4C4F-BB2E-BF780E7AE73E|2|1
None|None|M|T|...|1444146408|3|5338ABAB-1C96-488D-B55F-6A47AC505872|3|2
...|...|...|...|...|...|...|...|...|...
None|None|BM2TWN;|T|...|1496269085|31079|C2B606ED-7872-4B0B-BC9B-4EF45149F34B|31079|29205
-122.2865508|37.8698757|HM4TCS;|T|...|1496269085|31080|8FADF18D-7FE9-441D-8709-7BFEABDACA7A|31080|29206
-122.2565294|37.86720754|AR;|1194|...|1496269085|31081|F60BD2A4-8C47-4BE7-B1C6-4934BE9DF838|31081|29207

</div>


```center-result
29208 rows × 15 columns
```

```python
# طباعة اسماء العواميد
stops.columns
```

```ruby
Index(['sid', 'id', 'position', 'created_at', 'created_meta', 'updated_at',
       'updated_meta', 'meta', 'Incident Number', 'Call Date/Time', 'Location',
       'Incident Type', 'Dispositions', 'Location - Latitude',
       'Location - Longitude'],
      dtype='object')
```

يحتوي الموقع الذي حصلنا على البيانات منه على المعلومات التاليه عن العواميد:

**النوع**|**الوصف**|**العامود**
:-----:|:-----:|:-----:
نص|رقم المخالفه، تم توليد هذا الرقم بشكل تلقائي بواسطة برنامج (CAD)|Incident Number
تاريخ + وقت|تاريخ ووقت المخالفه - الإيقاف|Call Date/Time
نص|العنوان العام للمخالفه - الإيقاف|Location
نص|نوع المخالفه يتم توليده اوتوماتيكياً بواسطة (CAD). الكود (T) يعني ايقاف سياره من قبل الشرطه. ايقاف سياره مشبوهه رمز لها بالكود (1196). ايقاف مشاه رمز له (1194). ايقاف دراجه بالرمز (1194B)|Incident Type
نص|تصرفات ومعلومات عن الموقوف، مرتبة بالترتيب: أول حرف يعني العرق ، وهو كالتالي: A (اسيوي) B (اسود) H (اسباني) O (آخر) W (أبيض) . الحرف الثاني يعني الجنس: F (أنثى) M (ذكر). الحرف الثالث يعني مدى العمر وهو كالتالي: 1 (اقل من 18) 2 (بين 18-29) 3 (بين 30-39), 4 (أكبر من 40). الحرف الرابع يعني السبب: I (تحقيق) T (ايقاف مروري) R (اشتباه) K (افراج مشروط) W (مطلوب). الحرف الخامس يعني ما تم تطبيقه على الموقوف: A (اعتقال) C (مخالفه) O (اخرى) W (انذار). الحرف السادس يوضح اذا تم البحث في السياره: S (تم البحث) N (لم يتم البحث). معلومات اخرى قد تظهر، منها: P - تقرير الحاله الأولي. M - سرد عن طريق الهاتف فقط. AR - تقرير اعتقال فقط (لم يتم تسليم تقرير القضيه). IN - تقرير حادثه. FC - بطاقه ميدانيه. CO - تقرير حادثة تصادم. MH - تقييم نفسي عاجل. TOW - سياره محجوزه. 0 أو 00000 – الشرطي اوقف لأكثر من خمس اشخاص|Dispositions
رقم|خط العرض العام لموقع المكالمه. هذه المعلومه فقط تم البدء باضافتها في يناير 2017.|Location - Latitude
رقم|خط العرض العام لموقع المكالمه. هذه المعلومه فقط تم البدء باضافتها في يناير 2017.|Location - Longitude

نلاحظ ان الموقع لا يحتوي على وصف لأول ثمانية عواميد لجدول `stops`. كون هذه العواميد تحتوي على بيانات وصفيه `metadata` لسنا مهتمين بتحليلها في الوقت الحالي. سنقوم بحذف هذه العواميد من الجدول:

```python
columns_to_drop = ['sid', 'id', 'position', 'created_at', 'created_meta',
                   'updated_at', 'updated_meta', 'meta']

# هذه الداله تستقبل DataFrame
# وتقوم بحذف العواميد غير المرغوبه منها
# تسجل هذه العواميد في المتغير الذي سبق تعريفه columns_to_drop
def drop_unneeded_cols(stops):
    return stops.drop(columns=columns_to_drop)

stops = stops.pipe(drop_unneeded_cols)
```

<div class="table-wrapper" markdown="block">

**Location - Longitude**|**Location - Latitude**|**Dispositions**|**Incident Type**|**Location**|**Call Date/Time**|**Incident Number**| |
:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:
None|None|M|T|SAN PABLO AVE / MARIN AVE|2015-01-26T00:10:00|2015-00004825|0
None|None|M|T|SAN PABLO AVE / CHANNING WAY|2015-01-26T00:50:00|2015-00004829|1
None|None|M|T|UNIVERSITY AVE / NINTH ST|2015-01-26T01:03:00|2015-00004831|2
...|...|...|...|...|...|...|...
None|None|BM2TWN;|T|UNIVERSITY AVE/6TH ST|2017-04-30T22:59:26|2017-00024245|29205
-122.2865508|37.8698757|HM4TCS;|T|UNIVERSITY AVE / WEST ST|2017-04-30T23:19:27|2017-00024250|29206
-122.2565294|37.86720754|AR;|1194|CHANNING WAY / BOWDITCH ST|2017-04-30T23:38:34|2017-00024254|29207

</div>

```center-result
29208 rows × 7 columns
```

كما في بيانات المكالمات، سنجيب على الأسئله التاليه:
- **هل توجد بيانات مفقوده؟**  
- **هل توجد أي بيانات مفقوده تم تعبئتها؟ ( مثلاً كتابة رقم 999 لعمر مجهول أو 12:00 صباحاً لتاريخ مجهول)؟**  
- **اي جزء من البيانات ادخلت بواسطة اشخاص حقيقين؟**  

### هل توجد بيانات مفقوده ؟

يمكن ان نلاحظ ان هناك الكثير من البيانات المفقوده في عامودي خط الطول والعرض. في وصف البيانات ذكر ان العامودين تم البدء في تعبئتها في يناير 2017: 

```python
# ستظهر لنا الأسطر التي تحتوي على الاقل على قيمه واحده مفقوده
null_rows = stops.isnull().any(axis=1)

stops[null_rows]
```

<div class="table-wrapper" markdown="block">

**Location - Longitude**|**Location - Latitude**|**Dispositions**|**Incident Type**|**Location**|**Call Date/Time**|**Incident Number**||
:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:
None|None|M|T|SAN PABLO AVE / MARIN AVE|2015-01-26T00:10:00|2015-00004825|0
None|None|M|T|SAN PABLO AVE / CHANNING WAY|2015-01-26T00:50:00|2015-00004829|1
None|None|M|T|UNIVERSITY AVE / NINTH ST|2015-01-26T01:03:00|2015-00004831|2
...|...|...|...|...|...|...|...
None|None|BM4IWN;|1194|2180 M L KING JR WAY|2017-04-29T01:59:36|2017-00023764|29078
None|None|M;|1194|6TH/UNI|2017-04-30T12:54:23|2017-00024132|29180
None|None|BM2TWN;|T|UNIVERSITY AVE/6TH ST|2017-04-30T22:59:26|2017-00024245|29205

</div>

```center-result
25067 rows × 7 columns
```

يمكن ان نتحقق من بقية العواميد اذا كانت تحتوي على بيانات مفقوده:

```python
# ستظهر لنا الأسطر التي تحتوي على الاقل على قيمه واحده مفقوده
# بدون التحقق من المفقودات في عامودي خطوط الطول والعرض
null_rows = stops.iloc[:, :-2].isnull().any(axis=1)

df_interact(stops[null_rows])
```

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter5/df_interact_stops.png'>
</p>

```center-result
(63 rows, 7 columns) total
```

نلاحظ أن اكثر البيانات المفقوده تكون في عامود `Dispositions` "تصرفات ومعلومات عن الموقوف". للأسف، لم يتم التوضيح في شرح البيانات سبب فقدان هذه المعلومات. بما ان عدد البيانات المفقوده 63 مقارنه بمجموع البيانات 25000 في الجدول، سنكمل تنظيف البيانات مع الاخذ بالاعتبار ان البيانات المفقوده قد تؤثر على نتائجنا.

### هل توجد أي بيانات مفقوده تم تعبئتها ؟

لا يبدو لدينا ان اي من البيانات المفقوده تم تعبئتها. على عكس بيانات المكالمات، التي تم فيها تقسيم التاريخ والوقت في عواميد من منفصله، في جدول الإيقافات يظهر التاريخ والوقت في عامود واحد.

### اي جزء من البيانات ادخلت بواسطة اشخاص حقيقين ؟

ايضاً، كما في بيانات المكالمات، يبدو ان الكثير من بيانات الإيقافات تم تعبئتها بشكل تلقائي بواسطة الآله او تم اختيارها من قوائم محدده مسبقاً بواسطة اشخاص ( مثل عامود `Incident Type` "نوع المخالفه" ). 

نلاحظ ان عامود العنوان لا يحتوي على بيانات محدده مسبقاً من قوائم. يمكننا التحقق لنرى بعض الأخطاء الإملائيه في كتابة العناوين:

```python
stops['Location'].value_counts()
```

```ruby
2200 BLOCK SHATTUCK AVE            229
37.8693028530001~-122.272234021    213
UNIVERSITY AVE / SAN PABLO AVE     202
                                  ... 
VALLEY ST / DWIGHT WAY               1
COLLEGE AVE / SIXTY-THIRD ST         1
GRIZZLY PEAK BLVD / MARIN AVE        1
Name: Location, Length: 6393, dtype: int64
```

كما يظهر، يبدو أنه في مرات تم ادخال العنوان كاملاً، او التقاطعات، ومرات اخرى تم ادخال خطوط الطول والعرض. للأسف ليس لدينا بيانات كامله لخطوط الطول والعرض لإستخدامها بدلاً من العناوين في هذا العامود. قد نحتاج لتنظيف هذا العامود يدوياً اذا ما اردنا تحليل بياناته.

يمكننا أيضاً التحقق من عامود `Dispositions`:

```python
dispositions = stops['Dispositions'].value_counts()

# يظهر لنا جدول تفاعلي بالأنواع المختلفه لتصرفات ومعلومات الموقوف
interact(lambda row=0: dispositions.iloc[row:row+7],
         row=(0, len(dispositions), 7))
```

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter5/df_interact_stops_dspositions.png'>
</p>

يحتوي العامود على الكثير من التناقضات والاختلافات في تسجيل البيانات. مثلاً، في بعض الأحيان يتم اضافة مسافه قبل ادخال البيانات، وبعض المرات يتم اضافة فاصلة منقوطة في النهايه. بعض الأسطر ايضاً تحتوي على اكثر من ادخال. التنوع في البيانات يوضح انها قد تكون ادخلت بواسطة اشخاص حقيقين وليس الآله، لذا يجب علينا الحذر عند العمل عليها:

```python
# امثله على قيم تحتوي على مشاكل
dispositions.iloc[[0, 20, 30, 266, 1027]]
```

```ruby
M           1683
M;           238
 M           176
HF4TWN;       14
 OM4KWS        1
Name: Dispositions, dtype: int64
```

ايضاً، اكثر القيم تكراراً هو `M` وهو كقيمه أولاً لا تعتبر صحيحه لأنه ليس من قائمة الخيارات في العرق. هذا يقترح ان تم تغير طريقة ادخال البيانات في هذا العامود بعد فتره من بدأ الأدخال، أو انه مسموح للشرطي ادخال قيم من دون التحقق ان كانت تطابق شروط الادخال في العامود. على اية حال، سيشكل العمل على هذا العامود تحدي كبير.

يمكننا البدء بخطوات بسيطه لتنظيف هذا العامود عن طريق حذف المسافات الفارغه ان كانت في البدايه او النهايه، حذف الفواصل المنقوطه اذا كانت في آخر الجمله، واذا تكررت الفاصه المنقوطه في نفس السطر نستبدها بفاصله عاديه:

```python
def clean_dispositions(stops):
    cleaned = (stops['Dispositions']
               .str.strip()             # حذف المسافات الفارغه ان كانت في البدايه او النهايه
               .str.rstrip(';')         # حذف الفواصل المنقوطه اذا كانت في آخر الجمله
               .str.replace(';', ','))  # تبديل باقي الفواصل المنقوطه إلى فواصل
    return stops.assign(Dispositions=cleaned)
```

```python
stops_final = (stops
               .pipe(drop_unneeded_cols)
               .pipe(clean_dispositions))
df_interact(stops_final)
```

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter5/df_interact_stops_final.png'>
</p>

```center-result
(29208 rows, 7 columns) total
```

### الملخص

كما رأينا، كلا الملفين وضحت كيف ان تنظيف البيانات احياناً يكون صعباً ومملاً. تنظيف البيانات بشكل كامل يأخذ وقتاً طويل، ولكن عدم تنظيفها يد يوصلنا لنتائج خاطئه. عند مواجهة بيانات يجيب أن نحدد خياراتنا ونحقق التوازن في خطوات تنظيف البيانات حتى نحصل على نتائج صحيحه. القرارات التي نتخذها اثناء تنظيف البيانات تأثر على تحليلنا. مثلاً، قررنا عدم تنظيف عامود `Location` في بيانات الإيقافات، لذا يجب ان نحذر اثناء تحليله. يجب علينا تسجيل كل قرار اتخذناه اثناء تنظيف البيانات ليكون مرجع لنا لاحقاً، والأفضل يكون في ملف جوبتر مع الكود البرمجي كي يكونا كليهما امامنا وقت المراجعه.

[pd_pipe]: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.pipe.html