---
title: النزول الإشتقاقي وتحسين النتائج الكمية
show_title: true
chapter_number: 11
chapter_text: الفصل الحادي عشر
chapter_lessons: [[0, 'مقدمة'], [1, 'تقليل الخساره بإستخدام برنامج'], [2, 'النزول الإشتقاقي'], [3, 'التحدب'], [4, 'النزول الإشقاقي العشوائي']]
chapter_sublessons: [
    [],
    ['مشاكل simple_minimize'],
    ['الفكره', 'التحليل', 'تعريف دالة تقليل الخساره', 'تقليل خسارة Huber', 'ملخص النزول الإشتقاقي'],
    ['اكتشاف الحدود الدنيا بالنزول الإشتقاقي', 'تعريف التحدب', 'ملخص التحدب'],
    [['استخدام دالة الخساره MSE'], 'سلوك النزول الإشقاقي العشوائي', 'تعريف دالة للنزول الإشقاقي العشوائي', 'نزول اشتقاقي بدفعات صغيره', 'تعريف دالة للنزول الإشتقاقي بدفعات صغيره', ملخص النزول الإشقاقي العشوائي']
]
layout: default
---

## مقدمة

لإستخدام قاعدة بيانات للتنبؤ والتوقع، يجب علينا تكوين نموذجنا بشكل دقيق وإختيار دالة خساره. مثلاً، بيانات الإكراميات، نموذجنا توقع ان نسبة الإكراميه ثابته لا تتغير. ثم قررنا إستخدام دالة الخطأ التربيعي المتوسط MSE ووجدنا النموذج الأقل خساره.

وجدنا ايضاً ان هناك وصفاً ابسط لدالتي الخساره الخطأ التربيعي المتوسط و متوسط الخطأ الحتمي وهي: المتوسط والوسيط. ولكن، كلما كان نموذجنا ودالة الخساره اكثر تعقيداً لن نستطيع ايجاد وصفاً رياضياً مناسب. مثلاً، دالة Huber لديها خصائص مفيده ولكن صعب تمييزها.

يمكننا استخدام الكمبيوتر لحل هذه المشكل بواسطة النزول الإشتقاقي، طريقه حسابيه لتقليل دوال الخساره.

## تقليل الخساره بإستخدام برنامج

لنعود للنموذج من الفصل السابق:

$$ \theta = C $$

سنستخدم دالة الخساره MSE:

$$ \begin{split}
\begin{aligned}
L(\theta, \textbf{y})
&= \frac{1}{n} \sum_{i = 1}^{n}(y_i - \theta)^2\\
\end{aligned}
\end{split} $$

للتبسيط، سنستخدم البيانات التاليه: $ \textbf{y} = [ 12, 13, 15, 16, 17 ] $. نعلم من خلال تحليلنا للبيانات في الفصل السابق ان قيمة $ \theta $ لدالة الخساره MSE هي المتوسط $ \text{mean}(\textbf{y}) = 14.6 $. لنرى اذا كان بإمكاننا الحصول على نتيجه عند كتابتنا لبرنامج يوجدها.

اذا قمنا بكتابة برنامج بشكل متقن، فبإمكاننا استخدام نفس البرنامج على اي دالة خساره لإيجاد اقل قيمة ل $ \theta $، يشمل ذلك دالة Huber المعقده رياضياً:

$$ \begin{split}
L_\alpha(\theta, \textbf{y}) = \frac{1}{n} \sum_{i=1}^n \begin{cases}
    \frac{1}{2}(y_i - \theta)^2 &  | y_i - \theta | \le \alpha \\
    \alpha ( |y_i - \theta| - \frac{1}{2}\alpha ) & \text{otherwise}
\end{cases}
\end{split} $$

أولاً، نقوم برسم تخطيطي للبيانات. بالجانب الأيمن من الرسم نقوم برسم دالة الخساره MSE لقيم مختلفه ل $ \theta $:

```python
pts = np.array([12, 13, 15, 16, 17])
points_and_loss(pts, (11, 18), mse)
```

> في الكود البرمجي السابق استخدم الكاتب دالة عرفها مسبقاً بأسم points_and_loss، تقبل الدالة ثلاث متغيرات، الأولى هي البيانات. المتغير الثاني هي مقاسات ابعاد x-axis ، والقيمه الأخيره هي نوع دالة الخساره، والتي هي عباره عن داله اخرى عرفها ايضاً. تعريف كلا الدالتين هو كالتالي:
>
> ```python
> def mse(theta, y_vals):
>     return np.mean((y_vals - theta) ** 2)
> 
> def points_and_loss(y_vals, xlim, loss_fn):
>     thetas = np.arange(xlim[0], xlim[1] + 0.01, 0.05)
>     losses = [loss_fn(theta, y_vals) for theta in thetas]
>     
>     plt.figure(figsize=(9, 2))
>     
>     ax = plt.subplot(121)
>     sns.rugplot(y_vals, height=0.3, ax=ax)
>     plt.xlim(*xlim)
>     plt.title('Points')
>     plt.xlabel('Tip Percent')
>     
>     ax = plt.subplot(122)
>     plt.plot(thetas, losses)
>     plt.xlim(*xlim)
>     plt.title(loss_fn.__name__)
>     plt.xlabel(r'$ \theta $')
>     plt.ylabel('Loss')
>     plt.legend()
> ```
> 

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_basics_3_0.png'>
</p>

كيف بإمكاننا برمجة برنامج يقوم بإيجاد اقل قيمة ل $ \theta $ اوتوماتيكياً؟ الطريقه الأسهل هي بحساب قيمة الخساره لأكثر من قيمه ل $ \theta $، ثم نوجد قيمة $ \theta $ ذا الأقل خساره.

عرفنا داله بأسم `simple_minimize` والتي تقبل متغيرات هي دالة الخساره، مصفوفة البيانات، ومصفوفه بقيم $ \theta $:

```python
def simple_minimize(loss_fn, dataset, thetas):
    '''
    القيمه النهائيه لهذه الداله هي قيمة θ من بين عدة قيم من θ ذات الأقل خساره
    '''
    losses = [loss_fn(theta, dataset) for theta in thetas]
    return thetas[np.argmin(losses)]
```

ثم نعرف داله لإيجاد قيمة MSE واستخدامها في دالة `simple_minimize`:

```python
def mse(theta, dataset):
    return np.mean((dataset - theta) ** 2)

dataset = np.array([12, 13, 15, 16, 17])
thetas = np.arange(12, 18, 0.1)

simple_minimize(mse, dataset, thetas)
```

```ruby
14.599999999999991
```

النتيجه هذه قريبه للقيمه المتوقعه:

```python
# ايجاد القيمه بإستخدام المتوسط
np.mean(dataset)
```

```ruby
14.6
```

الآن يمكننا كتابة داله لحساب خسارة Huber ورسمها:

```python
def huber_loss(theta, dataset, alpha = 1):
    d = np.abs(theta - dataset)
    return np.mean(
        np.where(d < alpha,
                 (theta - dataset)**2 / 2.0,
                 alpha * (d - alpha / 2.0))
    )

points_and_loss(pts, (11, 18), huber_loss)
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_basics_12_0.png'>
</p>

على الرغم ان القيم الدنيا ل $ \theta $ يجب ان تكون أقرب إلى 15، ليس لدينا طريقه لتحليل وإيجاد قيمة $ \theta $ لدالة الخساره Huber. بدلاً، من ذلك، سنستخدم الداله `simple_minimize`:

```python
simple_minimize(huber_loss, dataset, thetas)
```

```ruby
14.999999999999989
```

الآن، لنعود لبيانات نسبة الإكراميات ونوجد افضل قيمه ل $ \theta $ بإستخدام Huber:

```python
tips = sns.load_dataset('tips')
tips['pcttip'] = tips['tip'] / tips['total_bill'] * 100
tips.head()
```

**pcttip**|**size**|**time**|**day**|**smoker**|**sex**|**tip**|**total\_bill**| 
:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:
5.944673|2|Dinner|Sun|No|Female|1.01|16.99|0
16.054159|3|Dinner|Sun|No|Male|1.66|10.34|1
16.658734|3|Dinner|Sun|No|Male|3.5|21.01|2
13.978041|2|Dinner|Sun|No|Male|3.31|23.68|3
14.680765|4|Dinner|Sun|No|Female|3.61|24.59|4

<br>
```python
points_and_loss(tips['pcttip'], (11, 20), huber_loss)
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_basics_17_0.png'>
</p>

```python
simple_minimize(huber_loss, tips['pcttip'], thetas)
```

```ruby
15.499999999999988
```

نلاحظ ان عند استخدام دالة خسارة Huber كانت النتيجه $ \hat{\theta} = 15.5 $ . يمكننا الآن مقارنة هذه النتيجه مع MSE و MAE:

```python
print(f"               MSE: theta_hat = {tips['pcttip'].mean():.2f}")
print(f"               MAE: theta_hat = {tips['pcttip'].median():.2f}")
print(f"        Huber loss: theta_hat = 15.50")
```

```ruby
            MSE: theta_hat = 16.08
            MAE: theta_hat = 15.48
    Huber loss: theta_hat = 15.50
```

نلاحظ ان دالة Huber اقرب إلى MAE كونها لا تتأثر بشكل كبير بسبب القيم الشاذه على الجانب الأيمن في الرسم البياني التالي لتوزيع بيانات الإكراميات:

```python
sns.distplot(tips['pcttip'], bins=50);
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_basics_22_0.png'>
</p>

### مشاكل simple_minimize

على الرغم من ان دالة `simple_minimize` تساعدنا على تقليل دالة الخساره، إلا ان لديها بعض المشاكل التي تجعلها غير مفيده للإستخدام بشكل عام. مشكلتها الأهم هي انها تعمل فقط مع قيم $ \theta $. مثلاً، في الكود البرمجي التالي، الذي سبق ان استخدمناه في الأعلى، احتجنا لتعريف قيم $ \theta $ يدوياً من 12 إلى 18:

```python
dataset = np.array([12, 13, 15, 16, 17])
thetas = np.arange(12, 18, 0.1)

simple_minimize(mse, dataset, thetas)
```

كيف وجدنا انه علينا التحقق من القيم بين 12 و 18؟ احتجنا لمراجعة الرسم البياني لدالة الخساره ووجدنا ان القيم الدنيا بين تلك القيمتين. هذه الطريقه غير عمليه لأننا قمنا بإضافة خطوه معقده جديده لنماذجنا. بالإضافه لذلك، قمنا بتعريف قيم الزياده 0.1 بشكل يدوي. ولكن، اذا كانت القيمه المثلى ل $ \theta $ هي 12.043، ستقوم الداله `simple_minimize` بتقريب النتيجه إلى 12.00 كونها الأقرب لمضاعفات 0.1

يمكننا حل تلك المشاكل بطريقه واحده بإستخدام ما يسمى بـ *النزول الإشتقاقي Gradient Descent*.

## النزول الإشتقاقي

### الفكره

### التحليل

### تعريف دالة تقليل الخساره

### تقليل خسارة Huber

### ملخص النزول الإشتقاقي

## التحدب

### اكتشاف الحدود الدنيا بالنزول الإشتقاقي

### تعريف التحدب

### ملخص التحدب

## النزول الإشقاقي العشوائي

#### استخدام دالة الخساره MSE

### سلوك النزول الإشقاقي العشوائي

### تعريف دالة للنزول الإشقاقي العشوائي

### نزول اشتقاقي بدفعات صغيره

### تعريف دالة للنزول الإشتقاقي بدفعات صغيره

### ملخص النزول الإشقاقي العشوائي