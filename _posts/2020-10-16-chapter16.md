---
title: الضبط
show_title: true
chapter_number: 16
chapter_text: الفصل السادس عشر
chapter_lessons: [[0, 'مقدمة'], [1, 'فكرة الضبط'], [2, 'الضبط L2: إنحدار Ridge'], [3, 'الضبط L1: إنحدار الوهق']]
chapter_sublessons: [
    [],
    ['بيانات سد المياة', 'التحقق من المعاملات', 'معْلَمات المعاقبة'],
    [['التعريف', 'متغيرات الضبط', 'إستبعاد مصطلح الإنحياز', 'تسوية البيانات'], 'إستخدام إنحدار Ridge', 'ملخص إنحدار Ridge'],
    ['تعريف الضبط', 'المقارنة بين إنحدار Ridge والوهق', 'اختيار الخصائص مع إنحدار الوهق', 'الجبلي × الوهق عملياً', 'ملخص إنحدار الوهق' ]

]
layout: default
---

## مقدمة

يمكن لهندسة الخصائص ان تقدم لنا معلومات مهمة عن خطوات توليد البيانات في النموذج. ولكن، إضافة الخصائص إلى البيانات عادة ما يزيد من التباين في نموذجنا ويمكن لذك ان يسيء من أداءة بشكل عام. بدلاً من إنشاء الخصائص بشكل عشوائي، يمكننا إستخدام طريقة تسمى بـ **الضبط Regularization** للتقليل من التباين في نموذجنا مع الإستمرار بتقديم أكبر قدر من المعلومات عن البيانات.

## فكرة الضبط

لنبدأ النقاش عن الضبط بإستخدام المثال التالي لتوضيح أهميته.

### بيانات سد المياة

البيانات التاليه توضح كمية المياة باللتر التي تتدفق من سد المياة في اليوم و قيمة التغير في مستوى المياة في نفس اليوم بالمتر.

> لتحميل البيانات water_large.csv [اضغط هنا]({{ site.baseurl }}/files/chapter16/water_large.csv).

```python
df = pd.read_csv('water_large.csv')
df
```

**water\_flow**|**water\_level\_change**| 
:-----:|:-----:|:-----:
6.04E+10|-15.936758|0
3.32E+10|-29.152979|1
9.73E+11|36.189549|2
...|...|...
2.36E+11|7.08548|20
1.49E+12|46.282369|21
3.78E+11|14.612289|22

```ruby
23 rows × 2 columns
```

عند رسم البيانات نلاحظ الإتجاه التصاعدي لتدفق المياه كلما كانت مستويات المياة إيجابية:

```python
df.plot.scatter(0, 1, s=50);
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter16/reg_intuition_8_0.png'>
</p>

لنمذجة هذا النمط، يمكننا إستخدام نموذج الإنحدار الخطي للمربعات الصغرى Least Square Linear Regression. يظهر في الرسم البياني التالي البيانات وتوقعات النموذج:

```python
df.plot.scatter(0, 1, s=50);
plot_curve(curves[0])
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter16/reg_intuition_10_0.png'>
</p>

> في الكود البرمجي السابق إستخدم الكاتب الدالة `plot_curve` والمصفوفة `curves` للمساعدة في الرسم، وعرفها كالتالي مع دالة مساعدة أخرى `make_curve`:
>
> ```python
>
> import matplotlib.pyplot as plt
> import numpy as np
> import pandas as pd
> from sklearn.pipeline import Pipeline
> from sklearn.preprocessing import PolynomialFeatures
> from sklearn.linear_model import LinearRegression
> from collections import namedtuple
>
> Curve = namedtuple('Curve', ['xs', 'ys'])
>
> def make_curve(clf, x_start=-50, x_end=50):
>     xs = np.linspace(x_start, x_end, num=100)
>     ys = clf.predict(xs.reshape(-1, 1))
>     return Curve(xs, ys)
>
> def plot_curve(curve, ax=plt, **kwargs):
>     ax.plot(curve.xs, curve.ys, **kwargs)
> 
> X = df.iloc[:, [0]].as_matrix()
> y = df.iloc[:, 1].as_matrix()
> 
> degrees = [1, 2, 8, 12]
> clfs = [Pipeline([('poly', PolynomialFeatures(degree=deg, include_bias=False)),
>                   ('reg', LinearRegression())])
>         .fit(X, y)
>         for deg in degrees]
>
> curves = [make_curve(clf) for clf in clfs]
> ```

يظهر في الرسم البياني السابق أن هذا النموذج لا يغطي كامل النقاط في نمط هذه البيانات؛ والنموذج لدية إنحياز عالي. كما فعلنا في السابق، يمكننا حل هذه المشكلة بإضافة خصائص متعددة الحدود للبيانات. نقوم بإضافة خصائص من الدرجة 2، 8 و 12؛ الرسوم البيانية التالية تظهر نتائج تدريب البيانات لكل نموذج:

```python
plot_curves(curves)
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter16/reg_intuition_12_0.png'>
</p>

> إستخدم الكاتب دالة `plot_curves` لنفس المصفوفة المستخدمة في المثال السابق `curves` للمساعدة في رسم النماذج، وعرفها كالتالي مع مزيد من الدوال المساعدة:
>
> ```python
> 
> def flatten(seq): return [item for subseq in seq for item in subseq]
>
> def plot_data(df=df, ax=plt, **kwargs):
>     ax.scatter(df.iloc[:, 0], df.iloc[:, 1], s=50, **kwargs)
>
> def plot_curves(curves, cols=2):
>     rows = int(np.ceil(len(curves) / cols))
>     fig, axes = plt.subplots(rows, cols, figsize=(10, 8),
>                              sharex=True, sharey=True)
>     for ax, curve, deg in zip(flatten(axes), curves, degrees):
>         plot_data(ax=ax, label='Training data')
>         plot_curve(curve, ax=ax, label=f'Deg {deg} poly')
>         ax.set_ylim(-5e10, 170e10)
>         ax.legend()
>         
>     # إضافة الرسوم الكبيرة وإخفاء الإطارات
>     fig.add_subplot(111, frameon=False)
>     # إخفاء الحدود والعلامات والعناوين في الرسم
>     plt.tick_params(labelcolor='none', top='off', bottom='off',
>                     left='off', right='off')
>     plt.grid(False)
>     plt.title('Polynomial Regression')
>     plt.xlabel('Water Level Change (m)')
>     plt.ylabel('Water Flow (Liters)')
>     plt.tight_layout()
> ```

كما هو متوقع، متعددة الحدود من الدرجة الـ 12 طابقت بيانات التدريب بشكل مناسب ولكن يبدو ان النموذج مضبوط بنمط زائف على البيانات بسبب التشويش. يظهر ذلك نموذج آخر للمقايضة بين الإنحياز والتباين؛ النموذج الخطي لدية إنحياز عالي وتباين قليل بينما متعددة الحدود من الدرجة 12 لديها إنحياز قليل وتبيان عالي.

### التحقق من المعاملات

عند التحقق من معاملات نموذج متعددة الحدود من الدرجة الـ 12 يظهر لنا أن النموذج يقوم بالتوقع بناءًا على المعادلة التالية:

$$ \begin{split}
207097470825 + 1.8x + 482.6x^2 + 601.5x^3 + 872.8x^4 + 150486.6x^5 \\
    + 2156.7x^6 - 307.2x^7 - 4.6x^8 + 0.2x^9 + 0.003x^{10} - 0.00005x^{11} + 0x^{12}
\end{split} $$

فيها $ x $ هي قيمة تغير مستوى المياة في ذلك اليوم.

معاملات النموذج كبيرة جداً، خاصة لمتعددات الحدود ذات الدرجات العالية والتي تساهم بشكل عالي في تباين النموذج ( مثلاً $ x^5 $ و $ x^6 $ ).

### معْلَمات المعاقبة

لنتذكر أن النموذج الخطي يقوم بالتوقعات بناءاً على التالي، $ \theta $ هي وزن النموذج و $ x $ هي متّجهة الخصائص:

$$ f_\hat{\theta}(x) = \hat{\theta} \cdot x $$

لضبط النموذج، نقوم بتقليل دالة التكلفة لمتوسط الخطأ التربيعي، فيها $ X $ تمثل مصفوفة البيانات و $ y $ هي النتائج التي إطلعت عليها:

$$ \begin{split}
\begin{aligned}
L(\hat{\theta}, X, y)
&= \frac{1}{n} \sum_{i}(y_i - f_\hat{\theta} (X_i))^2\\
\end{aligned}
\end{split} $$

لتقليل التكلفة في المعادلة السابقة، نقوم بضبط $ \hat{\theta} $ حتى نصل لأفضل قيم للأوزان Weights أياً كان حجمها. لكن، وجدنا أن كلما كان الوزن عالي وبخصائص اكثر تعقيداً ينتج لنا تباين عالي للنموذج. على العكس، إذا كان بأمكاننا التعديل في دالة التكلفة لمعاقبة الأوزان العالية، سيكون النموذج الناتج ذو تباين قليل. نستخدم الضبط للقيام بذلك.

## الضبط L2: إنحدار Ridge

في هذا الجزء سنتعرف على طريقة الضبط $ L_2 $، طريقة لمعاقبة الأوزان الكبيرة في دوال التكلفة للتقليل من تباين النموذج. راجعنا بشكل مبسط الإنحدار الخطي، ثم تعرفنا على الضبط كأداة للتعديل في دالة التكلفة.

لتطبيق الإنحدار الخطي للمربعات الصغرى، نستخدم النموذج:

$$ f_\hat{\theta}(x) = \hat{\theta} \cdot x $$ 

نقوم بضبط النموذج عن طريق التقليل دالة تكلفة متوسط الخطأ التربيعي:

$$ \begin{split}
\begin{aligned}
L(\hat{\theta}, X, y)
&= \frac{1}{n} \sum_{i}^n(y_i - f_\hat{\theta} (X_i))^2\\
\end{aligned}
\end{split} $$

في التعريف السابق، $ X $ تمثل مصفوفة البيناات $ n \times p $، و $ x $ تمثل سطر من أسطر $ X $، و $ y $ تمثل النتائج التي أطلع عليها، و $ \hat{\theta} $ هي وزن النموذج.
### التعريف

لإضافة الضبط $ L_2 $ إلى النموذج، نقوم بتعديل دالة التكلفة السابقة:

$$ \begin{aligned}
L(\hat{\theta}, X, y)
&= \frac{1}{n} \sum_{i}(y_i - f_\hat{\theta} (X_i))^2
    + \lambda \sum_{j = 1}^{p} \hat{\theta_j}^2
\end{aligned} $$ 

نلاحظ أن دالة التكلفة مشابهه للسابقة مع إضافة الضبط $ L_2 $ في المعادلة $ \lambda \sum_{j = 1}^{p} \hat{\theta_j}^2 $. المجموع ( $ \sum $ ) في هذه المعادلة يقوم بجمع تربيع كل وزن في النموذج $ \hat{\theta_1}, \hat{\theta_2}, \ldots, \hat{\theta_p} $. يظهر لنا في المعادلة أيضاً رمز جديد لمتغير رقمي في النموذج $ \lambda $ والذي يتم تعديله لتحديد قيمة **عقوبة الضبط Regularization Penalty**.

معادلة الضبط الذي اضفناها تسبب زيادة في التكلفة إذا كانت القيم في $ \hat{\theta} $ بعيدة عن صفر. مع إضافة الضبط، الوزن للنموذج المثالي يقلل من الخسارة ومعاقبة الضبط بدلاً من الخسارة فقط. بما أن نتائج أوزان النموذج عادة ما تكون قليلة، فسيكون النموذج ذو تبيان قليل وإنحياز عالي.

إستخدام الضبط $ L_2 $ مع نموذج خطي ودالة التكلفة لمتوسط الخطأ التربيعي يطلق عليها **إنحدار Ridge**. [📝][RidgeRegression]

#### متغيرات الضبط

يتحكم متغير الضبط $ \lambda $ بعقوبة الضبط. عندما تكون $ \lambda $ قليلة ينتج قيمة عقاب قليل، إذا كانت $ \lambda = 0 $ تكون قيمة معاقبة الضبط ايضاً $ 0 $ وبالتالي لم يتم ضبط دالة التكلفة.

عندما تكون $ \lambda $ عالية ينتج لنا قيمة عقابل عاليه وبالتالي يكون النموذج بسيط. الزيادة في $ \lambda $ تقلل من التباين وتزيد من الإنحياز في النموذج. نستخدم التحقق المتقاطع لإختيار قيمة $ \lambda $ التي تقلل من خطأ التحقق.

**ملاحظة عن الضبط في مكتبة `scikit-learn`**:

توفر مكتبة `scikit-learn` نموذج الإنحدار ومضاف إليها بشكل تلقائي الضبط. مثلاً، لتطبيق إنحدار Ridge يمكنك إستخدام النموذج الخطي [`sklearn.linear_model.Ridge`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html). لاحظ أن جميع النماذج في مكتبة `scikit-learn` تطلق على متغير الضبط `alpha` بدلاً من $ \lambda $.

توفر مكتبة `scikit-learn` بشكل بسيط ومناسب نماذج مضبوطة تقوم بتطبيق التحقق المتقاطع لإختيار قيمة مناسبة لـ $ \lambda $. مثلاً، [`sklearn.linear_model.RidgeCV`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html#sklearn.linear_model.RidgeCV) يسمح للمستخدم إدخال قيم متغيرات الضبط وبشكل أوتوماتيكي تقوم بتطبيق التحقق المتقاطع لإختيار افضل قيمة للمتغير والذي يحصل على أقل خطأ في بيانات التحقق.

#### إستبعاد مصطلح الإنحياز

لاحظ أن رمز الإنحياز $ \theta_0 $ لم يتم ضمة لعملية الجمع في معادلة الضبط. لا نقوم بمعاقبة الإنحياز لأن الزيادة في إنحياز النموذج لا تأدي للزيادة في التباين؛ ببساطة يقوم الإنحياز بتغير جميع التوقعات إلى قيم ثابته.

#### تسوية البيانات

لاحظ ان معادلة الضبط $ \lambda \sum_{j = 1}^{p} \hat{\theta_j}^2 $ تعاقب كل $ \hat{\theta_j} $ بشكل متكافئ. ولكن، تأثير كل قيمة من $ \hat{\theta_j} $ يختلف بناءاً على البيانات نفسها. لنأخذ مثلاً هذه الجزء من بيانات تدفق المياة بعد إضافة متغيرات متعددة الحدود من الدرجة الثامنه:

```python
pd.DataFrame(clfs[2].named_steps['poly'].transform(X[:5]),
             columns=[f'deg_{n}_feat' for n in range(8)])
```

**deg\_7\_feat**|**deg\_6\_feat**|**...**|**deg\_1\_feat**|**deg\_0\_feat**| 
:-----:|:-----:|:-----:|:-----:|:-----:|:-----:
4161020472|-261095791.1|...|253.98|-15.94|0
5.21751E+11|-17897014962|...|849.9|-29.15|1
2.94215E+12|81298431147|...|1309.68|36.19|2
3.90415E+12|1.04132E+11|...|1405.66|37.49|3
2.84568E+13|-5.92124E+11|...|2309.65|-48.06|4

```ruby
5 rows × 8 columns
```

نلاحظ أن متعددة الحدود من الدرجة السابعه حصلت على قيم أعلى بكثير عن متعددة الحدود من الدرجة الأولى. يعني ذلك أن الوزن العالي في نموذج متعددة الحدود من الدرجة السابعة أثر على التوقعات أكثر من الوزن عالي لنموذج متعددة الحدود من الدرجة الاولى. إذا قمنا بتطبيق الضبط على هذه البيانات مباشرة، ستقوم معاقبة الضبط بتقليل الوزن للنموذج بشكل غير مناسب للخصائص في الدرجات الصغرى. بشكل عملي، ينتج عن ذلك تباين عالي للنموذج حتى بعد تطبيق الضبط لأن الخصائص ذات التأثير الأكبر على التوقعات لن تتأثر.

لحل ذلك، نقوم بـ **التسوية Normalization** جميع الأعمدة عن طريق طرح المتوسط منها وتغير القيم فيها إلى شكل مدرج Scale بين -1 و 1. في مكتبة `scikit-learn` أغلب نماذج الإنحدار تسمح بتطيق التسوية بإستخدام المتغير `normalize=True` للقيام بتسوية البيانات قبل الضبط. [📝][StandardizationNormalization] [📝][StandardizationNormalization2]

طريقة أخرى مشابهة للتسوية هي **التوحيد Standardization** لأعمدة البيانات عن طريق طرح المتوسط وتقسيمه على الانحراف المعياري لكل عامود.

### إستخدام إنحدار Ridge

إستخدمنا سابقاً خصائص متعددة الحدود لضبط متعددات الحدود من الدرجة 2، 8 و 12 لبيانات تدفق المياة. البيانات الأساسية ونتائج توقعات النموذج كالتالي:

```python
df
```

**water\_flow**|**water\_level\_change**| 
:-----:|:-----:|:-----:
60422330446|-15.94|0
33214896576|-29.15|1
9.72706E+11|36.19|2
...|...|...
2.36352E+11|7.09|20
1.49426E+12|46.28|21
3.78146E+11|14.61|22

```ruby
23 rows × 2 columns
```

```python
plot_curves(curves)
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter16/reg_ridge_15_0.png'>
</p>

لتطبيق إنحدار Ridge، نقوم اولاً بإستخراج مصفوفة البيانات و متّجهة النتائج من البيانات:

```python
X = df.iloc[:, [0]].as_matrix()
y = df.iloc[:, 1].as_matrix()
print('X: ')
print(X)
print()
print('y: ')
print(y)
```

```ruby
X: 
[[-15.94]
 [-29.15]
 [ 36.19]
 ...
 [  7.09]
 [ 46.28]
 [ 14.61]]

y: 
[6.04e+10 3.32e+10 9.73e+11 ... 2.36e+11 1.49e+12 3.78e+11]
```

ثم نقوم بتحويل القيم في `X` إلى متعددة الحدود من الدرجة 12:

```python
from sklearn.preprocessing import PolynomialFeatures

# نريد تحديد include_bias=False
# لأن مصنفات مكتبة sklearns
# تقوم بشكل تلقائي بإضافة قيم الإنحياز

X_poly_8 = PolynomialFeatures(degree=8, include_bias=False).fit_transform(X)
print('First two rows of transformed X:')
print(X_poly_8[0:2])
```

```ruby
First two rows of transformed X:
[[-1.59e+01  2.54e+02 -4.05e+03  6.45e+04 -1.03e+06  1.64e+07 -2.61e+08
   4.16e+09]
 [-2.92e+01  8.50e+02 -2.48e+04  7.22e+05 -2.11e+07  6.14e+08 -1.79e+10
   5.22e+11]]
```

نقوم بتحديد قيم `alpha` التي ستختارها مكتبة `scikit-learn` بإستخدام التحقق المتقاطع، ثم نستخدم نموذج `RidgeCV` لضبط البيانات بعد التحويل:

```python
from sklearn.linear_model import RidgeCV

alphas = [0.01, 0.1, 1.0, 10.0]

# تذكر ان تحدد normalize=True
# ليتم تسوية البيانات
clf = RidgeCV(alphas=alphas, normalize=True).fit(X_poly_8, y)

# إظهار قيمة Alpha
clf.alpha_
```

```ruby
0.1
```

أخيراً، نقوم برسم نتائج التوقعات لنموذج متعددة الحدود من الدرجة الثامنة بجانب النموذج من الدرجة الثامنة الذي تم ضبطة:

```python
fig = plt.figure(figsize=(10, 5))

plt.subplot(121)
plot_data()
plot_curve(curves[2])
plt.title('Base degree 8 polynomial')

plt.subplot(122)
plot_data()
plot_curve(ridge_curves[2])
plt.title('Regularized degree 8 polynomial')
plt.tight_layout()
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter16/reg_ridge_23_0.png'>
</p>


> إستخدم الكاتب نفس الطريقة السابقة لرسم إنحدار Ridge، عن طريق إنشاء مصفوفة أسماها `ridge_curves` وعرفها كالتالي:
>
> ```python
> from sklearn.linear_model import Ridge
> 
> ridge_clfs = [Pipeline([('poly', PolynomialFeatures(degree=deg, include_bias=False)),
>                         ('reg', Ridge(alpha=0.1, normalize=True))])
>         .fit(X, y)
>         for deg in degrees]
> 
> ridge_curves = [make_curve(clf) for clf in ridge_clfs]
> ```

نلاحظ أن متعددة الحدود التي تم ضبطها منحناها أكثر سلاسة وتمر على أغلب نقاط البيانات.

بمقارنة المعامِلات للنموذج الذي تم ضبطة والذي لم يتم ضبطة نلاحظ أن إنحدار Ridge يفضل وضح وزن للنموذج على متعددات الحدود ذو الدرجات الدنيا:

```python
base = coef_table(clfs[2]).rename(columns={'Coefficient Value': 'Base'})
ridge = coef_table(ridge_clfs[2]).rename(columns={'Coefficient Value': 'Regularized'})

pd.options.display.max_rows = 20
display(base.join(ridge))
pd.options.display.max_rows = 7
```

**Regularized**|**Base**| 
:-----:|:-----:|:-----:
 | |degree
2.21064E+11|2.25782E+11|0
6846139066|13115217771|1
146158038|-144725750|2
1930090.04|-10355082.91|3
38240.62|567935.23|4
564.21|9805.14|5
7.25|-249.64|6
0.18|-2.09|7
0|0.03|8

> استخدم الكاتب دالتين `coef_table` و `coefs` تقومان بإنشاء المُعامِلات على شكل جدول في DataFrame، وتعريفها كالتالي:
>
> ```python
>
> def coefs(clf):
>     reg = clf.named_steps['reg']
>     return np.append(reg.intercept_, reg.coef_)
> 
> def coef_table(clf):
>     vals = coefs(clf)
>     return (pd.DataFrame({'Coefficient Value': vals})
>             .rename_axis('degree'))
> ```

إعادة تطبيق نفس العملية على متعددة الحدود من الدرجة 12 يظهر نتائج مماثلة:

```python
fig = plt.figure(figsize=(10, 5))

plt.subplot(121)
plot_data()
plot_curve(curves[3])
plt.title('Base degree 12 polynomial')
plt.ylim(-5e10, 170e10)

plt.subplot(122)
plot_data()
plot_curve(ridge_curves[3])
plt.title('Regularized degree 12 polynomial')
plt.ylim(-5e10, 170e10)
plt.tight_layout()
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter16/reg_ridge_27_0.png'>
</p>

نحصل على نموذج أكثر بساطة كلما قمنا في الزيادة في متغير الضبط. الرسم البياني التالي يوضح تأثير الزيادة في قيمة الضبط من 0.001 حتى 100.0:

```python
alphas = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]

alpha_clfs = [Pipeline([
    ('poly', PolynomialFeatures(degree=12, include_bias=False)),
    ('reg', Ridge(alpha=alpha, normalize=True))]
).fit(X, y) for alpha in alphas]

alpha_curves = [make_curve(clf) for clf in alpha_clfs]
labels = [f'$\\lambda = {alpha}$' for alpha in alphas]

plot_curves(alpha_curves, cols=3, labels=labels)
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter16/reg_ridge_29_0.png'>
</p>

> إستخدمت الدالة `plot_curves` للمساعدة على رسم المنحنيات، وتعريفها كالتالي:
>
> ```python
>
> def plot_curves(curves, cols=2, labels=None):
>    if labels is None:
>        labels = [f'Deg {deg} poly' for deg in degrees]
>    rows = int(np.ceil(len(curves) / cols))
>    fig, axes = plt.subplots(rows, cols, figsize=(10, 8),
>                             sharex=True, sharey=True)
>    for ax, curve, label in zip(flatten(axes), curves, labels):
>        plot_data(ax=ax, label='Training data')
>        plot_curve(curve, ax=ax, label=label)
>        ax.set_ylim(-5e10, 170e10)
>        ax.legend()
>        
>    # إضافة الرسوم الكبيرة وإخفاء الإطارات
>    fig.add_subplot(111, frameon=False)
>    # إخفاء الحدود والعلامات والعناوين في الرسم
>    plt.tick_params(labelcolor='none', top='off', bottom='off',
>                    left='off', right='off')
>    plt.grid(False)
>    plt.title('Polynomial Regression')
>    plt.xlabel('Water Level Change (m)')
>    plt.ylabel('Water Flow (Liters)')
>    plt.tight_layout()
> ```

كما نرى، الزيادة في متغير الضبط تزيد من إنحياز النموذج. إذا كان متغير الضبط عالي جداً، يصبح النموذج نموذجاً ثابتاً لأن أي قيمة لوزن النموذج غير صفر ينتجى عنها عقاب عالي.
### ملخص إنحدار Ridge

إستخدام ضبط $ L_2 $ يسمح لنا بضبط إنحياز وتباين النموذج عن طريق عقاب أوزان النموذج العالية. ضبط $ L_2 $ للإنحدار الخطي للمربعات الصغرى يعرف أيضاً بإسم إنحدار Ridge. إستخدام الضبط يضيف متغير إضافي للنموذج يرمز له $ \lambda $ والذي نقوم بضبطة بإستخدام التحقق المتقاطع.

## الضبط L1: إنحدار الوهق

### تعريف الضبط

### المقارنة بين إنحدار Ridge والوهق

### اختيار الخصائص مع إنحدار الوهق

### الجبلي × الوهق عملياً

### ملخص إنحدار الوهق




[RidgeRegression]: https://www.youtube.com/watch?v=Q81RR3yKn30
[StandardizationNormalization]: https://www.kdnuggets.com/2020/04/data-transformation-standardization-normalization.html
[StandardizationNormalization2]: https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/