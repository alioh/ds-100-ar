---
title: المقايضة بين الإنحياز والتباين
show_title: true
chapter_number: 15
chapter_text: الفصل الخامس عشر
chapter_lessons: [[0, 'مقدمة'], [1, 'تقليل المخاطر والخسائر'], [2, 'إنحياز وتباين النموذج'], [3, 'التحقق المتقاطع']]
chapter_sublessons: [
    [],
    ['المخاطر', 'الخطر التجريبي', 'ملخص المخاطر'],
    ['تحليل الإنحياز والتباين', 'مشتقة تحليل الإنحياز والتبيان', 'مثال: الإنحدار الخطي وموجات الجيب Sine Waves', 'التطبيق العملي للإنحياز والتباين', 'النقاط الأساسية', 'ملخص إنحياز وتباين النموذج'],
    [['تقسيم بيانات التدريب والتحقق والإختبار', 'حجم تقسيم بيانات التدريب والتحقق والإختبار'],'خطأ التدريب و خطأ الإختبار', 'التحقق المتقاطع K-Flod', 'مقايضة الإنحياز والتباين', 'مثال: اختيار النموذج لبيانات تقييم الآيس كريم', 'ملخص التحقق المتقاطع']
]
layout: default
---

## مقدمة

في بعض الأحيان، نختار نموذج بسيط جداً ليمثل البيانات. وفي بعض المرات، نختار نموذجاً معقد، بسبب ضبطنا على الضوضاء في البيانات بدلاً من البيانات نفسها.

لفهم سبب قيامنا بذلك، نقوم بتحليل النموذج بإستخدام أدوات الإحتمالات والإحصاء. هذه الأدوات تسمح لنا بالتعميم من أمثله بسيطة محدودة حتى وصف المهام الأساسية في النمذجة. بشكل خاص، سنستخدم **التوقع Expectation** و **التباين Variance**  لعرض وفهم المقايضة بين الإنحياز والتباين.

## تقليل المخاطر والخسائر

للقيام بالتوقع بإستخدام البيانات، نقوم بتعريف نموذج، إختيار دالة خساره لجميع البيانات، وضبط النموذج مع المتغيرات عن طريق تقليل الخساره. مثلاً،  للقيام بالإنحدار الخطي للمربعات الصغرى، نختار النموذج:

$$ \begin{aligned}
f_\hat{\theta} (x) &= \hat{\theta} \cdot x
\end{aligned} $$

ودالة الخساره:

$$ \begin{split}
\begin{aligned}
L(\hat{\theta}, X, y)
&= \frac{1}{n} \sum_{i}(y_i - f_\hat{\theta} (X_i))^2\\
\end{aligned}
\end{split} $$

كما فعلنا مسبقاً، نستخدم $ \hat{\theta} $ كمصفوفة لمتغيرات النموذج، و $ x $ متّجه تحتوي على سطر من مصفوفة البيانات $ X $، و $ y $ هي مصفوفة للبيانات التي تعلم عليها لتساعد على التوقع. $ X_i $ هي السطر $ i $ من المصفوفة $ X $ و $ y_i $ هي النتيجة رقم $ i $ في المصفوفة $ y $.

لاحظ أن الخساره لكامل البيانات هي متوسط نتائج دالة الخساره لكل سطر فيها. إذا قمنا بتعريف دالة الخسارة التربيعيه:

$$ \begin{aligned}
\ell(y_i, f_\hat{\theta} (x))
&= (y_i - f_\hat{\theta} (x))^2
\end{aligned} $$

إذاً، يمكننا إعادة كتابة متوسط الخساره بشكل أبسط:

$$ \begin{aligned}
L(\hat{\theta}, X, y)
&= \frac{1}{n} \sum_{i} \ell(y_i, f_\hat{\theta} (X_i))
\end{aligned} $$

التعريف في الأعلى يختصر فكرة دالة الخساره؛ إياً كانت دالة الخساره التي نستخدمها، خسارتنا لكامل البيانات هي متوسط الخساره.

بالتقليل من متوسط الخساره، نختار متغيرات النموذج التي تضبطه بناءاً على البيانات التي يتعلم منها. حتى الآن، لم نقل شيئاً عن المجتمع الأحصائي الذي انشأ البيانات. في الحقيقه، نحن مهتمين بإجراء توقعات على جميع المجتمع الأحصائي، ليس فقط البيانات التي رأيناها.

### المخاطر

اذا كانت البيانات التي رأيناها $ X $ و $ y $ تم جمعها بشكل عشوائي، فأنها تعتبر بياناتنا تعتبر متغيرات عشوائية. واذا كانت متغيرات عشوائية، فأن متغيرات النموذج ايضاً عشوائيه، في كل مره نجمع فيها المزيد من البيانات ونضبط النموذج عليها، متغيرات النموذج $ f_\hat{\theta} (x) $ ستتغير قليلاً.

لنفترض أننا قمنا بسحب أحد المدخلات والمخرجات $ z, \gamma $ من مجتمعنا الإحصائي بشكل عشوائي. الخسارة التي يكونها النموذج لهذه القيم هي:

$$ \begin{aligned}
\ell(\gamma, f_\hat{\theta} (z))
\end{aligned} $$

لاحظ أن هذه الخساره هي متغير عشوائي؛ تتغير الخساره عندما تستقبل قيم جديده من $ X $ و $ y $ ونقاط اخرى من $ z, \gamma $ من المجتمع الإحصائي.

**المخاطر Risk** لنموذج $ f_\hat{\theta} $ هي النتيجة المتوقعه من الخساره السابقه لجميع بيانات التدريب $ X $ و $ y $ وجميع النقاط $ z, \gamma $ في المجتمع الإحصائي:

$$ \begin{aligned}
R(f_\hat{\theta}(x)) = \mathbb{E}[ \ell(\gamma, f_\hat{\theta} (z)) ]
\end{aligned} $$

لاحظ ان المخاطر هي توقع لمتغير عشوائي وليست عشوائيه بحد ذاتها. القيمة المتوقعه من رمي حجر النرد ذو الست جهات بشكل عادل وعشوائي هي 3.5 على الرغم من أن عمليات الرمي نفسها عشوائية.

المثال السابق يطلق علية أحياناً **المخاطر الحقيقه True Risk** لأنها تخبرنا عن أداء النموذج على كامل المجتمع الإحصائي. إذا تمكنا من إيجاد المخاطر الحقيقه لجميع النماذج، يمكننا ببساطة اختيار النموذج الأقل مخاطر ونكون متأكدين أن النموذج سيكون أداة أفضل من بقية النماذج على المدى البعيد على دالة الخسارة التي إخترناها.

### الخطر التجريبي

الواقع ليس بهذه السهوله والروعه. إذا قمنا بتغير تعريف التوقع إلى معادة المخاطر الحقيقه، سنحصل على التالي:

$$ \begin{split}
\begin{aligned}
R(f_\hat{\theta})
&= \mathbb{E}[ \ell(\gamma, f_\hat{\theta} (z)) ] \\
&= \sum_\gamma \sum_z \ell(\gamma, f_\hat{\theta} (z)) P(\gamma, z) \\
\end{aligned}
\end{split} $$

للتبسيط، نريد أن نعرف ما تعنيه $ P(\gamma, z) $، وهي توزيع الإحتمالية العام لأي من النقاط في المجتمع الإحصائي. للأسف، إيجاد ذلك ليس سهلاً. لنفترض أننا نريد أن نتوقع قيمة الإكراميه بناءًا على عدد العملاء في الطاولة. ما هي إحتمالية أن طاولة بثلاث أشخاص يقومون بإعطاء إكرامية بقيمة $14.50؟ إذا كنا نعرف توزيع النقاط بشكل دقيقه، فلا نحتاج لجمع بيانات وضبط النموذج، سيكون لدينا معرفة بالقيمة المحتملة للإكراميه لأي عدد من العملاء في الطاولة.

على الرغم أننا لا نعرف بشكل دقيقه توزيع المجتمع الإحصائي، يمكننا توقعه بناءًا على ما أطلعنا عليه في البيانات $ X $ و $ y $. إذا قمنا بسحب قيم ل $ X $ و  $ y $ بشكل عشوائي من المجتمع الإحصائي، توزيع النقاط في $ X $ و $ y $ سيكون مشابه لتوزيع المجتمع الإحصائي. لذا، نعامل $ X $ و $ y $ كمجتمعنا الإحصائي. لذا، إحتمالية الظهور لإي من المدخلات والمخرجات $ X_i $ و $ y_i $ هي $ \frac{1}{n} $ بما أن كل كلاهما يظهر مره واحده من بين كل النقاط $ n $.

ذلك يسمح لنا بحساب **الخطر التجريبي Empirical Risk**، قيمة تقريبه للخطر الحقيقي:

$$ \begin{split}
\begin{aligned}
\hat R(f_\hat{\theta})
&= \mathbb{E}[ \ell(y_i, f_\hat{\theta} (X_i)) ] \\
&= \sum_{i=1}^n \ell(y_i, f_\hat{\theta} (X_i)) \frac{1}{n} \\
&= \frac{1}{n} \sum_{i=1}^n \ell(y_i, f_\hat{\theta} (X_i)) 
\end{aligned}
\end{split} $$

اذا كانت البيانات لدينا ذات حجم كبير وتم سحبها بشكل عشوائي من المجتمع الإحصائي، فأن الخطر التجريبي $ \hat R(f_\hat{\theta}) $ سيكون قريب جداً من الخطر الحقيقي $ R(f_\hat{\theta}) $. يسمح لنا ذلك بإختيار النموذج الذي يقلل من الخطر التجريبي.

لاحظ أن هذا المصطلح هو متوسط دالة الخساره في بداية هذا القسم! بالتقليل من متوسط الخساره، فأننا ايضاً نقلل من الخطر التجريبي. يوضح ذلك لماذا نستخدم في العادة متوسط الخساره كنتيجة لدالة الخساره بدلاً مثلاً من أعلى قيمة للخساره.

### ملخص المخاطر

الخطر الحقيقي لتنبؤ النموذح يوضح لنا خسارة النموذج على المدى البعيد التي سيحصل عليها من المجتمع الإحصائي. بما أننا عادةً لا نستطيع حساب الخطر الحقيقي بشكل مباشر، فأننا نقوم بحساب الخطر التجريبي ونستخدمه لإيجاد النموذج المناسب لتوقعنا. لأن الخطر التجريبي هو متوسط الخساره على البيانات التي رأيناها، فأننا عادةً نقلل من متوسط الخساره عند ضبط النماذج.

## إنحياز وتباين النموذج

رأينا سابقاً أن لدينا مصدرين لإتخاذ قرار الأفضلية بين النماذج.

النموذج قد يكون بسيط جداً. مثلاً، نموذج خطي بسيط لن يتمكن من ضبط البيانات إذا كانت من الدرجة الثانيه. يظهر هذا الخطأ بسبب **الإنحياز Bias** في النموذج.

النموذج قد يقوم بضبط البيانات العشوائية المجوده في البيانات، حتى لو قمنا بضبط بيانات من الدرجة الثانية على نموذج مماثل، سيتوقع النموذج نتائج مختلفة عن الصحيحه. هذا الخطأ يظخر في النموذج بسبب **التباين Variance**. [📝][BaisVariance]

### تحليل الإنحياز والتباين

يمكننا تحليل التعريفات السابقه وفهمها بشكل أوضح بإستخدام معادلة مخاطر النموذج. لنتذكر أن **المخاطر Risk** للنموذج $ f_\hat{\theta} $ هي الخساره المتوقه لجميع بيانات التدريب $ X $ و $ y $ وجميع النقاط المدخله والمخرجة $ z, \gamma $ في المجتمع الإحصائي:

$$ \begin{aligned}
R(f_\hat{\theta}) = \mathbb{E}[ \ell(\gamma, f_\hat{\theta} (z)) ]
\end{aligned} $$

نرمز لعملية إنشاء بيانات المجتمع الإحصائي الحقيقه ب $ f_\theta(x) $. القيمة الناتجة $ \gamma $ تنتج بواسطة بيانات المجتمع الإحصائي اضافة لقيمة عشوائية مشوشة من البيانات: $ \gamma_i = f_\theta(z_i) + \epsilon $. **التشويش العشوائي Random Noise** $ \epsilon $ هي قيمة عشوائية لديها متوسط يساوي صفر: $ \mathbb{E}[\epsilon] = 0 $. [📝][Noise] [📝][Noise2]

إذا استخدمنا الخطأ التربيعي كدالة الخسارة، تصفح المعادلة السابقة كالتالي:

$$ \begin{aligned}
R(f_\hat{\theta}) = \mathbb{E}[ (\gamma - f_\hat{\theta} (z))^2 ]
\end{aligned} $$

مع قليل من عمليات المعالجة الجبريه، يمكن ان نعرف المعادلة السابقة كالتالي:

$$ \begin{aligned}
R(f_\hat{\theta}) = (\mathbb{E}[f_\hat{\theta}(z)] - f_\theta(z))^2 + \text{Var}(f_\hat{\theta}(z)) + \text{Var}(\epsilon)
\end{aligned} $$

المصطلح الأول في المعادلة، $ (\mathbb{E}[f_\hat{\theta}(z)] - f_\theta(z))^2 $، هي التعبير الرياضي للإنحياز في النموذج. (يمكننا القول تقنياً أن المصطلح هو تربيع الإنحياز $ \text{bias}^2 $ ). الإنحياز سيساوي صفر على المدى البعيد إذا كان إختيارنا للنموذج $ f_\hat{\theta}(z) $ يتوقع نفس النتائج التي تم نتجت من خطوات معالجة المجتمع الإحصائي $ f_\theta(z) $. يكون الإنحياز عالي إذا كان النموذج الذي إخترناه يتوقع نتائج خاطئة في خطوات معالجة المجتمع الإحصائي حتى لو كانت كامل المجتمع الإحصائي كبيانات تدريب.

المصطلح الثاني في المعادلة، $ \text{Var}(f_\hat{\theta}(z)) $، هو تباين النموذج. يكون التباين أقل عندما تكون توقعات النموذج لا تتغير كثيراً عند تدريبه على بيانات مختلفة من المجتمع الإحصائي. يكون التباين عالي عندما يكون التغير كبير عند تدريبه على بيانات مختلفه من المجتمع الإحصائي.

المصطلح الاخير في المعادلة، $ \text{Var}(\epsilon) $، يعني الخطأ الغير قابل للإختزال أو التشويش الناتج عند بناء البيانات أو جمعها. يكون أقل عندما يكون بناء البيانات وجمعها دقيق. والعكس يكون أعلى عندما يكون التشويش عالي في البيانات.

### مشتقة تحليل الإنحياز والتبيان

أولاً، نبدأ بمتوسط الخطأ التربيعي:

$$ \mathbb{E}[(\gamma - f_{\hat{\theta}}(z))^2] $$

ثم نوسع التربيع ونطبق خطية التوقع Linearity of Expectation:

$$ =\mathbb{E}[\gamma^2 -2\gamma f_{\hat{\theta}} +f_\hat{\theta}(z)^2 $$

$$ = \mathbb{E}[\gamma^2] - \mathbb{E}[2\gamma f_{\hat{\theta}}(z)] + \mathbb{E}[f_{\hat{\theta}}(z)^2] $$

لأن $ \gamma $ و $ f_{\hat{\theta}}(z) $ مستقلان (نتائج النموذج وما اطلع عليه من المجتمع الإحصائي لا يعتمدان على بعضهما)، يمكننا القول أن $ \mathbb{E}[2\gamma f_{\hat{\theta}}(z)] = \mathbb{E}[2\gamma]  \mathbb{E}[f_{\hat{\theta}}(z)] $. ثم نقوم بتعويض $ f_\theta(z) + \epsilon $ بدلاً من $ \gamma $:

$$ =\mathbb{E}[(f_\theta(z) + \epsilon)^2] - \mathbb{E}[2(f_\theta(z) + \epsilon)] \mathbb{E}[f_{\hat{\theta}}(z)] + \mathbb{E}[f_{\hat{\theta}}(z)^2] $$   

للتبسيط أكثر: (لاحظ أن $ \mathbb{E}[f_\theta(z)] = f_\theta(z) $ لأن $ f_\theta(z) $ دالة حتمية، إذا اعطيت نقطة معينة $ z $.)

$$ =\mathbb{E}[f_\theta(z)^2 + 2f_\theta(z) \epsilon + \epsilon^2] - (2f_\theta(z) + \mathbb{E}[2\epsilon]) \mathbb{E}[f_{\hat{\theta}}(z)] + \mathbb{E}[f_{\hat{\theta}}(z)^2] $$ 

تطبيق خطية التوقع مره أخرى:

$$ = f_\theta(z)^2 + 2f_\theta(z)\mathbb{E}[\epsilon] + \mathbb{E}[\epsilon^2] - (2f_\theta(z) + 2\mathbb{E}[\epsilon]) \mathbb{E}[f_{\hat{\theta}}(z)] + \mathbb{E}[f_{\hat{\theta}}(z)^2] $$   

لاحظ أن $ \big( \mathbb{E}[\epsilon] = 0 \big) => \big( \mathbb{E}[\epsilon^2] = \text{Var}(\epsilon) \big) $ لأن $ \text{Var}(\epsilon) = \mathbb{E}[\epsilon^2]-\mathbb{E}[\epsilon]^2 $:

$$ = f_\theta(z)^2 + \text{Var}(\epsilon) - 2f_\theta(z) \mathbb{E}[f_{\hat{\theta}}(z)] + \mathbb{E}[f_{\hat{\theta}}(z)^2] $$ 

يمكننا الآن إعادة كتابة المعادلة كالتالي:

$$ = f_\theta(z)^2 + \text{Var}(\epsilon) - 2f_\theta(z) \mathbb{E}[f_{\hat{\theta}}(z)] + \mathbb{E}[f_{\hat{\theta}}(z)^2] -  \mathbb{E}[f_{\hat{\theta}}(z)]^2 + \mathbb{E}[f_{\hat{\theta}}(z)]^2 $$

لأن $ \mathbb{E}[f_{\hat{\theta}}(z)^2] -  \mathbb{E}[f_{\hat{\theta}}(z)]^2 = Var(f_{\hat{\theta}}(z)) $:

$$ =  f_\theta(z)^2 - 2f_\theta(z) \mathbb{E}[f_{\hat{\theta}}(z)] + \mathbb{E}[f_{\hat{\theta}}(z)]^2 + Var(f_{\hat{\theta}}(z)) + \text{Var}(\epsilon) $$ 


$$ = (f_\theta(z) - \mathbb{E}[f_{\hat{\theta}}(z)])^2 + Var(f_{\hat{\theta}}(z)) + \text{Var}(\epsilon) $$       

$$= \text{bias}^2 + \text{model variance} + \text{noise} $$


لإختيار نموذج يؤدي بشكل ممتاز، نسعى دائماً أن نحصل على مخاطر أقل. ولتقليلها، نحاول تقليل الإنحياز، التباين والتشويش. تقليل التشويش يتطلب عادةً تحسين في طريقة جمع البيانات. للتقليل من الإنحياز والتباين، يجب علينا ضبط النماذج وتعقيدها. النماذج البسيطه يكون فيها الإنحياز مرتفع؛ النماذج المعقدة جداً لديها تباين عالي. هذا هو مفهوم *المقايضة بين الإنحياز والتباين bias-variance tradeoff*، مشكلة أساسية نواجهها دائماً عند إختيار نماذج التنبؤ.

### مثال: الإنحدار الخطي وموجات الجيب Sine Waves

لنفترض أننا نحاول إنشاء نموذج للدالة المتأرجه التالي:

```python
from collections import namedtuple
from sklearn.linear_model import LinearRegression

np.random.seed(42)

Line = namedtuple('Line', ['x_start', 'x_end', 'y_start', 'y_end'])

def f(x): return np.sin(x) + 0.3 * x

def noise(n):
    return np.random.normal(scale=0.1, size=n)

def draw(n):
    points = np.random.choice(np.arange(0, 20, 0.2), size=n)
    return points, f(points) + noise(n)

def fit_line(x, y, x_start=0, x_end=20):
    clf = LinearRegression().fit(x.reshape(-1, 1), y)
    return Line(x_start, x_end, clf.predict(x_start)[0], clf.predict(x_end)[0])

population_x = np.arange(0, 20, 0.2)
population_y = f(population_x)

avg_line = fit_line(population_x, population_y)

datasets = [draw(100) for _ in range(20)]
random_lines = [fit_line(x, y) for x, y in datasets]
```

> قام الكاتب في الكود البرمجي السابق إنشاء بيانات عشوائية لغرض العمل عليها في هذا الجزء من الدرس، الناتج من هذا الكود يظهر في الرسم البياني التالي

```python
plt.plot(population_x, population_y)
plt.title('True underlying data generation process');
```

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter15/bias_modeling_11_0.png'>
</p>

إذا حاولنا أخذ مجموعه بيانات من المجتمع الإحصائي في هذه الرسم، قد نصل إلى الشكل التالي:

```python
xs, ys = draw(100)
plt.scatter(xs, ys, s=10)
plt.title('One set of observed data');
```

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter15/bias_modeling_13_0.png'>
</p>

لنفترض أننا قمنا بسحب مجموعات مختلفة من البيانات من هذا المجتمع الإحصائي وقمنا بضبط نموذج خطي بسيط لكل مجموعة. في الأسفل، قمنا برسم شكل خطي لبيانات المجتمع الإحصائي الناتجة باللون الأزرق و توقعات النموذج باللون الأخضر:


```python
plt.figure(figsize=(8, 5))
plt.plot(population_x, population_y)

for x_start, x_end, y_start, y_end in random_lines:
    plt.plot([x_start, x_end], [y_start, y_end], linewidth=1, c='g')

plt.title('Population vs. linear model predictions');
```

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter15/bias_modeling_15_0.png'>
</p>

الرسمه السابقه توضح أن توقع النموذج الخطي سينتج أخطاء. يمكننا حل هذه الأخطاء بإستخدام الإنحياز، التباين والتشويش الغير قابل للإختزال. نوضح الإنحياز في نموذجنا عن طريق إظهار ان متوسط النموذج الخطي على المدى البعيد سيتوقع نتائج مختلفة عن تلك في المجتمع الإحصائي:

```python
plt.figure(figsize=(8, 5))
xs = np.arange(0, 20, 0.2)
plt.plot(population_x, population_y, label='Population')

plt.plot([avg_line.x_start, avg_line.x_end],
         [avg_line.y_start, avg_line.y_end],
         linewidth=2, c='r',
         label='Long-run average linear model')
plt.title('Bias of linear model')
plt.legend();
```

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter15/bias_modeling_17_0.png'>
</p>

التباين للنموذج هو مدى إختلاف توقعات النموذج حول متوسط النموذج على المدى البعيد:

```python
plt.figure(figsize=(8, 5))
for x_start, x_end, y_start, y_end in random_lines:
    plt.plot([x_start, x_end], [y_start, y_end], linewidth=1, c='g', alpha=0.8)
    
plt.plot([avg_line.x_start, avg_line.x_end],
         [avg_line.y_start, avg_line.y_end],
         linewidth=4, c='r')

plt.title('Variance of linear model');
```

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter15/bias_modeling_19_0.png'>
</p>

أخيراً، نستعرض الخطأ الغير قابل للإختزال عن طريق إظهار إنحرافات النقاط في بيانات المجتمع الإحصائي:

```python
plt.plot(population_x, population_y)


xs, ys = draw(100)
plt.scatter(xs, ys, s=10)
plt.title('Irreducible error');
```

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter15/bias_modeling_21_0.png'>
</p>

### التطبيق العملي للإنحياز والتباين

في عالمٍ مثالي، نقوم بالتقليل من الخطأ المتوقع من تنبؤ النموذج لجميع النقاط الداخله والخارجه في المجتمع الإحصائي. ولكن، عملياً، نحن لا نعرف خطوات بناء البيانات ولذلك لن نستطيع بشكل دقيق تحديد إنحياز، تباين والخطأ الغير قابل للإختزال للنموذج. بدلاً من ذلك، نستخدم البيانات التي مرت علينا وقيمه قريبه لما في المجتمع الإحصائي.

كما رأينا في الأعلى، الوصول إلى قيمة أقل للخطأ في بيانات التدريب لا يعني أن النموذج سيحصل على قيمه أقل ايضاً في بيانات الإختبار. من السهل الحصول على نموذج بقيمة إنحياز قليله جداً وبالتالي قيمة خطأ أقل في بيانات التدريق عن طريق ضبط النموذج بإستخدام منحنى يمر على جميع بيانات التدريب. ولكن هذا النموذج سيكون تباينه عالي جداً والذي يؤدي إلى خطأ عالي في بيانات الإختبار. على العكس، النموذج الذي يتوقع قيمه ثابته لدية تباين أقل و إنحياز عالي. أساساً، يحدث ذلك بسبب أن خطأ التدريب يعكس إنحياز النموذج ولكن ليس تباينه؛ ولكن خطأ الإختبار يعكس كليهما. للتقليل من خطأ الإختبار، يحتاج نموذجنا أن يحقق إنحياز وتباين قليلان معاً. للقيام بذلك، نحتاج لطريقة لمحاكاة خطأ الإختبار بدون إستخدام بيانات الإختبار. يتم ذلك عادةً بإستخدام التحقق المتقاطع Cross Validation.

### النقاط الأساسية

المقايضة بين الإنحياز والتباين تسمح لنا بشكل دقيق وصف ظواهر النماذج التي رأيناها سابقاً.

يحدث فرط التعميم Underfitting عادةً بسبب الإنحياز؛ فرط التخصيص يحدث بسبب التباين.

جمع المزيد من البيانات يقلل من التباين. مثلاً، تباين نموذج الإنحدار الخطي يقل بمعدل عامل $ 1/n $، هنا $ n $ هي عدد النقاط في البيانات. لذا، مضاعفة حجم البيانات يقلل التباين إلى النصف، وسحب المزيد من البيانات سيقلل التباين إلى صفر. أحدى الخطوات الشائعه هي إختيار نموذج بإنحياز قليل وتباين عالي (مثلاً شبكة عصبيه) ومن ثم سحب المزيد من البيانات للتقليل من تباين النموذج إلى ان يصل لقيمه قليله تمكنه من القيام بتوقعات صحيحه. على الرغم فاعليتها عملياً، إلى ان جمع المزيد من البيانات لهذه النماذج عادةً ما يأخذ وقت و جهد ومصاريف أكثر.

جمع المزيد من البيانات يقلل الإنحياز إذا تم ضبط النموذج على بيانات المجتمع الإحصائي كاملة. إذا لم نتمكن من ضبط النموذج على كامل المجتمع الإحصائي (كما في المثال السابق)، حتى لو كان لدينا عدد لا نهائي من البيانات لن نتمكن من التخلص من الإنحياز.

إضافة خصائص Features مفيدة للبيانات، مثلاً التربيع عندما تكون البيانات الأساسيه من الدرجة الثانية، تقلل من الإنحياز. إضافة خصائص غير مفيدة نادراً منا تزيد من الإنحياز.

إضافة خصائص سواءًا كانت مفيدة أم لا، عادةً ما تزيد من التباين كون كل خاصية نضيفها تزيد من المتغيرات في النموذج. بشكل عام، النماذج ذات المتغيرات الكثيره لديها أشكال مختلفه من المتغيرات ولذلك  لديها تباين أعلى من نماذج بمتغيرات أقل. للزيادة من دقة توقعات النموذج، إضافة خاصية جديده يجب أن تقلل من الإنحياز أكثر من ما تزيد من التباين.

حذف الخصائص عادةً ما يزيد من الإنحياز وقد يتسبب بفرط التعميم. مثلاً، نموذج خطي بسيط لديه إنحياز أعلى من نفس النموذج إذا إضيفت له خاصية تربيعيه. إذا تم إنشاء البيانات بإستخدام التربيع، فأن النموذج الخطي البسيط سيتناسب مع البيانات.

في الرسم البياني التالي، المحور السيني يقيس تعقيد النموذج والمحور الصادي يقيس حجمه. لاحظ أنه كلما زاد تعقيد النموذج، يقل الإنحياز بشكل واضح وبينما يزيد التباين بنفس الشكل. عندما نختار نموذجاً معقد، خطأ الإختبار في البداية يقل ثم يزيد حتى يصل تباين النموذج ويتجاوز الإنحياز أثناء هبوطه:

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter15/bias_modeling_bias_var_plot.png'>
</p>

كما هو واضح في الرسم، النموذج ذو التعقيد العالي قد يصل إلى خطأ قليل في بيانات التدريب ولكن قد يفشل في تعميم هذه النتائج على بيانات الإختبار بسبب تباينه العالي. بشكل آخر، النموذج بتعقيد أقل سيكون ذو تباين أقل ولكن قد يفشل أيضاً في التعميم بسبب إنحيازة العالي. لإختيار النموذج المفيد، يجب أن نصل إلى التوازن بين إنحياز وتباين النموذج.

كلما أضفنا المزيد إلى البيانات، نقوم بتحريك المنحنى في رسمنا البياني إلى اليمين والأسفل، نقلل من الإنحياز والتباين:

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter15/bias_modeling_bias_var_shift.png'>
</p>

### ملخص إنحياز وتباين النموذج

مقايضة الإنحياز والتبيان تكشف لنا عن مشكلة أساسية في النمذجة. من أجل التقليل من مخاطر النموذج، فعلينا ان نستخدم مزيجاً من هندسة الخصائص، إختيار النماذج والتحقق المتقاطع للوصول إلى توازن بين الإنحياز والتباين.

## التحقق المتقاطع

### تقسيم بيانات التدريب والتحقق والإختبار

#### حجم تقسيم بيانات التدريب والتحقق والإختبار

### خطأ التدريب و خطأ الإختبار

### التحقق المتقاطع K-Flod

### مقايضة الإنحياز والتباين

### مثال: اختيار النموذج لبيانات تقييم الآيس كريم

### ملخص التحقق المتقاطع

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter15/'>
</p>



[BaisVariance]: https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/
[Noise]: https://sci2s.ugr.es/noisydata
[Noise2]: statisticshowto.com/statistical-noise/