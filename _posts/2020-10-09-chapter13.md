---
title: النماذج الخطية
show_title: true
chapter_number: 13
chapter_text: الفصل الثالث عشر
chapter_lessons: [[0, 'مقدمة'], [1, 'التنبؤ بالإكراميات'], [2, 'ضبط النموذج الخطي بإستخدام النزول الإشتقاقي'], [3, 'الإنحدار الخطي المتعدد'], [4, 'المربعات الصغرى - منظور هندسي'], [5, 'تطبيق عملي للإنحدار الخطي']]
chapter_sublessons: [
    [],
    [['تعريف نموذج خطي بسيط', 'توقع النموذج الخطي']],
    [['ضبط النموذج الخطي مع النزول الإشتقاقي', 'ملاحظة عن إستخدام العلاقات'],'مشتقة خسارة الخطأ التربيعي المتوسط MSE', 'تطبيق النزول الإشتقاقي'],
    ['خسارة الخطأ التربيعي المتوسط وإنحدارها', 'ضبط النموذج مع النزول الإشتقاقي', 'الرسم البياني لتنبؤاتنا', 'استخدام كامل البيانات', 'ملخص الإنحدار الخطي المتعدد'],
    ['المربعات الصغرى: النموذج الثابت', 'المربعات الصغرى: نموذج خطي بسيط', 'الفكره الهندسيه', 'الجبر الخطي', 'انهاء الدراسه', 'عندما تعتمد المتغيرات خطياً', 'طريقتين للتفكير'],
    ['نظره عامه على البيانات', 'تنظيف البيانات', 'فصل بيانات التدريب والإختبار', 'استكشاف البيانات وتصويرها', 'النماذج الخطيه البسيطه', 'تحويل المتغيرات', 'نموذج الإنحدار الخطي المتعدد', 'تقييم نموذجنا'],
]
layout: default
---

## مقدمة

الآن، بعد ان تعلمنا بشكل عام ادوات لضبط النماذج مع دوال التكلفه، نتحول لطرق تحسين النموذج. للتبسيط، في السابق حددنا عملنا على النموذج الثابت: نموذجنا فقط يتوقع رقم واحد.

ولكن، إعطاء نموذج كهذا للمضيف لن يرضيه. يود المضيف ان يوضح انه حصل على معلومات اكثر نسبة الإكراميه من الطاولات التي خدمها. لماذا لا نستخدم بياناته الأخرى، مثلاً حجم العملاء في الطاوله ومجموع الفاتوره، لغرض جعل النموذج اكثر فائدة.

في هذا الفصل سنتعرف على النماذج الخطية والذي يسمح لنا بإستخدام جميع البيانات التي لدينا لإجراء توقعات. النماذج الخطية لا تستخدم بنطاقٍ واسع فقط، بل ايضاً لديها اسس نظريه غنية بالمعلومات التي تجعلنا نفهم ادوات مستقبليه للنماذج. سنتعرف على نموذج الإنحدار الخطي البسيط الذي يستخدم متغير واحد، سنتعلم كيف يستخدم النزول الإشتقاقي لضبط النموذج، واخيراً التوسع في النموذج وإضافة المزيد من المتغيرات له.

## التنبؤ بالإكراميات

سابقاً، تعاملنا مع بيانات تحتوي على صف واحد لكل طاولة قام المضيف بخدمتها لمدة اسبوع. المضيف قام بجمع البيانات للقيام بالتوقع بقيمة الإكرامية التي سيحصل عليها في المستقبل:

```python
tips = sns.load_dataset('tips')
tips.head()
```

|**size**|**time**|**day**|**smoker**|**sex**|**tip**|**total\_bill**| 
|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:
|2|Dinner|Sun|No|Female|1.01|16.99|0
|3|Dinner|Sun|No|Male|1.66|10.34|1
|3|Dinner|Sun|No|Male|3.5|21.01|2
|2|Dinner|Sun|No|Male|3.31|23.68|3
|4|Dinner|Sun|No|Female|3.61|24.59|4

<br>
```python
sns.distplot(tips['tip'], bins=25);
```

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter13/linear_tips_3_0.png'>
</p>

كما تحدثنا سابقاً، اذا اخترنا النموذج الثابت ودالة الخطأ التربيعي المتوسط، فأن نموذجنا سيتوقع متوسط الإكراميات:

```python
np.mean(tips['tip'])
```

```ruby
2.9982786885245902
```

يعني ذلك ان عندما يأتي مجموعة من الزبائن للمضيف ثم يسألنا المضيف عن مجموعه الإكرامية التي سيحصل عليها، فسنجيب عليه "حوالي $ \\$3 $"، اياً كان عدد الزبائن ومجموع الفاتوره.

ولكن، عند التحقق من باقي المتغيرات في البيانات، يمكننا ان نقوم بتوقعات دقيقه إذا اضفنا تلك المتغيرات للنموذج. مثلاً، الرسم البياني التالي يوضح مجموع الإكراميه مقارنة بمبلغ الفاتورة ويظهر العلاقة الإيجابية بينهما:

```python
sns.lmplot(x='total_bill', y='tip', data=tips, fit_reg=False)
plt.title('Tip amount vs. Total Bill')
plt.xlabel('Total Bill')
plt.ylabel('Tip Amount');
```

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter13/linear_tips_7_0.png'>
</p>

على الرغم ان متوسط الإكراميه هو $ \\$3 $، اذا كانت الطاولة طلبت ما مجموعه $ \\$40 $ من الوجبات فأننا متأكدين ان المضيف سيحصل على اكثر من $ \\$3 $ كأكرامية. لذا، نريد التعديل في نموذجنا ليتمكن من التوقع بناءًا على متغيرات في بياناتنا بدلاً من توقع متوسط الإكرامية. للقيام بذلك، سنستخدم النموذج الخطي بدلاً من الثابت.

لنراجع أولاً الأدوات التي لدينا لبناء النموذج والتوقع وتعريف بعض الرموز الجديده لنتمكن بشكل افضل من تمثيل العمليات الرياضيه الإضافيه في النموذج الخطي.

### تعريف نموذج خطي بسيط

نحن مهتمون بتوقع قيمة الإكراميه بناءًا على مجموع الفاتوره. لنجعل $ y $ تمثل مجموع الإكراميه، المتغير الذي نريد ان يتوقعه النموذج. و $ x $ تمثل مجموع الفاتوره، المتغير الذي نريد استخدامه للتوقع.

نقوم بتعريف النموذج الخطي $ f_\boldsymbol\theta^* $ الذي يعتمد على $ x $:

$$ f_\boldsymbol\theta^* (x) = \theta_1^* x + \theta_0^* $$

نتعامل مع $ f_\boldsymbol\theta^* (x) $ على انها الدالة التي أنشأت البيانات.

تفترض $ f_\boldsymbol\theta^* (x) $ ان $ y $ لديها علاقه خطيه كامله مع $ x $. ولكن، في بياناتنا لا يظهر لنا خط مستقيم بسبب وجود بعض البيانات العشوائية المزعجه $ \epsilon $. رياضياً، نأخذ بعين الإعتبار هذه المشكله بإضافة المصطلح:

$$ y = f_\boldsymbol\theta^* (x) + \epsilon $$

أذا كانت الإفتراضيه ان $ y $ لديها علاقخ خطية كامله مع $ x $، وتمكنا بطريقة ما من توقع القيمة الصحيحه ل $ \theta_1^* $ و $ \theta_0^* $، وبشكل غير اعتيادي لم يكنا لدينا أي عشوائية في البيانات، سنتمكن من التوقع بشكل دقيق قيمة الإكرامية التي سيحصل عليها المضيف من جميع الزبائن، وبشكل دائم. بالطبع، لا يمكننا تلبية جميع المعايير في الحياه الواقعية. بدلاً من ذلك، نتوقع $ \theta_1^* $ و $ \theta_0^* $ بإستخدام البيانات لجعل توقعاتنا اقرب دقه للواقع.

#### توقع النموذج الخطي

بما أننا لا نستطيع إيجاد قيمة $ \theta_1^* $ و $ \theta_0^* $ بشكل دقيق، سنفترض ان بياناتنا تتوقع قيمة هذه المتغيرات. نشير للتوقعات ب $ \theta_1 $ و $ \theta_0 $، وتوقعنا الذي يتم ضبطه بالنموذج ب $ \hat{\theta_1} $ و $ \hat{\theta_0} $، و النموذج:

$$ f_\boldsymbol\theta (x) = \theta_1 x + \theta_0 $$

أحياناً سترى $ h(x) $ بدلاً من $ f_\hat{\boldsymbol\theta} (x) $؛ ال $ h $ تعني الفرضية *Hypothesis*، كون $ f_\hat{\boldsymbol\theta} (x) $ هي فرضيتنا ل $ f_{\boldsymbol\theta^*} (x) $.

من أجل تحديد قيمة $ \hat{\theta_1} $ و $ \hat{\theta_0} $، نقوم بإختيار دالة تكلفة وتقليلها بإستخدام النزول الإشتقاقي.

## ضبط النموذج الخطي بإستخدام النزول الإشتقاقي

نريد ضبط نموذج خطي يتنبأ بقيمة الإكراميه من مجموع الفاتوره:

$$ f_\boldsymbol\theta (x) = \theta_1 x + \theta_0 $$

ولتسحين قيم $ \theta_1 $ و $ \theta_0 $، نحتاج أولاً لإختيار دالة خسارة. سنختار دالة خسارة الخطأ التربيعي المتوسط MSE:

$$ \begin{split}
\begin{aligned}
L(\boldsymbol\theta, \textbf{x}, \textbf{y})
&= \frac{1}{n} \sum_{i = 1}^{n}(y_i - f_\boldsymbol\theta (x_i))^2\\
\end{aligned}
\end{split} $$

لاحظ اننا عدلنا على دالة الخساره لتوضيح إضافتنا لمتغير في النموذج. الآن، $ \textbf{x} $ هي مصفوفه أحادية البعد تحتوي على جميع الفواتير، و $ \textbf{y} $ هي مصفوفة أحادية البعد تحتوي على قيمة كل إكراميه، و $ \boldsymbol\theta $ هي مصفوفة تحتوي على التالي: $ \boldsymbol\theta = [ \theta_1, \theta_0 ] $.

يطلق على إستخدام النموذج الخطي مع دالة خسارة الخطأ التربيعي المتوسط بإسم الإنحدار الخطي للمربعات الصغرى Least-squares Linear Regression. يمكننا استخدام النزول الإشتقاقي لإيجاد قيمة $ \boldsymbol\theta $ التي تقلل الخساره. [📝][LinearLeastSquares]

#### ملاحظة عن إستخدام العلاقات

اذا سبق أن رأيت الإنحدار الخطي للمربعات الصغرى، قد تلاحظ اننا نستطيع حساب معامل الإرتباط وإستخدامه لتحديد قيمة $ \theta_1 $ و $ \theta_0 $. هذه طريقة اسهل واسرع للحساب بدلاً من إستخدام النزول الإشتقاقي في أي معادله، تماماً كما يكون اسهل لنا حساب المتوسط بدلاً من حساب النزول الإشتقاقي لضبط النموذج الثابت. على أية حال، سنستخدم النزول الإشتقاقي لأنها طريقة عامله لتقليل الخساره وستعمل معنا لاحقاً عندما نتعرف على نماذج لا يمكن حساب خسارتها إحصائياً. بالأصح، في كثير من المشاكل في العالم الحقيقي، سنستخدم النزول الإشتقاقي حتى ولو كانت هناك طرق إحصائيه تحليله لأن حسابها يأخذ وقتاً اطول من النزول الإشتقاقي، خاصة عندما تكون البيانات ذات حجم كبير.

### مشتقة خسارة الخطأ التربيعي المتوسط MSE

لإستخدام النزول الإشتقاقي، نحتاج لحساب مشتقة خسارة الخطأ التربيعي المتوسط بالنسبة ل $ \boldsymbol\theta $. الآن بما ان $ \boldsymbol\theta $ عبارة عن مصفوفه ذات طول 2 وليست قيمة عددية مدرجة Scalar، و $ \nabla_{\boldsymbol\theta} L(\boldsymbol\theta, \textbf{x}, \textbf{y}) $ ايضاً مصفوفه من الحجم 2. [📝][Scalar]

$$ \begin{split}
\begin{aligned}
\nabla_{\boldsymbol\theta} L(\boldsymbol\theta, \textbf{x}, \textbf{y})
&= \nabla_{\boldsymbol\theta} \left[ \frac{1}{n} \sum_{i = 1}^{n}(y_i - f_\boldsymbol\theta (x_i))^2 \right] \\
&= \frac{1}{n} \sum_{i = 1}^{n}2 (y_i - f_\boldsymbol\theta (x_i))(- \nabla_{\boldsymbol\theta} f_\boldsymbol\theta (x_i))\\
&= -\frac{2}{n} \sum_{i = 1}^{n}(y_i - f_\boldsymbol\theta (x_i))(\nabla_{\boldsymbol\theta} f_\boldsymbol\theta (x_i))\\
\end{aligned}
\end{split} $$

نعرف ان:

$$ f_\boldsymbol\theta (x) = \theta_1 x + \theta_0 $$

نريد حساب قيمة $ \nabla_{\boldsymbol\theta} f_\boldsymbol\theta (x_i) $ والتي هي مصفوفه طولها 2:

$$ \begin{split}
\begin{aligned}
\nabla_{\boldsymbol\theta} f_\boldsymbol\theta (x_i)
&= \begin{bmatrix}
     \frac{\partial}{\partial \theta_0} f_\boldsymbol\theta (x_i)\\
     \frac{\partial}{\partial \theta_1} f_\boldsymbol\theta (x_i)
   \end{bmatrix} \\
&= \begin{bmatrix}
     \frac{\partial}{\partial \theta_0} [\theta_1 x_i + \theta_0]\\
     \frac{\partial}{\partial \theta_1} [\theta_1 x_i + \theta_0]
   \end{bmatrix} \\
&= \begin{bmatrix}
     1 \\
     x_i
   \end{bmatrix} \\
\end{aligned}
\end{split} $$

اخيراً، نعوضها في معادلتنا الأساسيه لنحصل على التالي:

$$ \begin{split}
\begin{aligned}
\nabla_{\boldsymbol\theta} L(\theta, \textbf{x}, \textbf{y})
&= -\frac{2}{n} \sum_{i = 1}^{n}(y_i - f_\boldsymbol\theta (x_i))(\nabla_{\boldsymbol\theta} f_\boldsymbol\theta (x_i))\\
&= -\frac{2}{n} \sum_{i = 1}^{n} (y_i - f_\boldsymbol\theta (x_i)) \begin{bmatrix} 1 \\ x_i \end{bmatrix} \\
&= -\frac{2}{n} \sum_{i = 1}^{n} \begin{bmatrix}
    (y_i - f_\boldsymbol\theta (x_i)) \\
    (y_i - f_\boldsymbol\theta (x_i)) x_i
    \end{bmatrix} \\
\end{aligned}
\end{split} $$

هذه مصفوفة من طولها 2 كون $ (y_i - f_\boldsymbol\theta (x_i)) $ قيمة عددية مدرجه.

### تطبيق النزول الإشتقاقي

الآن، لنقوم بضبط النموذج الخطي على بيانات الإكراميات لتوقع قيمة الإكرامية من مجموع الفاتورة.

أولاً، نقوم بتعريف دالة في بايثون لحساب الخساره:

```python
def simple_linear_model(thetas, x_vals):
    '''نتيجة هذه الداله هي القيمه المتوقعه من النموذج الخطي'''
    return thetas[0] + thetas[1] * x_vals

def mse_loss(thetas, x_vals, y_vals):
    return np.mean((y_vals - simple_linear_model(thetas, x_vals)) ** 2)
```

ثم نعرف طالة تقوم بحساب خطية الخساره:

```python
def grad_mse_loss(thetas, x_vals, y_vals):
    n = len(x_vals)
    grad_0 = y_vals - simple_linear_model(thetas, x_vals)
    grad_1 = (y_vals - simple_linear_model(thetas, x_vals)) * x_vals
    return -2 / n * np.array([np.sum(grad_0), np.sum(grad_1)])
```

سنقوم بإستخدام الدالة `minimize` التي سبق ان عرفناها لتطبيق النزول الإشتقاقي:

```python
def minimize(loss_fn, grad_loss_fn, x_vals, y_vals,
             alpha=0.0005, progress=True):
        '''
    تستخدم النزول الإشتقاقي للتقليل من دالة الخساره loss_fn.
    تنتج لنا الداله القيمه الصغرى ل theta_hat (θ^) عندما يكون
    التغيير اقل من 0.001 بين التكرارات.
    '''
    theta = np.array([0., 0.])
    loss = loss_fn(theta, x_vals, y_vals)
    while True:
        if progress:
            print(f'theta: {theta} | loss: {loss}')
        gradient = grad_loss_fn(theta, x_vals, y_vals)
        new_theta = theta - alpha * gradient
        new_loss = loss_fn(new_theta, x_vals, y_vals)
        
        if abs(new_loss - loss) < 0.0001:
            return new_theta
        
        theta = new_theta
        loss = new_loss
```

والآن نقوم بتطيبق النزول الإشتقاقي:

```python
thetas = minimize(mse_loss, grad_mse_loss, tips['total_bill'], tips['tip'])
```

```ruby
theta: [0. 0.] | cost: 10.896283606557377
theta: [0.   0.07] | cost: 3.8937622006094705
theta: [0.  0.1] | cost: 1.9359443267168215
theta: [0.01 0.12] | cost: 1.388538448286097
theta: [0.01 0.13] | cost: 1.235459416905535
theta: [0.01 0.14] | cost: 1.1926273731479433
theta: [0.01 0.14] | cost: 1.1806184944517062
theta: [0.01 0.14] | cost: 1.177227251696266
theta: [0.01 0.14] | cost: 1.1762453624313751
theta: [0.01 0.14] | cost: 1.1759370980989148
theta: [0.01 0.14] | cost: 1.175817178966766
```

نلاحظ ان النزول الإشتقاقي يقترب لقيمة $ \hat\theta_0 = 0.01 $ و $ \hat\theta_0 = 0.14 $. نموذجنا الخطي الآن:

$$ y = 0.14x + 0.01 $$

يمكننا استخدام النتيجة السابقه لرسم توقعاتنا بجانب البيانات الحقيقيه:

```python
x_vals = np.array([0, 55])
sns.lmplot(x='total_bill', y='tip', data=tips, fit_reg=False)
plt.plot(x_vals, simple_linear_model(thetas, x_vals), c='goldenrod')
plt.title('Tip amount vs. Total Bill')
plt.xlabel('Total Bill')
plt.ylabel('Tip Amount');
```

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter13/linear_grad_16_0.png'>
</p>

نلاحظ انه عندما تكون قيمة الفاتوره $ \\$10 $، فأن نموذجنا يتوقع ان المضيف سيحصل على إكرامية بحوالي $ \\$1.50 $. بنفس الطريقة، اذا كانت قيمة الفاتوره $ \\$40 $ فأن النموذج يتوقع حصول المضيف على إكرامية بقيمة $ \\$6.00 $.

## الإنحدار الخطي المتعدد

نموذجنا الخطي البسيط لميزة إضافية عن النموذج الثابت، الميزه هي استخدامه البيانات للتوقع. ولكن، لا يزال النموذج محدود كونه يستخدم متغير واحد من بياناتنا. الكثير من البيانات تحتوي على اكثر من متغير مهم ومفيد للإستخدام، ويمكن للإنحدار الخطي المتعدد الإستفاده من ذلك. مثلاً، لنأخذ البيانات التاليه لأنواع السيارات ومعلومات صرف الوقود بالميل لكل جالون (Milage Per Gallon MPG):

```python
mpg = pd.read_csv('mpg.csv').dropna().reset_index(drop=True)
mpg
```

| car name                  | origin | model year | \.\.\. | displacement | cylinders | mpg    |        |
|:-------------------------:|:------:|:----------:|:------:|:------------:|:---------:|:------:|:------:|
| chevrolet chevelle malibu | 1      | 70         | \.\.\. | 307          | 8         | 18     | 0      |
| buick skylark 320         | 1      | 70         | \.\.\. | 350          | 8         | 15     | 1      |
| plymouth satellite        | 1      | 70         | \.\.\. | 318          | 8         | 18     | 2      |
| \.\.\.                    | \.\.\. | \.\.\.     | \.\.\. | \.\.\.       | \.\.\.    | \.\.\. | \.\.\. |
| dodge rampage             | 1      | 82         | \.\.\. | 135          | 4         | 32     | 389    |
| ford ranger               | 1      | 82         | \.\.\. | 120          | 4         | 28     | 390    |
| chevy s\-10               | 1      | 82         | \.\.\. | 119          | 4         | 31     | 391    |


```ruby
392 rows × 9 columns
```

> لتحميل قاعدة البيانات mpg.csv [اضغط هنا]({{ site.baseurl }}/files/chapter13/mpg.csv).

يبدو لنا أن اكثر من متغير يأثر على صرف السياره للوقود. مثلاً، يبدو ان صرف الوقد يقل عندما تزيد قوة الحصان للسياره:

```python
sns.lmplot(x='horsepower', y='mpg', data=mpg);
```

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter13/linear_multiple_6_0.png'>
</p>

ولكن، السيارات التي في السنوات الأخيره لديها صرف وقود افضل بشكل عام عن السيارات القديمه:

```python
sns.lmplot(x='model year', y='mpg', data=mpg);
```

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter13/linear_multiple_8_0.png'>
</p>

يظهر أن بأمكاننا الحصول على نتائج اكثر دقة للنموذج إذا استطعنا إستخدام قوة الحصان وسنة صناعة السياره للتنبؤ عن كمية صرف الوقود بالميل لكل جالون MPG. بالأصح، يبدو ان النموذج المثالي يأخذ بعين الإعتبار جميع المتغيرات الرقميه في بياناتنا. يمكننا توسيع نموذجنا الخطي ذو المتغير الواحد ليتمكن من التنبؤ بناءًا على أي عدد من المتغيرات.

ذكرنا ان تعريف النموذج كالتالي:

$$ f_\boldsymbol\theta (\textbf{x}) = \theta_0 + \theta_1 x_1 + \ldots + \theta_p x_p $$

فيها $ \textbf{x} $ تمثل متّجهه Vector تحتوي على عدد $ p $ من المتغيرات لسياره واحده. النموذج السابق يقول التالي، "خذ أكثر من متغير عن السياره، اضربهم بوزن Weight معين، ثم اجمعهم معاً للقيام بتوقع صرفية السياره للوقود بالميل لكل جالون".

> توجد انواع متعددة من طرق تمثيل الأرقام: [📝][Vector]
> <p align="center"> 
> <img src='{{ site.baseurl }}/img/chapter13/scalar-vector-matrix.png'>
> </p>
> - عدد Scalar: رقم صحيح مثلاً $ 7, -4, 0.345 $.
> - المتّجه Vector: هي مصفوفة أرقام احادية الأبعاد، تكون اما من صف واحد أو عامود واحد.
> - مصفوفه Matrix: مصفوفه أرقام تحتوي على اكثر من صف أو عامود.
>


مثلاً، إذا اردنا اجراء توقع لأول سياره في بيانتنا بإستخدام قوة الحصان، الوزن، وسنة الصناعه، فسيكون شكل المتّجهه $ \textbf{x} $ كالتالي:

| model year |  weight | horsepower    |        |
|:-------------------------:|:------:|:----------:|:------:|
| 70 | 3504.0      | 130.0         | 0 |

في هذا المثال ابقينا اسماء العواميد للتوضيح، ولكن تذكر ان $ \textbf{x} $ تحتوي فقط على القيم الرقمية من الجدول السابق: $ \textbf{x} = [130.0, 3504.0, 70] $.

الآن، سنقوم بتعريف طريقه حسابيه ستسهل العمليات الحسابيه القادمه. سنقوم بإضافة الرقم $ 1 $ إلى المتجهه $ \textbf{x} $، وسيكون شكل الصف كالتالي:

| model year |  weight | horsepower    |  bias  ||
|:-------------------------:|:------:|:----------:|:------:|:--:|
| 70 | 3504.0      | 130.0         | 1 |0|

الآن، لاحظ ما سيحدث للعملية السحابيه لنموذجنا:

$$ \begin{split}
\begin{aligned}
f_\boldsymbol\theta (\textbf{x})
&= \theta_0 + \theta_1 x_1 + \ldots + \theta_p x_p \\
&= \theta_0 (1) + \theta_1 x_1 + \ldots + \theta_p x_p \\
&= \theta_0 x_0 + \theta_1 x_1 + \ldots + \theta_p x_p \\
f_\boldsymbol\theta (\textbf{x}) &= \boldsymbol\theta \cdot \textbf{x}
\end{aligned}
\end{split} $$

فيها $ \boldsymbol\theta \cdot \textbf{x} $ هي متّجه لحاصل ضرب $ \boldsymbol\theta $ و $ \textbf{x} $. صُممت المتّجهات والمصفوفات لكتابة التركيبات الخطيه ولذلك فهي  مناسبة جداً لنموذجنا الخطي. ولكن، يجب عليك من الآن وصاعدًا تذكر ان $ \boldsymbol\theta \cdot \textbf{x} $ هي حاصل ضرب متّجه بأخرى. يمكن أيضاً لنزع الشك، توسيع عملية ضرب المتجهتين  إلى عملية جمع وضرب مبسطه.

الآن، نقوم بتعريف المصفوفه $ \textbf{X} $ والتي ستكون المصفوفة التي تحتوي على جميع انواع السيارات كصفوف، وأول عامود هو قيمة التحيز Bias. مثلاً، هذه أول خمس اسطر من المصفوفه $ \textbf{X} $:

| model year |  weight | horsepower    |  bias  ||
|:-------------------------:|:------:|:----------:|:------:|:--:|
| 70 | 3504.0      | 130.0         | 1 |0|
| 70 | 3693.0      | 165.0         | 1 |1|
| 70 | 3436.0	      | 150.0         | 1 |2|
| 70 | 3433.0	      | 150.0         | 1 |3|
| 70 | 3449.0	     | 140.0         | 1 |4|

للتذكير مره أخرى، المصفوفة الحقيقه $ \textbf{X} $ فقط تحتوي على القيم الرقميه من الجدول السابق.

لاحظ ان $ \textbf{X} $ تحتوي على أكثر من مُتجهه $ \textbf{x} $ فوق بعضها البعض. ليكون الوصف واضحاً، نقوم بتعريف $ \textbf{X}\_{i} $ والتي ترمز للمتّجه في الصف رقم $ i $ في المصفوفه $ \textbf{X} $. نقوم بتعريف $ X_{i,j} $ والتي تمثل القيمه ذات الرقم $ j $ في الصف ذو الرقم $ i $ في المصفوفه $ \textbf{X} $. لذا، $ \textbf{X}\_{i} $ هي متّجه ذات ابعاد $ p $ و $ $ \textbf{X}\_{i,j} $ هي عدد. $ $ \textbf{X} $ هي مصفوفة $ n \times p $، فيها $ n $ هي عدد السيارات لدينا و $ p $ هي عدد المتغيرات لكل سياره.

مثلاً، في الجدول السابق لدينا $ \textbf{X}\_4 = [1, 140, 3449, 70] $ و $ X_{4,1} = 140 $ لذا الرموز هي مهمه عند تعريف دوال الخساره لأننا سنحتاج إلى كلا القيمتين $ \textbf{X} $، مصفوفة البيانات المدخله للنموذج، و $ y $، متّجه صرف الوقود بالميل لكل جالون.

> - $ \textbf{X} $ هي مصفوفه Matrix.
> - $ \textbf{x} $ هي متّجه Vector وهي هنا كل صف على حده. مثلاً السطر الثاني: $ [1, 165.0, 3693.0, 70] $.
> - $ j $ هي رقم صحيح Scalar مثلاً وزن السياره في الصف الثالث $ 3436.0 $.

### خسارة الخطأ التربيعي المتوسط وإنحدارها

دالة خسارة الخطأ التربيعي المتوسط تأخذ متّجه بوزن $ \boldsymbol\theta $ و مدخلات على شكل مصفوفه $ \textbf{X} $، و متّحه لصرف الوقود بالميل لكل جالون لكل سياره $ \textbf{y} $:

$$ \begin{split}
\begin{aligned}
L(\boldsymbol\theta, \textbf{X}, \textbf{y})
&= \frac{1}{n} \sum_{i}(y_i - f_\boldsymbol\theta (\textbf{X}_i))^2\\
\end{aligned}
\end{split} $$

اوجدنا مسبقاً مشتقة دالة خسارة الخطأ التربيعي المتوسط بالنسبة ل $ \boldsymbol\theta $:

$$ \begin{split}
\begin{aligned}
\nabla_{\boldsymbol\theta} L(\boldsymbol\theta, \textbf{X}, \textbf{y})
&= -\frac{2}{n} \sum_{i}(y_i - f_\boldsymbol\theta (\textbf{X}_i))(\nabla_{\boldsymbol\theta} f_\boldsymbol\theta (\textbf{X}_i))\\
\end{aligned}
\end{split} $$

نعرف أيضاً ان:

$$ \begin{split}
\begin{aligned}
f_\boldsymbol\theta (\textbf{x}) &= \boldsymbol\theta \cdot \textbf{x} \\
\end{aligned}
\end{split} $$

لنقوم بحساب $ \nabla_{\boldsymbol\theta} f_\boldsymbol\theta (\textbf{x}) $. عملية الحساب اسهل من المتوقع لأن $ \boldsymbol\theta \cdot \textbf{x} = \theta_0 x_0 + \ldots + \theta_p x_p $ إذاً $ \frac{\partial}{\partial \theta_0}(\boldsymbol\theta \cdot \textbf{x}) = x_0 $ و $ \frac{\partial}{\partial \theta_1}(\boldsymbol\theta \cdot \textbf{x}) = x_1 $ إلى آخره:

$$ \begin{split}
\begin{aligned}
\nabla_{\boldsymbol\theta} f_\boldsymbol\theta (\textbf{x})
&= \nabla_{\boldsymbol\theta} [ \boldsymbol\theta \cdot \textbf{x} ] \\
&= \begin{bmatrix}
     \frac{\partial}{\partial \theta_0} (\boldsymbol\theta \cdot \textbf{x}) \\
     \frac{\partial}{\partial \theta_1} (\boldsymbol\theta \cdot \textbf{x}) \\
     \vdots \\
     \frac{\partial}{\partial \theta_p} (\boldsymbol\theta \cdot \textbf{x}) \\
   \end{bmatrix} \\
&= \begin{bmatrix}
     x_0 \\
     x_1 \\
     \vdots \\
     x_p
   \end{bmatrix} \\
\nabla_{\boldsymbol\theta} f_\boldsymbol\theta (\textbf{x}) &= \textbf{x}
\end{aligned}
\end{split} $$

اخيراً، نقوم بإدخال النتيجه لحساب الخطيه:

$$ \begin{split}
\begin{aligned}
\nabla_{\boldsymbol\theta} L(\boldsymbol\theta, \textbf{X}, \textbf{y})
&= -\frac{2}{n} \sum_{i}(y_i - f_\boldsymbol\theta (\textbf{X}_i))(\nabla_{\boldsymbol\theta} f_\boldsymbol\theta (\textbf{X}_i))\\
&= -\frac{2}{n} \sum_{i}(y_i - \boldsymbol\theta \cdot \textbf{X}_i)(\textbf{X}_i)\\
\end{aligned}
\end{split} $$

تذكر أنه بما ان $ y_i - \boldsymbol\theta \cdot \textbf{X}\_i $ هي عدد و $ \textbf{X}_i $ هي متّجه ذات $ p $ ابعاد، الخطيه $ \nabla\_{\boldsymbol\theta} L(\boldsymbol\theta, \textbf{X}, \textbf{y}) $ هي ايضاً متّجه ذات $ p $ ابعاد.

رأينا نفس هذا النوع من النتائج عندما قمنا بحساب خطية الإنحدار الخطي ووجدنا انها ثنائية الأبعاد بما ان $ \boldsymbol\theta $ كانت ثنائية الأبعاد.

### ضبط النموذج الخطي مع النزول الإشتقاقي

يمكننا الآن إدخال الخساره ومشتقتها إلى دالة النزول الإشتقاقي. كالعاده، سنقوم بتعريف النموذج، دالة الخساره و دالة النزول الإشتقاقيمشتقتها في بايثون:

```python
def linear_model(thetas, X):
    '''Returns predictions by a linear model on x_vals.'''
    return  X @ thetas

def mse_loss(thetas, X, y):
    return np.mean((y - linear_model(thetas, X)) ** 2)

def grad_mse_loss(thetas, X, y):
    n = len(X)
    return -2 / n * (X.T @ y  - X.T @  X @ thetas)
```

الآن، ببساطه يمككنا ادخال دوالنا إلى النزول الإشتقاقي:

```python
X = (mpg_mat
     .loc[:, ['bias', 'horsepower', 'weight', 'model year']]
     .to_numpy())
y = mpg_mat['mpg'].to_numpy()

thetas = minimize(mse_loss, grad_mse_loss, X, y)
print(f'theta: {thetas} | loss: {mse_loss(thetas, X, y):.2f}')
```

```ruby
theta: [ 0.  0.  0.  0.] | cost: 610.47
theta: [ 0.    0.    0.01  0.  ] | cost: 178.95
theta: [ 0.01 -0.11 -0.    0.55] | cost: 15.78
theta: [ 0.01 -0.01 -0.01  0.58] | cost: 11.97
theta: [-4.   -0.01 -0.01  0.63] | cost: 11.81
theta: [-13.72  -0.    -0.01   0.75] | cost: 11.65
theta: [-13.72  -0.    -0.01   0.75] | cost: 11.65
```

> استخدم الكاتب في الكود البرمجي السابق الدالة `minimize` وهي مختلفه قليلاً عن السابقه، اجري عليها التعديلات التاليه:
>
> ```python
> from scipy.optimize import minimize as sci_min
> def minimize(loss_fn, grad_loss_fn, X, y, progress=True):
>     '''
>     تستخدم دالة من مكتبة scipy للتقليل من خسارة الداله loss_fun 
>     بإستخدام نموذج من النزول الإشتقاقي
>     '''
>    theta = np.zeros(X.shape[1])
>    iters = 0
>    
>    def objective(theta):
>        return loss_fn(theta, X, y)
>    def gradient(theta):
>        return grad_loss_fn(theta, X, y)
>    def print_theta(theta):
>        nonlocal iters
>        if progress and iters % progress == 0:
>            print(f'theta: {theta} | loss: {loss_fn(theta, X, y):.2f}')
>        iters += 1
>        
>    print_theta(theta)
>    return sci_min(
>        objective, theta, method='BFGS', jac=gradient, callback=print_theta,
>        tol=1e-7
>    ).x
> ```

بناءًا على النزول الإشتقاقي، فأن نموذجنا الخطي هو كالتالي:

$$ y = -13.72 - 0.01x_2 + 0.75x_3 $$

### الرسم البياني لتنبؤاتنا

كيف أدى نموذجنا؟ نلاحظ ان الخساره قلت بشكل كبير (من 610 حتى 11.6). يمكننا طباعة نتائج توقع النموذج بجانب النتائج الحقيقه:

```python
reordered = ['predicted_mpg', 'mpg', 'horsepower', 'weight', 'model year']
with_predictions = (
    mpg
    .assign(predicted_mpg=linear_model(thetas, X))
    .loc[:, reordered]
)
with_predictions
```

| model year | weight | horsepower | mpg    | predicted\_mpg |        |
|:----------:|:------:|:----------:|:------:|:--------------:|:------:|
| 70         | 3504   | 130        | 18     | 15\.447125     | 0      |
| 70         | 3693   | 165        | 15     | 14\.053509     | 1      |
| 70         | 3436   | 150        | 18     | 15\.785576     | 2      |
| \.\.\.     | \.\.\. | \.\.\.     | \.\.\. | \.\.\.         | \.\.\. |
| 82         | 2295   | 84         | 32     | 32\.4569       | 389    |
| 82         | 2625   | 79         | 28     | 30\.354143     | 390    |
| 82         | 2720   | 82         | 31     | 29\.726608     | 391    |

```ruby
392 rows × 5 columns
```

بما اننا أوجدنا $ \boldsymbol\theta $ من النزول الإشتقاقي، يمكننا ان نتأكد من اول سطر في بياناتنا ان $ \boldsymbol\theta \cdot \textbf{X}_0 $ تطابق توقعنا السابق:

```python
print(f'Prediction for first row: '
      f'{thetas[0] + thetas[1] * 130 + thetas[2] * 3504 + thetas[3] * 70:.2f}')
```

```ruby
Prediction for first row: 15.45
```

يمكننا رسم الفرق بين توقعنا والنتيجة الحقيقه (النتيجة الحقيقه - التوقع):

```python
resid = y - linear_model(thetas, X)
plt.scatter(np.arange(len(resid)), resid, s=15)
plt.title('Residuals (actual MPG - predicted MPG)')
plt.xlabel('Index of row in data')
plt.ylabel('MPG');
```

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter13/linear_multiple_34_0.png'>
</p>

يبدو واضحاً ان نموذجنا يعطي توقعات منطقيه لكثير من السيارات، على الرغم ان بعض النتائج كان الفرق فيها أكثر من 10 ميل لكل جالون (بعض السيارات لديها اقل من 10!). قد يهمنا اكثر معرفة نسبة الخط بين التوقع والنتيجة الصحيحه لصرف الوقود:

```python
resid_prop = resid / with_predictions['mpg']
plt.scatter(np.arange(len(resid_prop)), resid_prop, s=15)
plt.title('Residual proportions (resid / actual MPG)')
plt.xlabel('Index of row in data')
plt.ylabel('Error proportion');
```

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter13/linear_multiple_36_0.png'>
</p>

يظهر لنا ان النموذج عادةً ابعد بحدود 20% من القيمه الصحيحه.

### استخدام كامل البيانات

لاحظ ان في مثالنا حتى الآن، المصفوفه $ \textbf{X} $ تحتوي على اربع عواميد: أولها يحتوي على القيمه 1 في جميع الصفوف، عامود قوة السياره بالحصان، وزنها، وسنة الصناعه. ولكن، يسمح لنا النموذج بإستخدام اكثر من هذا العدد:

$$ \begin{aligned}
f_\boldsymbol\theta (\textbf{x}) &= \boldsymbol\theta \cdot \textbf{x}
\end{aligned} $$

عندما نضيف المزيد من العواميد لمصفوفتنا، نقوم بتوسيع $ \boldsymbol\theta $ لتحتوي على متغير لكل عامود في $ \textbf{x} $. بدلاً من اختيار فقط 3 عواميد رقميه لإجراء التنبؤ، لماذا لا نستخدم جميع العواميد السبعه؟

```python
cols = ['bias', 'cylinders', 'displacement', 'horsepower',
        'weight', 'acceleration', 'model year', 'origin']
X = mpg_mat[cols].to_numpy()
mpg_mat[cols]
```

| origin | model year | acceleration | weight | horsepower | displacement | cylinders | bias   |        |
|:------:|:----------:|:------------:|:------:|:----------:|:------------:|:---------:|:------:|:------:|
| 1      | 70         | 12           | 3504   | 130        | 307          | 8         | 1      | 0      |
| 1      | 70         | 11\.5        | 3693   | 165        | 350          | 8         | 1      | 1      |
| 1      | 70         | 11           | 3436   | 150        | 318          | 8         | 1      | 2      |
| \.\.\. | \.\.\.     | \.\.\.       | \.\.\. | \.\.\.     | \.\.\.       | \.\.\.    | \.\.\. | \.\.\. |
| 1      | 82         | 11\.6        | 2295   | 84         | 135          | 4         | 1      | 389    |
| 1      | 82         | 18\.6        | 2625   | 79         | 120          | 4         | 1      | 390    |
| 1      | 82         | 19\.4        | 2720   | 82         | 119          | 4         | 1      | 391    |

```ruby
392 rows × 8 columns
```

```python
thetas_all = minimize(mse_loss, grad_mse_loss, X, y, progress=10)
print(f'theta: {thetas_all} | loss: {mse_loss(thetas_all, X, y):.2f}')
```

```ruby
theta: [0. 0. 0. 0. 0. 0. 0. 0.] | loss: 610.47
theta: [-0.5  -0.81  0.02 -0.04 -0.01 -0.07  0.59  1.3 ] | loss: 11.22
theta: [-17.23  -0.49   0.02  -0.02  -0.01   0.08   0.75   1.43] | loss: 10.85
theta: [-17.22  -0.49   0.02  -0.02  -0.01   0.08   0.75   1.43] | loss: 10.85
```

وفقاً لنتيجة النزول الإشتقاقي، فأن النموذج الخطي يمكننا تعريفه كالتالي:

$$ y = -17.22 - 0.49x_1 + 0.02x_2 - 0.02x_3 - 0.01x_4 + 0.08x_5 + 0.75x_6 + 1.43x_7 $$

نلاحظ ان خسارتنا قلت من 11.6 بإستخدام ثلاث عواميد إلى 10.85 بإستخدام جميع العواميد الرقمية السبعه في بياناتنا. سنرى نسبة الخطأ في الرسم البياني لكل التوقعين السابق (بإستخدام ثلاث عواميد) والجديد (بإستخدام سبع عواميد):

```python
resid_prop_all = (y - linear_model(thetas_all, X)) / with_predictions['mpg']
plt.figure(figsize=(10, 4))
plt.subplot(121)
plt.scatter(np.arange(len(resid_prop)), resid_prop, s=15)
plt.title('Residual proportions using 3 columns')
plt.xlabel('Index of row in data')
plt.ylabel('Error proportion')
plt.ylim(-0.7, 0.7)

plt.subplot(122)
plt.scatter(np.arange(len(resid_prop_all)), resid_prop_all, s=15)
plt.title('Residual proportions using 7 columns')
plt.xlabel('Index of row in data')
plt.ylabel('Error proportion')
plt.ylim(-0.7, 0.7)

plt.tight_layout();
```

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter13/linear_multiple_42_0.png'>
</p>

على الرغم ان الفرق بسيط، نلاحظ ان الفرق اقل عندما نستخدم السبع عواميد. كلا النموذجين افضل من النموذج الثابت، كما يوضح الرسم البياني التالي:

```python
constant_resid_prop = (y - with_predictions['mpg'].mean()) / with_predictions['mpg']
plt.scatter(np.arange(len(constant_resid_prop)), constant_resid_prop, s=15)
plt.title('Residual proportions using constant model')
plt.xlabel('Index of row in data')
plt.ylabel('Error proportion')
plt.ylim(-1, 1);
```

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter13/linear_multiple_44_0.png'>
</p>

استخدام النموذج الثابت وصلت فيه نتائج الخطأ إلى أكثر من 75% لكثير من السيارات!

### ملخص الإنحدار الخطي المتعدد

تعرفنا على النموذج الخطي للإنحدار. على عكس النموذج الثابت، الإنحدار الخطي يأخذ خصائص من البيانات بالحسبان عند اجراء التوقعات، مما يجعله اكثر فائدة عندما يكون لدينا علاقات في بياناتنا.

خطوات ضبط النموذج من المفترض ان تكون واضحه الآن:
- اختيار النموذج.
- اختيار دالة الخسارة.
- تقليل دالة الخساره بإستخدام النزول الإشتقاقي.

من المفيد معرفة ان بإمكاننا التعديل على احد المكونات دون الأخرى. في هذا الجزء، تعرفنا على النموذج الخطي دون التغير في دالة الخساره أو استخدام خوارزميات تقليل اخرى. على الرغم ان النمذجه قد تكون معقده، يكون اسهل التركيز على مكون واحد فقط في كل مره، ثم جمع المكونات مع بعضها البعض.

## المربعات الصغرى - منظور هندسي

لنتذكر اننا اوجدنا المتغيرات الرياضيه المثاليه للنموذج الخطي بواسطة التحسين من دالة الخساره بإستخدام النزول الإشتقاقي. ذكرنا أيضاً ان الإنحدار الخطي للمربعات الصغرى يمكن حسابه تحليلياً. على الرغم ان النزول الإشتقاقي اكثر عملياً، هذا المنظور الهندسي سيساعدك على فهم الإنحدار الخطي بشكل اكبر.

يتوقع من القارئ ان يكون على علم بفضاء المتجهات  Vector Space وطريقة القيام بالعمليات الحسابيه عليها.

لنفترض اننا نبحث عن النموذج الخطي للبيانات التاليه:

**y**|**x**
:-----:|:-----:
2|3
1|0
-2|-1

```python
data = pd.DataFrame(
    [
        [3,2],
        [0,1],
        [-1,-2]
    ],
    columns=['x', 'y']
)

sns.regplot(x='x', y='y', data=data, ci=None, fit_reg=False);
```

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter13/linear_projection_5_0.png'>
</p>

لنفترض أن النموذج المثالي هو النموذج بأقل خساره، وان خطأ المربعات الصغرى هي اداة مقبولة للقياس.

### المربعات الصغرى: النموذج الثابت

كما فعلنا في بيانات الإكراميات، لنبدأ بالنموذج الثابت: نموذج يتوقع رقم واحد فقط.

$$ \theta = C $$

نعمل نحن فقط مع قيم $ y $:

|**y**|
|:-----:|
|2|
|1|
|-2|

وهدفنا هو إيجاد قيمة $ \theta $ التي تنتج لنا خطاً يقلل من الخساره التربيعيه:

$$ \begin{split} L(\theta, \textbf{y}) = \sum_{i = 1}^{n}(y_i - \theta)^2\\ \end{split} $$

لنتذكر ان للنموذج الثابت، قيمة $ \theta $ التي تقلل الخساره في دالة الخطأ التربيعي المتوسط MSE هي $ \bar{\textbf{y}} $، متوسط قيم $ \textbf{y} $. يمكن إيجاد العملية الحسابية الكامله في درس [دوال الخساره](/chapter10/#%D8%AF%D8%A7%D9%84%D8%A9%20%D8%A7%D9%84%D8%AE%D8%B3%D8%A7%D8%B1%D9%87) في فصل [النماذج والتوقعات](/ds-100-ar/chapter10/). 

لاحظ ان دالة الخساره لدينا هي مجمع التربيع. القاعدة L2 للمتجهات هي ايضاً مجموع التربيع، ولكن مع الجذر التربيعي:

$$ \Vert \textbf{v} \Vert = \sqrt{v_1^2 + v_2^2 + \dots + v_n^2} $$

إذا جعلنا $ y_i - \theta = v_i $:

$$ \begin{split}
\begin{aligned}
L(\theta, \textbf{y}) 
&= v_1^2 + v_2^2 + \dots + v_n^2 \\
&= \Vert \textbf{v} \Vert^2
\end{aligned}
\end{split} $$

يعني ذلك ان بإمكاننا تعريف الخساره على انها تربيع القاعدة L2 لمتّجه $ \textbf{v} $. يمكننا وصف $ v_i $ كالتالي $ y_i - \theta \quad \forall i \in [1,n] $ إذاً ذلك في كتعريف ديكارتي:

$$ \begin{split}
\begin{aligned}
\textbf{v} \quad &= \quad \begin{bmatrix} y_1 - \theta \\ y_2 - \theta \\ \vdots \\ y_n - \theta \end{bmatrix} \\
&= \quad \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n  \end{bmatrix} \quad - \quad 
\begin{bmatrix} \theta \\ \theta \\ \vdots \\ \theta \end{bmatrix} \\
&= \quad \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n  \end{bmatrix} \quad - \quad 
\theta \begin{bmatrix} 1 \\ 1 \\ \vdots \\ 1 \end{bmatrix}
\end{aligned}
\end{split} $$

إذاً، يمكننا كتابة دالة الخساره كالتالي:

$$ \begin{split} 
\begin{aligned}
L(\theta, \textbf{y})
\quad &= \quad \left \Vert  \qquad   
\begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n  \end{bmatrix} \quad - \quad 
\theta \begin{bmatrix} 1 \\ 1 \\ \vdots \\ 1 \end{bmatrix}
\qquad \right \Vert ^2 \\
\quad &= \quad \left \Vert  \qquad  
\textbf{y} 
\quad - \quad 
\hat{\textbf{y}}
\qquad \right \Vert ^2 \\
\end{aligned}
\end{split} $$

الوصف $$ \theta \begin{bmatrix} 1 \\ 1 \\ \vdots \\ 1 \end{bmatrix} $$ هو مضاعف عددي للعواميد في المتّجه $ \textbf{1} $، وهي ايضاً نتيجة التوقع، ويرمز لها $ \hat{\textbf{y}} $.

يعطينا ذلك منظوراً جديداً عن معنى تقليل الخساره في خطأ المربعات الصغرى.

قيم $ \textbf{y} $ و $ \textbf{1} $ ثابته، ولكن $ \theta $ يمكن أن تأخذ أي قيمة، لذا $ \hat{\textbf{y}} $ يمكن انت تكون اي مضاعف عددي ل $ \textbf{1} $. نريد إيجاد $ \theta $ لتكون $ \theta \textbf{1} $ اقرب ما تكون إلى $ \textbf{y} $. نستخدم $ \hat{\theta} $ لوصف ذلك الضبط المثالي ل $ \theta $.

### المربعات الصغرى: نموذج خطي بسيط

الآن، لنلقي نظره على نموذج الإنحدار الخطي البسيط. يشبه النموذج بشكل كبير لإشتقاق النموذج الثابت، ولكن لاحظ الفرق وفكر بطريقة عامة لإجراء الإنحدار الخطي المتعدد.

النموذج الخطي البسيط هو:

$$ \begin{split}
\begin{aligned}
f_\boldsymbol\theta (x_i) 
&= \theta_0 + \theta_1 x_i \\
\end{aligned}
\end{split} $$

هدفنا إيجاد $ \boldsymbol\theta $ التي تنتج لنا خطاً بأقل خطأ تربيعي:

$$ \begin{split}
\begin{aligned}
L(\boldsymbol\theta, \textbf{x}, \textbf{y})
&= \sum_{i = 1}^{n}(y_i - f_\boldsymbol\theta (x_i))^2\\
&= \sum_{i = 1}^{n}(y_i - \theta_0 - \theta_1 x_i)^2\\
&= \sum_{i = 1}^{n}(y_i - \begin{bmatrix} 1 & x_i \end{bmatrix}
\begin{bmatrix} 
     \theta_0 \\
     \theta_1
\end{bmatrix} ) ^2
\end{aligned}
\end{split} $$

لمساعدتنا على تحويل شكل جمع الخساره إلى شكل مصفوفه، دعنا نوسع من الخساره ب $ n = 3 $:

$$ \begin{split}
\begin{aligned}
L(\boldsymbol{\theta}, \textbf{x}, \textbf{y})
&=
(y_1 - \begin{bmatrix} 1 & x_1 \end{bmatrix}
\begin{bmatrix} 
     \theta_0 \\
     \theta_1
\end{bmatrix})^2  \\
&+
(y_2 - \begin{bmatrix} 1 & x_2 \end{bmatrix}
\begin{bmatrix} 
     \theta_0 \\
     \theta_1
\end{bmatrix})^2 \\
&+
(y_3 - \begin{bmatrix} 1 & x_3 \end{bmatrix}
\begin{bmatrix} 
     \theta_0 \\
     \theta_1
\end{bmatrix})^2 \\
\end{aligned}
\end{split} $$

مرة أخرى، دالة الخساره هي مجموع التربيع و القاعدة L2 للمتّجه هي الجذر التربيعي لمجموع التربيع:

$$ \Vert \textbf{v} \Vert = \sqrt{v_1^2 + v_2^2 + \dots + v_n^2} $$

إذا جعلنا $$ y_i - \begin{bmatrix} 1 & x_i \end{bmatrix}
\begin{bmatrix} 
     \theta_0 \\
     \theta_1
\end{bmatrix} $$ $ v_i = $: 

$$ \begin{split}
\begin{aligned}
L(\boldsymbol{\theta}, \textbf{x}, \textbf{y}) 
&= v_1^2 + v_2^2 + \dots + v_n^2 \\
&= \Vert \textbf{v} \Vert^2
\end{aligned}
\end{split} $$

كما في السابق، يمكننا وصف خسارتنا على انها تربيع القاعدة L2 للمتّجه $ \textbf{v} $. 

$$ v_i = y_i - \begin{bmatrix} 1 & x_i \end{bmatrix}
\begin{bmatrix} 
     \theta_0 \\
     \theta_1
\end{bmatrix} \quad \forall i \in [1,3] $$

$$ 
\begin{aligned}
L(\boldsymbol{\theta}, \textbf{x}, \textbf{y})
&= \left \Vert  \qquad   
\begin{bmatrix} y_1 \\ y_2 \\ y_3  \end{bmatrix} \quad - \quad 
\begin{bmatrix} 1 & x_1 \\ 1 & x_2 \\ 1 & x_3 \end{bmatrix}
\begin{bmatrix} 
     \theta_0 \\
     \theta_1
\end{bmatrix}
\qquad \right \Vert ^2 \\
&= \left \Vert  \qquad  
\textbf{y} 
\quad - \quad 
\textbf{X}
\begin{bmatrix} 
     \theta_0 \\
     \theta_1
\end{bmatrix}
\qquad \right \Vert ^2 \\
&= \left \Vert  \qquad  
\textbf{y} 
\quad - \quad 
f_\boldsymbol\theta(\textbf{x})
\qquad \right \Vert ^2 \\
&= \left \Vert  \qquad  
\textbf{y} 
\quad - \quad 
\hat{\textbf{y}}
\qquad \right \Vert ^2 \\
\end{aligned}
$$

عملية ضرب المصفوفة $$ \begin{bmatrix} 1 & x_1 \\ 1 & x_2 \\ 1 & x_3 \end{bmatrix}
\begin{bmatrix} 
     \theta_0 \\
     \theta_1
\end{bmatrix} $$ هي تركيبة خطية للعواميد في $ \textbf{X} $: كل $ \theta_i $ يتم ضربها بعامود واحد من $ \textbf{X} $، يظهر لنا هذا المنظور ان $ f_\boldsymbol\theta $ تركيبة خطية للخصائص في بياناتنا.

$ \textbf{X} $ و $ \textbf{y} $ ثابتين، ولكن $ \theta_0 $ و $ \theta_1 $ يمكن أن يأخذان أي قيمه، لذا $ \hat{\textbf{y}} $ يمكن ان تأخذ اياً من التركيبات الخطية العواميد في $ \textbf{X} $. للحصول على أقل خساره، تريد أن نختار $ \boldsymbol\theta $ التي فيها $ \hat{\textbf{y}} $ أقرب ما تكون إلى $ \textbf{y} $، يرمز لها بالرمز $ \hat{\boldsymbol\theta} $.

### الفكره الهندسيه

الآن، لنحاول تكوين فكرة عن اهمية ان تكون $ \hat{\textbf{y}} $ محدوده للتركيبات الخطية للعواميد في $ \textbf{X} $. على الرغم ان مدى اي متّجه يحتوي على عدد غير محدود من التركيبات الخطية، غير محدود لا تعني اياً كان، التركيبات الخطية محدوده بواسطة أساس المتّجه.

للتذكير، هذه دالة الخساره ومخطط اَلتَّشَتُّت:

$$ L(\boldsymbol{\theta}, \textbf{x}, \textbf{y}) \quad = \quad \left \Vert  \quad  
\textbf{y} 
\quad - \quad 
\textbf{X} \boldsymbol\theta
\quad \right \Vert ^2 $$

```python
sns.regplot(x='x', y='y', data=data, ci=None, fit_reg=False);
```

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter13/linear_projection_19_0.png'>
</p>

من خلال مشاهدتنا لمخطط التشتت، نلاحظ انه لا يوجد خط مثالي للنقاط، لذا لن نستطيع الوصول إلى خساره تساوي صفر. نعرف ان $ \textbf{y} $ ليست على مستوى خطي مع $ \textbf{x} $ و $ \textbf{1}:

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter13/proj1.png'>
</p>

بما ان حساب الخساره يكون بالمسافه، يمكننا ملاحظة انه لتقليل الخساره $ L(\boldsymbol\theta, \textbf{x}, \textbf{y}) = \left \Vert  \textbf{y} - \textbf{X} \boldsymbol\theta \right \Vert ^2 $، فنريد ان تكون $ \textbf{X} \boldsymbol\theta $ اقرب ما تكون إلى $ \textbf{y} $.

رياضياً، نحنى نرى توقع $ \textbf{y} $ في فضاء المتّجه الممتد بواسطة العواميد في $ \textbf{X} $، لأن التوقع لأي متّجه هي اقرب نقطة في $ Span(\textbf{X})$ للمتّجه. لذا، إختيار $ \boldsymbol\theta $ يكون فيها $ \hat{\textbf{y}} $ $ \textbf{X}  \boldsymbol\theta = $ $ \mathit{proj}_{Span(\textbf{X})}  \textbf{y} = $ هو الحل المثالي.

> proj = projection = التوقع

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter13/proj2.png'>
</p>

لنعرف لماذا، لنأخذ بالإعتبار النقاط الأخرى في فضاء المتّجه، النقاط باللون البنفسجي:

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter13/proj3.png'>
</p>

بناءًا على نظرية فيثاغورس، اي نقطة على السطح هي ابعد عن $ \textbf{y} $ من $ \hat{\textbf{y}} $. طول العامود المقابل ل $ \hat{\textbf{y}} $ هو خطأ المربعات الصغرى. 

### الجبر الخطي

تحدثنا بشكل كبير عن الجبر الخطي، ما تبقى لنا الآن هو حل $ \hat{\boldsymbol\theta} $ التي تكون لنا ما نبحث عنه $ \hat{\textbf{y}} $

بعض النقاط نأخذها بالإعتبار:

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter13/proj4.png'>
</p>

- $ \hat{\textbf{y}} + \textbf{e} = \textbf{y} $
- $ \textbf{e} $ متوازيه عامودياً مع $ \textbf{x} $ و $ \textbf{1} $
- $ \hat{\textbf{y}} = \textbf{X} \hat{\boldsymbol\theta} $ هي المتّجه الأقرب إلى $ \textbf{y} $ في فضاء المتّجهات الممتدة من $ \textbf{x} $ و $ \textbf{1} $

ولذا، تنتج لنا المعادله التاليه:

$$ \textbf{X}  \hat{\boldsymbol\theta} + \textbf{e} = \textbf{y} $$

ضرب الجهه اليسرى لكل القيم ب $ \textbf{X}^T $ ينتج لنا التالي:

$$ \textbf{X}^T \textbf{X}  \hat{\boldsymbol\theta} + \textbf{X}^T \textbf{e} = \textbf{X}^T \textbf{y} $$

بما أن $ \textbf{e} $ متعامده مع العواميد في $ \textbf{X} $، فأن $ \textbf{X}^T \textbf{e} $ هو عامود متّجه يحتوي على اصفار. لذا، نصل إلى المعادلة التاليه:

$$ \textbf{X}^T \textbf{X}  \hat{\boldsymbol\theta} = \textbf{X}^T \textbf{y} $$

من هنا، يمكننا بسهولة الحل لإيجاد $ \hat{\boldsymbol\theta} $ بواسطة ضرب الجهه اليسرى في كلا الجانبين ب $ (\textbf{X}^T \textbf{X})^{-1} $:

$$ \hat{\boldsymbol\theta} = (\textbf{X}^T \textbf{X})^{-1} \textbf{X}^T \textbf{y} $$

ملاحظة: يمكننا الحصول على نفس النتجية بواسطة التقليل بإستخدام متجهات التفاضل والتكامل، ولكن بالنسبة لخطأ المربعات الصغرى، متجهات التفاضل والتكامل ليست ضروريه. لدوال الخساره الأخلى، سنحتاج لإستخدام متجهات التفاضل والتكامل للحصول على النتيجة التحليليه.

### انهاء الدراسه

لنعود لتجربتنا، لنطبق ما تعلمناه، ونشرح إجابتنا:

$$ \begin{split}
\textbf{y} = \begin{bmatrix} 2 \\ 1 \\ -2  \end{bmatrix} \qquad \textbf{X} = \begin{bmatrix} 1 & 3 \\ 1 & 0 \\ 1 & -1 \end{bmatrix}
\end{split} $$

$$ \begin{split}
\begin{align}
\hat{\boldsymbol\theta} 
&= 
\left(
\begin{bmatrix} 1 & 1 & 1 \\ 3 & 0 & -1 \end{bmatrix}
\begin{bmatrix} 1 & 3 \\ 1 & 0 \\ 1 & -1 \end{bmatrix}
\right)^{-1}
\begin{bmatrix} 1 & 1 & 1 \\ 3 & 0 & -1 \end{bmatrix}
\begin{bmatrix} 2 \\ 1 \\ -2  \end{bmatrix} \\
&= 
\left(
\begin{bmatrix} 3 & 2\\ 2 & 10 \end{bmatrix}
\right)^{-1}
\begin{bmatrix} 1 \\ 8 \end{bmatrix} \\
&=
\frac{1}{30-4}
\begin{bmatrix} 10 & -2\\ -2 & 3 \end{bmatrix}
\begin{bmatrix} 1 \\ 8 \end{bmatrix} \\
&=
\frac{1}{26}
\begin{bmatrix} -6 \\ 22 \end{bmatrix}\\
&=
\begin{bmatrix} - \frac{3}{13} \\ \frac{11}{13} \end{bmatrix}
\end{align}
\end{split} $$

قمنا تحليلياً بإيجاد النموذج المثالي لإنحدار المربعات الصغرى وهو $ f_\boldsymbol{\boldsymbol\theta}(x_i) = - \frac{3}{13} + \frac{11}{13} x_i $. نعرف ان اختيارنا ل $ \boldsymbol\theta $ صحيح بإستخدام الخاصيه الرياضية التي تقول توقع $ \textbf{y} $ لأمتداد العواميد في $ \textbf{X} $ ينتج لنا اقرب نقطة في فضاء المتّجه ل $ \textbf{y} $. تحت القيود الخطيه بإستخدام خسارة المربعات الصغرى، الحل ل $ \hat{\boldsymbol\theta} $ ينتج لنا توقع مضمون انه الحل الأفضل.

### عندما تعتمد المتغيرات خطياً

لكل متغير اضافي، نضيف عامود جديد إلى $ \textbf{X} $. امتداد عواميد ل $ \textbf{X} $ هو التركيب الخطي لعواميد المتّجهات، لذا إضافة أعمدة تغير من الإمتداد فقط اذا كانت مستقلة خطياً عن بقية الأعمدة الموجوده مسبقاً.

عندما يكون العامود المضاف غير مستقل خطياً، يمكن وصفة كتركيب خطي من احد العواميد الأخرى، لذا، لن يكون لنا اي متّجه جديده في الفضاء الجزئي.

لنتذكر ان امتداد $ \textbf{X} $ مهم لأنه الفضاء الجزئي الذي نريد ان نتوقع $ \textbf{y} $ فيه. إذا لم يتغير هذا الفضاء، فأن التوقع لن يتغير.

مثلاً، عندما عرفنا $ \textbf{x} $ على النموذج الثابت لنحصل على النموذج الخطي البسيط، عرفنا دالة مستقله. $$ \textbf{x} = \begin{bmatrix} 3 \\ 0 \\ -1 \end{bmatrix} $$ لا يمكن وصفها كعدديه من $$ \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix} $$. لذا، انتقلنا من إيجاد توقع ل $ \textbf{y} $ في الخط:

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter13/1dprojection.png'>
</p>

إلى ايجاد توقع ل $ \textbf{y} $ على السطح:

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter13/proj1.png'>
</p>

الآن، لنعرف متغير آخر، $ \textbf{z} $، يكون بشكل واضح هذا العامود متحيز:

**y**|**x**|**1**|**z**
:-----:|:-----:|:-----:|:-----:
2|3|1|4
1|0|1|1
-2|-1|1|0

لاحظ ان $ \textbf{z} = \textbf{1} + \textbf{x} $. بما ان $ \textbf{z} $ متركبة خطياً من $ \textbf{1} $ و $ \textbf{x} $، فأنها تقع في $ Span(\textbf{X}) $. الآن، $ \textbf{z} $ معتمده خطياً على $ \\{\textbf{1} ,\textbf{x}\\} $ ولا تغيّر في $ Span(\textbf{X}) $. لذا، توقع $ \textbf{y} $ في الفضاء الجزئي الممتد بواسطة $ \textbf{1} $، $ \textbf{x} $ و $ \textbf{z} $ سيكون مطابق لتوقع $ \textbf{y} $ في الفضاء الجزئي الممتد بواسطة $ \textbf{1} $ و $ \textbf{x} $.

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter13/dependent_variablesz.png'>
</p>

يمكننا ايضاً ملاحظة ذلك عند تقليل دالة الخساره:

$$ \begin{split} 
\begin{aligned}
L(\boldsymbol\theta, \textbf{d}, \textbf{y})
&= \left \Vert  \qquad   
\begin{bmatrix} y_1 \\ y_2 \\ y_3  \end{bmatrix} \quad - \quad 
\begin{bmatrix} 1 & x_1 & z_1 \\ 1 & x_2 & z_2\\ 1 & x_3 & z_3\end{bmatrix}
\begin{bmatrix} 
     \theta_0 \\
     \theta_1 \\
     \theta_2
\end{bmatrix}
\qquad \right \Vert ^2
\end{aligned}
\end{split} $$

الحل المتوقع لنا سيكون كالشكل التالي $ \theta_0 \textbf{1} + \theta_1 \textbf{x} + \theta_2 \textbf{z} $.

بما ان $ \textbf{z} = \textbf{1} + \textbf{x} $، ايضاً كانت $ \theta_0 $، $ \theta_1 $ و $ \theta_2 $، القيم المتوقه يمكن إعادة كتابتها كالتالي:

$$ \begin{split}
\begin{aligned}
\theta_0 \textbf{1} + \theta_1 \textbf{x} + \theta_2 (\textbf{1} + \textbf{x})
&= 
(\theta_0 + \theta_2) \textbf{1} + (\theta_1 + \theta_2) \textbf{x} \\
\end{aligned}
\end{split} $$

إذاً، اضافة $ \textbf{z} $ لن تغير اي شيء. الفرق الوحيد هو ان بإمكاننا وصف هذا التوقع بعدة أشكال. لنتذكر اننا اوجدنا توقع $ \textbf{y} $ على امتداد $ \textbf{1} $ و $ \textbf{x} $ وكان:

$$ \begin{split} \begin{bmatrix} \textbf{1} & \textbf{x} \end{bmatrix}  \begin{bmatrix} - \frac{3}{13} \\ \frac{11}{13} \end{bmatrix} = - \frac{3}{13} \textbf{1} + \frac{11}{13} \textbf{x}\end{split} $$

لكن، مع التعرف على $ \textbf{z} $، يمكننا وصف توقع المتّجه بأكثر من طريقه.

بما ان $ \textbf{1} = \textbf{z} - \textbf{x} $، ف $ \hat{\textbf{y}} $ يمكن وصفها كالتالي:

$$ - \frac{3}{13} (\textbf{z} - \textbf{x}) + \frac{11}{13} \textbf{x} = - \frac{3}{13} \textbf{z} + \frac{14}{13} \textbf{x} $$

بما ان $ \textbf{x} = \textbf{z} + \textbf{1} $، ف $ \hat{\textbf{y}} $ يمكن وصفها كالتالي:

$$ - \frac{3}{13} \textbf{1} + \frac{11}{13} (\textbf{z} + \textbf{1}) = \frac{8}{13} \textbf{1} + \frac{11}{13} \textbf{z} $$

ولكن جميع الأوصاف الثلاثه تقدم نفس التوقع.

في الختام، اضافة عامود غير مستقل خطياً إلى $ \textbf{x} $ لا يغير في $ Span(\textbf{X}) $، ولذلك لن يغير في التوقع والحل لمشكلة المربعات الصغرى.

### طريقتين للتفكير

اضفنا مخططات اَلتَّشَتُّت مرتين في هذا الدرس. اول مره للتذكير انه كما في السابق، نحاول ايجاد الخط المثالي للبيانات. المره الثانية كانت تأكد انه لا يوجد خط مثالي لجميع النقاط. بصرف النظر هن ذلك، حاولنا عدم تخريب رسم فضاء المتّجه بمخططات اَلتَّشَتُّت. ذلك لأن مخططات اَلتَّشَتُّت تتوافق مع نظرية مساحة الصف في مشكلة المربعات الصغرى: النظر لكل نقاط البيانات ومحاولة التقليل في المسافه بين توقعنا و كل نقطة. في هذا الدرس، تعرفنا على نطريقة مساحة العامود: كل متغير كان متّجه، تكون لنا فضاءًا من الإجابات المُحتملة (التوقعات).

## تطبيق عملي للإنحدار الخطي

### نظره عامه على البيانات

### تنظيف البيانات

### فصل بيانات التدريب والإختبار

### استكشاف البيانات وتصويرها

### النماذج الخطيه البسيطه

### تحويل المتغيرات

### نموذج الإنحدار الخطي المتعدد

### تقييم نموذجنا




[LinearLeastSquares]: https://towardsdatascience.com/linear-regression-using-least-squares-a4c3456e8570
[Scalar]: https://www.youtube.com/watch?v=rcDXQ-5H8mk
[Vector]: https://www.mathsisfun.com/algebra/scalar-vector-matrix.html