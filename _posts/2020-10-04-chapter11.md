---
title: النزول الإشتقاقي وتحسين النتائج الكمية
show_title: true
chapter_number: 11
chapter_text: الفصل الحادي عشر
chapter_lessons: [[0, 'مقدمة'], [1, 'تقليل الخساره بإستخدام برنامج'], [2, 'النزول الإشتقاقي'], [3, 'التحدب'], [4, 'النزول الإشقاقي العشوائي']]
chapter_sublessons: [
    [],
    ['مشاكل simple_minimize'],
    ['الفكره', 'تحليل النزول الإشتقاقي', 'تعريف دالة `minimize`', 'تقليل خسارة Huber', 'ملخص النزول الإشتقاقي'],
    ['اكتشاف الحدود الدنيا بالنزول الإشتقاقي', 'تعريف التحدب', 'ملخص التحدب'],
    [['استخدام دالة الخساره MSE'], 'سلوك النزول الإشقاقي العشوائي', 'تعريف دالة للنزول الإشقاقي العشوائي', 'نزول اشتقاقي بدفعات صغيره', 'تعريف دالة للنزول الإشتقاقي بدفعات صغيره', ملخص النزول الإشقاقي العشوائي']
]
layout: default
---

## مقدمة

لإستخدام قاعدة بيانات للتنبؤ والتوقع، يجب علينا تكوين نموذجنا بشكل دقيق وإختيار دالة خساره. مثلاً، بيانات الإكراميات، نموذجنا توقع ان نسبة الإكراميه ثابته لا تتغير. ثم قررنا إستخدام دالة الخطأ التربيعي المتوسط MSE ووجدنا النموذج الأقل خساره.

وجدنا ايضاً ان هناك وصفاً ابسط لدالتي الخساره الخطأ التربيعي المتوسط و متوسط الخطأ الحتمي وهي: المتوسط والوسيط. ولكن، كلما كان نموذجنا ودالة الخساره اكثر تعقيداً لن نستطيع ايجاد وصفاً رياضياً مناسب. مثلاً، دالة Huber لديها خصائص مفيده ولكن صعب تمييزها.

يمكننا استخدام الكمبيوتر لحل هذه المشكل بواسطة النزول الإشتقاقي، طريقه حسابيه لتقليل دوال الخساره.

## تقليل الخساره بإستخدام برنامج

لنعود للنموذج من الفصل السابق:

$$ \theta = C $$

سنستخدم دالة الخساره MSE:

$$ \begin{split}
\begin{aligned}
L(\theta, \textbf{y})
&= \frac{1}{n} \sum_{i = 1}^{n}(y_i - \theta)^2\\
\end{aligned}
\end{split} $$

للتبسيط، سنستخدم البيانات التاليه: $ \textbf{y} = [ 12, 13, 15, 16, 17 ] $. نعلم من خلال تحليلنا للبيانات في الفصل السابق ان قيمة $ \theta $ لدالة الخساره MSE هي المتوسط $ \text{mean}(\textbf{y}) = 14.6 $. لنرى اذا كان بإمكاننا الحصول على نتيجه عند كتابتنا لبرنامج يوجدها.

اذا قمنا بكتابة برنامج بشكل متقن، فبإمكاننا استخدام نفس البرنامج على اي دالة خساره لإيجاد اقل قيمة ل $ \theta $، يشمل ذلك دالة Huber المعقده رياضياً:

$$ \begin{split}
L_\alpha(\theta, \textbf{y}) = \frac{1}{n} \sum_{i=1}^n \begin{cases}
    \frac{1}{2}(y_i - \theta)^2 &  | y_i - \theta | \le \alpha \\
    \alpha ( |y_i - \theta| - \frac{1}{2}\alpha ) & \text{otherwise}
\end{cases}
\end{split} $$

أولاً، نقوم برسم تخطيطي للبيانات. بالجانب الأيمن من الرسم نقوم برسم دالة الخساره MSE لقيم مختلفه ل $ \theta $:

```python
pts = np.array([12, 13, 15, 16, 17])
points_and_loss(pts, (11, 18), mse)
```

> في الكود البرمجي السابق استخدم الكاتب دالة عرفها مسبقاً بأسم `points_and_loss`، تقبل الدالة ثلاث متغيرات، الأولى هي البيانات. المتغير الثاني هي مقاسات ابعاد x-axis ، والقيمه الأخيره هي نوع دالة الخساره، والتي هي عباره عن داله اخرى عرفها ايضاً بأسم `mse`. تعريف كلا الدالتين هو كالتالي:
>
> ```python
> def mse(theta, y_vals):
>     return np.mean((y_vals - theta) ** 2)
> 
> def points_and_loss(y_vals, xlim, loss_fn):
>     thetas = np.arange(xlim[0], xlim[1] + 0.01, 0.05)
>     losses = [loss_fn(theta, y_vals) for theta in thetas]
>     
>     plt.figure(figsize=(9, 2))
>     
>     ax = plt.subplot(121)
>     sns.rugplot(y_vals, height=0.3, ax=ax)
>     plt.xlim(*xlim)
>     plt.title('Points')
>     plt.xlabel('Tip Percent')
>     
>     ax = plt.subplot(122)
>     plt.plot(thetas, losses)
>     plt.xlim(*xlim)
>     plt.title(loss_fn.__name__)
>     plt.xlabel(r'$ \theta $')
>     plt.ylabel('Loss')
>     plt.legend()
> ```
> 

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_basics_3_0.png'>
</p>

كيف بإمكاننا برمجة برنامج يقوم بإيجاد اقل قيمة ل $ \theta $ اوتوماتيكياً؟ الطريقه الأسهل هي بحساب قيمة الخساره لأكثر من قيمه ل $ \theta $، ثم نوجد قيمة $ \theta $ ذا الأقل خساره.

عرفنا داله بأسم `simple_minimize` والتي تقبل متغيرات هي دالة الخساره، مصفوفة البيانات، ومصفوفه بقيم $ \theta $:

```python
def simple_minimize(loss_fn, dataset, thetas):
    '''
    القيمه النهائيه لهذه الداله هي قيمة θ من بين عدة قيم من θ ذات الأقل خساره
    '''
    losses = [loss_fn(theta, dataset) for theta in thetas]
    return thetas[np.argmin(losses)]
```

ثم نعرف داله لإيجاد قيمة MSE واستخدامها في دالة `simple_minimize`:

```python
def mse(theta, dataset):
    return np.mean((dataset - theta) ** 2)

dataset = np.array([12, 13, 15, 16, 17])
thetas = np.arange(12, 18, 0.1)

simple_minimize(mse, dataset, thetas)
```

```ruby
14.599999999999991
```

النتيجه هذه قريبه للقيمه المتوقعه:

```python
# ايجاد القيمه بإستخدام المتوسط
np.mean(dataset)
```

```ruby
14.6
```

الآن يمكننا كتابة داله لحساب خسارة Huber ورسمها:

```python
def huber_loss(theta, dataset, alpha = 1):
    d = np.abs(theta - dataset)
    return np.mean(
        np.where(d < alpha,
                 (theta - dataset)**2 / 2.0,
                 alpha * (d - alpha / 2.0))
    )

points_and_loss(pts, (11, 18), huber_loss)
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_basics_12_0.png'>
</p>

على الرغم ان القيم الدنيا ل $ \theta $ يجب ان تكون أقرب إلى 15، ليس لدينا طريقه لتحليل وإيجاد قيمة $ \theta $ لدالة الخساره Huber. بدلاً، من ذلك، سنستخدم الداله `simple_minimize`:

```python
simple_minimize(huber_loss, dataset, thetas)
```

```ruby
14.999999999999989
```

الآن، لنعود لبيانات نسبة الإكراميات ونوجد افضل قيمه ل $ \theta $ بإستخدام Huber:

```python
tips = sns.load_dataset('tips')
tips['pcttip'] = tips['tip'] / tips['total_bill'] * 100
tips.head()
```

**pcttip**|**size**|**time**|**day**|**smoker**|**sex**|**tip**|**total\_bill**| 
:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:
5.944673|2|Dinner|Sun|No|Female|1.01|16.99|0
16.054159|3|Dinner|Sun|No|Male|1.66|10.34|1
16.658734|3|Dinner|Sun|No|Male|3.5|21.01|2
13.978041|2|Dinner|Sun|No|Male|3.31|23.68|3
14.680765|4|Dinner|Sun|No|Female|3.61|24.59|4

<br>
```python
points_and_loss(tips['pcttip'], (11, 20), huber_loss)
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_basics_17_0.png'>
</p>

```python
simple_minimize(huber_loss, tips['pcttip'], thetas)
```

```ruby
15.499999999999988
```

نلاحظ ان عند استخدام دالة خسارة Huber كانت النتيجه $ \hat{\theta} = 15.5 $ . يمكننا الآن مقارنة هذه النتيجه مع MSE و MAE:

```python
print(f"               MSE: theta_hat = {tips['pcttip'].mean():.2f}")
print(f"               MAE: theta_hat = {tips['pcttip'].median():.2f}")
print(f"        Huber loss: theta_hat = 15.50")
```

```ruby
            MSE: theta_hat = 16.08
            MAE: theta_hat = 15.48
    Huber loss: theta_hat = 15.50
```

نلاحظ ان دالة Huber اقرب إلى MAE كونها لا تتأثر بشكل كبير بسبب القيم الشاذه على الجانب الأيمن في الرسم البياني التالي لتوزيع بيانات الإكراميات:

```python
sns.distplot(tips['pcttip'], bins=50);
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_basics_22_0.png'>
</p>

### مشاكل simple_minimize

على الرغم من ان دالة `simple_minimize` تساعدنا على تقليل دالة الخساره، إلا ان لديها بعض المشاكل التي تجعلها غير مفيده للإستخدام بشكل عام. مشكلتها الأهم هي انها تعمل فقط مع قيم $ \theta $. مثلاً، في الكود البرمجي التالي، الذي سبق ان استخدمناه في الأعلى، احتجنا لتعريف قيم $ \theta $ يدوياً من 12 إلى 18:

```python
dataset = np.array([12, 13, 15, 16, 17])
thetas = np.arange(12, 18, 0.1)

simple_minimize(mse, dataset, thetas)
```

كيف وجدنا انه علينا التحقق من القيم بين 12 و 18؟ احتجنا لمراجعة الرسم البياني لدالة الخساره ووجدنا ان القيم الدنيا بين تلك القيمتين. هذه الطريقه غير عمليه لأننا قمنا بإضافة خطوه معقده جديده لنماذجنا. بالإضافه لذلك، قمنا بتعريف قيم الزياده 0.1 بشكل يدوي. ولكن، اذا كانت القيمه المثلى ل $ \theta $ هي 12.043، ستقوم الداله `simple_minimize` بتقريب النتيجه إلى 12.00 كونها الأقرب لمضاعفات 0.1

يمكننا حل تلك المشاكل بطريقه واحده بإستخدام ما يسمى بـ *النزول الإشتقاقي Gradient Descent*.

## النزول الإشتقاقي

نحن مُهتمون لبناء داله تستطيع التقليل من دالة الخساره بدون تقييد المستخدم لتحديد قيم مسبقه ل $ \theta $ للتجربه عليها. بمعنى أصح، بما ان دالة `simple_minimize` شكلها كالتالي:

```python
simple_minimize(loss_fn, dataset, thetas)
```

نريد دالة لديها الشكل التالي

```python
minimize(loss_fn, dataset)
```

> لاحظ في الشكل الذي نبحث عنه، لا يحتاج المستخدم لإضافة لقيم مسبقه ل $ \theta $ في المتغيرات المطلوبه لدالة تقليل الخساره `minimize`.

تحتاج هذه الداله لإيجاد قيم $ \theta $ الأقل خساره اوتوماتيكياً اياً كان حجمها. سنستخدم طريقة تسمى بالنزول الإشتقاقي لبناء الدالة الجديده المسماه `minimize`.

### الفكره

كما في دوال الخساره، سنتحدث عن فكرة النزول الإشتقاقي أولاً، ثم نتعرف ونفهم العمليه الرياضيه فيها.

بما ان الداله `minimize` لا يقدم لها قيم ل $ \theta $ للتجربه عليها، نقوم بإختيار قيمه ل $ \theta $ بأي مكانٍ. ثم، نقوم بشكل تكراري بتحسين نتائج $ \theta $. وللتحسين من النتائج، نقوم بملاحظة الميلان Slope لتلك القيمه من $ \theta $ التي اخترناها في الرسم البياني.

مثلاً، سنستخدم MSE على البيانات التاليه $ \textbf{y} = [ 12.1, 12.8, 14.9, 16.3, 17.2 ] $ وقيمة $ \theta $ التي اختراناها هي 12:

```python
pts = np.array([12.1, 12.8, 14.9, 16.3, 17.2])
plot_loss(pts, (11, 18), mse)
plot_theta_on_loss(pts, 12, mse)
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_descent_define_5_0.png'>
</p>

> استخدم الكاتب دالتين كما فعل مسبقاً لتساعده على القيام بالعمليه الحسابيه والرسم البياني وهما `plot_loss` و `plot_theta_on_loss`. والكود البرمجي في الأسفل هو تعريف كلا الدالتين:
>
> ```python
> def plot_loss(y_vals, xlim, loss_fn):
>    thetas = np.arange(xlim[0], xlim[1] + 0.01, 0.05)
>    losses = [loss_fn(theta, y_vals) for theta in thetas]
>
>    plt.figure(figsize=(5, 3))
>    plt.plot(thetas, losses, zorder=1)
>    plt.xlim(*xlim)
>    plt.title(loss_fn.__name__)
>    plt.xlabel(r'$ \theta $')
>    plt.ylabel('Loss')
>   
>   
> def plot_theta_on_loss(y_vals, theta, loss_fn, **kwargs):
>    loss = loss_fn(theta, y_vals)
>    default_args = dict(label=r'$ \theta $', zorder=2,
>                        s=200, c=sns.xkcd_rgb['green'])
>    plt.scatter([theta], [loss], **{**default_args, **kwargs})
> ```
> 

نريد اختيار قيمه جديده ل $ \theta $ لتقليل الخساره. ولعمل ذلك، كما ذكرنا سابقاً، نلاحظ الميلان لقيمة $ \theta= 12 $:

```python
pts = np.array([12.1, 12.8, 14.9, 16.3, 17.2])
plot_loss(pts, (11, 18), mse)
plot_tangent_on_loss(pts, 12, mse)
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_descent_define_7_0.png'>
</p>

قيمة الميلان سلبيه، يعني ذلك ان زيادة قيمة $ \theta $ سيقلل من الخساره.
اذا كانت $ \theta= 16.5 $، فأن قيمة الميلان ستكون موجبه:

```python
pts = np.array([12.1, 12.8, 14.9, 16.3, 17.2])
plot_loss(pts, (11, 18), mse)
plot_tangent_on_loss(pts, 16.5, mse)
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_descent_define_9_0.png'>
</p>

عندما تكون نتيجة الميلان إيجابيه، فأن تقليل قيمة $ \theta $ سيقلل الخساره.

الميلان في الخط يخبرنا بإي اتجاه نختار $ \theta $ لتقليل الخساره. اذا كان الميلان نتيجته سلبيه، فنحتاج لتحرك $ \theta $ إلى الجانب الإيجابي. وإذا كان إيجابياً، فعلينا تحريك $ \theta $ إلى الجانب السلبي. رياضاً، نقول التالي:

$$ \theta^{(t+1)} = \theta^{(t)} - \frac{\partial}{\partial \theta} L(\theta^{(t)}, \textbf{y}) $$

وفيها $ \theta^{(t)} $ هي القيمه الحاليه، و $ \theta^{(t+1)} $ هي القيمه التاليه.

بالنسبه ل MSE، فستكون كالتالي:

$$ \begin{split}
\begin{aligned}
L(\theta, \textbf{y})
&= \frac{1}{n} \sum_{i = 1}^{n}(y_i - \theta)^2\\
\frac{\partial}{\partial \hat{\theta}} L(\theta, \textbf{y})
&= \frac{1}{n} \sum_{i = 1}^{n} -2(y_i - \theta) \\
&= -\frac{2}{n} \sum_{i = 1}^{n} (y_i - \theta) \\
\end{aligned}
\end{split} $$

عندما تكون $ \theta^{(t)} = 12 $، فالنتيجه هي $ -\frac{2}{n} \sum_{i = 1}^{n} (y_i - \theta) = -5.32 $، ثم نستخدمها بالمعادله السابقه: $ \theta^{(t+1)} = 12 - (-5.32) = 17.32 $

رسمنا في الأسفل القيمه السابقه ل $ \theta $ بدائره مفرغه بحدود خضراء والقيمه الجديده لها بدائره باللون الأخضر:

```python
pts = np.array([12.1, 12.8, 14.9, 16.3, 17.2])
plot_loss(pts, (11, 18), mse)
plot_theta_on_loss(pts, 12, mse, c='none',
                   edgecolor=sns.xkcd_rgb['green'], linewidth=2)
plot_theta_on_loss(pts, 17.32, mse)
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_descent_define_11_0.png'>
</p>

على الرغم ان $ \theta $ انتقلت إلى الجانب الأيمن، لكنها لا زالت بعيده جداً عن القيمه الدنيا. يمكننا حل ذلك عن طريق ضرب الميلان بقيمه صغيره قبل طرحه من $ \theta $. والعمليه الحسابيه النهاية ستبدو كالتالي:

$$ \theta^{(t+1)} = \theta^{(t)} - \alpha \cdot \frac{\partial}{\partial \theta} L(\theta^{(t)}, \textbf{y}) $$

وفيها $ \alpha $ هي قيمه ثابته صغيره. مثلاً، اذا حددنا قيمة $ \alpha = 0.3 $، فأن القيمه الجديده ل $ \theta^{(t+1)} $ ستكون:

```python
plot_one_gd_iter(pts, 12, mse, grad_mse)
```

```ruby
old theta: 12
new theta: 13.596
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_descent_define_14_1.png'>
</p>


> عرف الكاتب دالة جديده بأسم `plot_one_gd_iter` تقوم بكتابة ورسم بياني لقيمه $ \theta $ السابقه والجديده بعد القيام بالعمليه الحسابيه المشروحه مسبقاً، استخدم الكاتب ايضاً دالة بأسم `grad_mse` وهي تعريف لدالة `mse` بإستخدام النزول الإشتقاقي، الكود البرمجي لكلا الدالتين:
>
> ```python
>   def plot_one_gd_iter(y_vals, theta, loss_fn, grad_loss, alpha=0.3):
>       new_theta = theta - alpha * grad_loss(theta, y_vals)
>       plot_loss(pts, (11, 18), loss_fn)
>       plot_theta_on_loss(pts, theta, loss_fn, c='none',
>                          edgecolor=sns.xkcd_rgb['green'], linewidth=2)
>       plot_theta_on_loss(pts, new_theta, loss_fn)
>       print(f'old theta: {theta}')
>       print(f'new theta: {new_theta}')
>
>   def grad_mse(theta, y_vals):
>       return -2 * np.mean(y_vals - theta)
> ```

في الرسم التالي، قيم $ \theta $ بعد عدة تكرارات بنفس الطريقه السابقه. لاحظ ان $ \theta $ تتغير بشكل بسيط كلما اقتربنا من القيمه الدنيا لأن الميلان ايضاً اصبحت قيمته اقل:

```python
plot_one_gd_iter(pts, 13.60, mse, grad_mse)
```

```ruby
old theta: 13.6
new theta: 14.236
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_descent_define_16_1.png'>
</p>

```python
plot_one_gd_iter(pts, 14.24, mse, grad_mse)
```

```ruby
old theta: 14.24
new theta: 14.492
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_descent_define_17_1.png'>
</p>

```python
plot_one_gd_iter(pts, 14.49, mse, grad_mse)
```

```ruby
old theta: 14.49
new theta: 14.592
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_descent_define_18_1.png'>
</p>

### تحليل النزول الإشتقاقي

لدينا الآن فكرة عن طريقة عمل خوارزمية النزول الإشتقاقي:
- اختيار قيمه اوليه ل $ \theta $ ( في العاده تكون 0 ).
- اجراء العمليه الحسابيه $ \theta - \alpha \cdot \frac{\partial}{\partial \theta} L(\theta, \textbf{y}) $ عليها وحفظ النتيجه كقيمه جديده ل $ \theta $.
- تكرار العمليه حتى تتوقف $ \theta $ عن التغير.

غالباً ستلاحظ استخدام رمز النزول (الإنحدار) $ \nabla_\theta $ بدلاً من الإشتقاق الجزئي $ \frac{\partial}{\partial \theta} $. 
كلا الرمزين متشابهان، ولكن بما انا استخدام رمز النزول اكثر بشكل عام، فسنقوم بإستخدامه في المعادله:

$$ \theta^{(t+1)} = \theta^{(t)} - \alpha \cdot \nabla_\theta L(\theta^{(t)}, \textbf{y}) $$

لمراجعة الرموز:

- $ \theta^{(t)} $ هي التوقع الحالي ل $ \theta^{*} $ في التكرار $ t $.
- $ \theta^{(t+1)} $ القيمه التاليه ل $ \theta $.
- $ \alpha $ يطلق عليها معدل التعلّم Learning Rate، وعادة ما تكون رقم صغير ثابت. في بعض المرات من المفيد ان تبدأ برقم عالي ل $ \alpha $ والتقليل منه. اذا تغيرت قيمة $ \alpha $ بين عمليات التكرار، نستخدم الرمز $ \alpha^t $ لتوضيح تغير $ \alpha $ في $ t $.
- $ \nabla_\theta L(\theta^{(t)}, \textbf{y}) $ هي اشتقاق جزئي لدالة الخساره فيها قيمه متوقعه ل $ \theta $ في التكرار $ t $.

يمكننا ملاحظه اهمية استخدام دالة خساره قابله للتفاضل: $ \nabla_\theta L(\theta, \textbf{y}) $ هي جزء مهم من خوارزمية النزول الإشتقاقي. (على الرغم ان بالإمكان توقع قيمة النزول (الإنحدار) بحساب الفرق في الخساره بين قيمتين $ \theta $ وقسمتها على المسافه بينهما، لكن ذلك يزيد من مدة إيجاد النتيجه للنزول الإشتقاقي بشكل كبير مما يجعلها غير مفيده للإستخدام).

خوارزمية النزول الإشتقاقي بسيطه ومفيده بشكل كبير وذلك لإن بإمكاننا إستخدامها في كثير من انواع النماذج والكثير من دوال الخساره. هي الطريقه الحسابيه الأهم لضبط النماذج، بما فيها الإنحدار الخطي على بيانات بحجم كبير والشبكات العصبيه.

### تعريف دالة `minimize`

الآن نعود لمهمتنا الأساسيه: تعريف دالة `minimize`. سنحتاج للتعديل قليلاً من تعريف الداله كوننا نريد إيجاد النزول الإشتقاقي لدالة الخساره:

```python
def minimize(loss_fn, grad_loss_fn, dataset, alpha=0.2, progress=True):
    '''
    تستخدم النزول الإشتقاقي للتقليل من دالة الخساره loss_fn.
    تنتج لنا الداله القيمه الصغرى ل theta_hat (θ^) عندما يكون
    التغيير اقل من 0.001 بين التكرارات.
    '''
    theta = 0
    while True:
        if progress:
            print(f'theta: {theta:.2f} | loss: {loss_fn(theta, dataset):.2f}')
        gradient = grad_loss_fn(theta, dataset)
        new_theta = theta - alpha * gradient
        
        if abs(new_theta - theta) < 0.001:
            return new_theta
        
        theta = new_theta
```

ثم يمكننا تعريف دوال تقوم بحساب MSE و نزولها (انحدارها):

```python
def mse(theta, y_vals):
    return np.mean((y_vals - theta) ** 2)

def grad_mse(theta, y_vals):
    return -2 * np.mean(y_vals - theta)
```

اخيراً، يمكننا استخدام الداله `minimize` لحساب قيمة $ \theta $ الأدنى للبيانات التاليه $ \textbf{y} = [12.1, 12.8, 14.9, 16.3, 17.2] $ 

```python
%%time
theta = minimize(mse, grad_mse, np.array([12.1, 12.8, 14.9, 16.3, 17.2]))
print(f'Minimizing theta: {theta}')
print()
```

```ruby
theta: 0.00 | loss: 218.76
theta: 5.86 | loss: 81.21
theta: 9.38 | loss: 31.70
theta: 11.49 | loss: 13.87
theta: 12.76 | loss: 7.45
theta: 13.52 | loss: 5.14
theta: 13.98 | loss: 4.31
theta: 14.25 | loss: 4.01
theta: 14.41 | loss: 3.90
theta: 14.51 | loss: 3.86
theta: 14.57 | loss: 3.85
theta: 14.61 | loss: 3.85
theta: 14.63 | loss: 3.84
theta: 14.64 | loss: 3.84
theta: 14.65 | loss: 3.84
theta: 14.65 | loss: 3.84
theta: 14.66 | loss: 3.84
theta: 14.66 | loss: 3.84
Minimizing theta: 14.658511131035242

CPU times: user 7.88 ms, sys: 3.58 ms, total: 11.5 ms
Wall time: 8.54 ms
```

نلاحظ ان النزول الإشتقاقي قام بإيجاد نفس النتيجة بشكل سريع ل:

```python
np.mean([12.1, 12.8, 14.9, 16.3, 17.2])
```

```ruby
14.66
```

### تقليل خسارة Huber

الآن، يمكننا تطبيق النزول الإشتقاقي للتقليل من دالة الخساره Huber على بيانات الإكراميات.

```python
tips = sns.load_dataset('tips')
tips['pcttip'] = tips['tip'] / tips['total_bill'] * 100
```

دالة الخساره Huber تعرف كالتالي:

$$ \begin{split}
L_\delta(\theta, \textbf{y}) = \frac{1}{n} \sum_{i=1}^n \begin{cases}
    \frac{1}{2}(y_i - \theta)^2 &  | y_i - \theta | \le \delta \\
     \delta (|y_i - \theta| - \frac{1}{2} \delta ) & \text{otherwise}
\end{cases}
\end{split} $$

والنزول الإشتقاقي لدالة Huber:

$$ \begin{split}
\nabla_{\theta} L_\delta(\theta, \textbf{y}) = \frac{1}{n} \sum_{i=1}^n \begin{cases}
    -(y_i - \theta) &  | y_i - \theta | \le \delta \\
    - \delta \cdot \text{sign} (y_i - \theta) & \text{otherwise}
\end{cases}
\end{split} $$

(لاحظ اننا في التعاريف السابقه لدالة خسارة Huber استخدمنا المتغير $ \alpha $ للإشاره لنقطة الإنتقال. ولإبعاد الشك بينها وبين $ \alpha $ المستخدمه في النزول الإشتقاقي، قمنا بتغير رمز نقطة الإنتقال في دالة الخساره Huber إلى الرمز $ \delta $.)

```python
def huber_loss(theta, dataset, delta = 1):
    d = np.abs(theta - dataset)
    return np.mean(
        np.where(d <= delta,
                 (theta - dataset)**2 / 2.0,
                 delta * (d - delta / 2.0))
    )

def grad_huber_loss(theta, dataset, delta = 1):
    d = np.abs(theta - dataset)
    return np.mean(
        np.where(d <= delta,
                 -(dataset - theta),
                 -delta * np.sign(dataset - theta))
    )
```

لنقوم بالتقليل من دالة الخساره Huber في بيانات الإكراميه:

```python
%%time
theta = minimize(huber_loss, grad_huber_loss, tips['pcttip'], progress=False)
print(f'Minimizing theta: {theta}')
print()
```

```ruby
Minimizing theta: 15.506849531471964

CPU times: user 194 ms, sys: 4.13 ms, total: 198 ms
Wall time: 208 ms
```

### ملخص النزول الإشتقاقي

يوفر لنا النزول الإشتقاقي طريقه عامله للتقليل من دالة الخساره عندما لا نستطيع إيجاد القيمه الدنيا ل $ \theta $. عندما يكون نموذجنا ودالة الخساره اكثر تعقيداً، نستخدم النزول الإشتقاقي كوسيله لضبط النماذج.

## التحدب

### اكتشاف الحدود الدنيا بالنزول الإشتقاقي

### تعريف التحدب

### ملخص التحدب

## النزول الإشقاقي العشوائي

#### استخدام دالة الخساره MSE

### سلوك النزول الإشقاقي العشوائي

### تعريف دالة للنزول الإشقاقي العشوائي

### نزول اشتقاقي بدفعات صغيره

### تعريف دالة للنزول الإشتقاقي بدفعات صغيره

### ملخص النزول الإشقاقي العشوائي