---
title: النماذج الخطية
show_title: true
chapter_number: 13
chapter_text: الفصل الثالث عشر
chapter_lessons: [[0, 'مقدمة'], [1, 'التنبؤ بالإكراميات'], [2, 'ضبط النموذج الخطي بإستخدام النزول الإشتقاقي'], [3, 'الإنحدار الخطي المتعدد'], [4, 'المربعات الصغرى - منظور هندسي'], [5, 'تطبيق عملي للإنحدار الخطي']]
chapter_sublessons: [
    [],
    [['تعريف نموذج خطي بسيط', 'توقع النموذج الخطي']],
    [['ضبط النموذج الخطي مع النزول الإشتقاقي', 'ملاحظة عن إستخدام العلاقات'],'مشتقة خسارة الخطأ التربيعي المتوسط MSE', 'تطبيق النزول الإشتقاقي'],
    ['خسارة الخطأ التربيعي المتوسط وإنحدارها', 'ضبط النموذج مع النزول الإشتقاقي', 'الرسم البياني لتنبؤاتنا', 'استخدام كامل البيانات', 'ملخص الإنحدار الخطي المتعدد'],
    ['المربعات الصغرى: النموذج الثابت', 'المربعات الصغرى: نموذج خطي بسيط', 'الفكره الهندسيه', 'الجبر الخطي', 'انهاء الدراسه', 'عندما تعتمد المتغيرات خطياً', 'طريقتين للتفكير'],
    ['نظره عامه على البيانات', 'تنظيف البيانات', 'فصل بيانات التدريب والإختبار', 'استكشاف البيانات وتصويرها', 'نماذج خطية أبسط', 'تحويل المتغيرات', 'نموذج الإنحدار الخطي المتعدد', 'تقييم نموذجنا'],
]
layout: default
---

## مقدمة

الآن، بعد ان تعلمنا بشكل عام ادوات لضبط النماذج مع دوال التكلفه، نتحول لطرق تحسين النموذج. للتبسيط، في السابق حددنا عملنا على النموذج الثابت: نموذجنا فقط يتوقع رقم واحد.

ولكن، إعطاء نموذج كهذا للنادل لن يرضيه. يود النادل ان يوضح انه حصل على معلومات اكثر نسبة الإكراميه من الطاولات التي خدمها. لماذا لا نستخدم بياناته الأخرى، مثلاً حجم العملاء في الطاوله ومجموع الفاتوره، لغرض جعل النموذج اكثر فائدة.

في هذا الفصل سنتعرف على النماذج الخطية والذي يسمح لنا بإستخدام جميع البيانات التي لدينا لإجراء توقعات. النماذج الخطية لا تستخدم بنطاقٍ واسع فقط، بل ايضاً لديها اسس نظريه غنية بالمعلومات التي تجعلنا نفهم ادوات مستقبليه للنماذج. سنتعرف على نموذج الإنحدار الخطي البسيط الذي يستخدم متغير واحد، سنتعلم كيف يستخدم النزول الإشتقاقي لضبط النموذج، واخيراً التوسع في النموذج وإضافة المزيد من المتغيرات له.

## التنبؤ بالإكراميات

سابقاً، تعاملنا مع بيانات تحتوي على صف واحد لكل طاولة قام النادل بخدمتها لمدة اسبوع. النادل قام بجمع البيانات للقيام بالتوقع بقيمة الإكرامية التي سيحصل عليها في المستقبل:

```python
tips = sns.load_dataset('tips')
tips.head()
```

|**size**|**time**|**day**|**smoker**|**sex**|**tip**|**total\_bill**| 
|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:
|2|Dinner|Sun|No|Female|1.01|16.99|0
|3|Dinner|Sun|No|Male|1.66|10.34|1
|3|Dinner|Sun|No|Male|3.5|21.01|2
|2|Dinner|Sun|No|Male|3.31|23.68|3
|4|Dinner|Sun|No|Female|3.61|24.59|4

<br>
```python
sns.distplot(tips['tip'], bins=25);
```

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter13/linear_tips_3_0.png'>
</p>

كما تحدثنا سابقاً، اذا اخترنا النموذج الثابت ودالة الخطأ التربيعي المتوسط، فأن نموذجنا سيتوقع متوسط الإكراميات:

```python
np.mean(tips['tip'])
```

```ruby
2.9982786885245902
```

يعني ذلك ان عندما يأتي مجموعة من الزبائن للنادل ثم يسألنا النادل عن مجموعه الإكرامية التي سيحصل عليها، فسنجيب عليه "حوالي $ \\$3 $"، اياً كان عدد الزبائن ومجموع الفاتوره.

ولكن، عند التحقق من باقي المتغيرات في البيانات، يمكننا ان نقوم بتوقعات دقيقه إذا اضفنا تلك المتغيرات للنموذج. مثلاً، الرسم البياني التالي يوضح مجموع الإكراميه مقارنة بمبلغ الفاتورة ويظهر العلاقة الإيجابية بينهما:

```python
sns.lmplot(x='total_bill', y='tip', data=tips, fit_reg=False)
plt.title('Tip amount vs. Total Bill')
plt.xlabel('Total Bill')
plt.ylabel('Tip Amount');
```

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter13/linear_tips_7_0.png'>
</p>

على الرغم ان متوسط الإكراميه هو $ \\$3 $، اذا كانت الطاولة طلبت ما مجموعه $ \\$40 $ من الوجبات فأننا متأكدين ان النادل سيحصل على اكثر من $ \\$3 $ كأكرامية. لذا، نريد التعديل في نموذجنا ليتمكن من التوقع بناءًا على متغيرات في بياناتنا بدلاً من توقع متوسط الإكرامية. للقيام بذلك، سنستخدم النموذج الخطي بدلاً من الثابت.

لنراجع أولاً الأدوات التي لدينا لبناء النموذج والتوقع وتعريف بعض الرموز الجديده لنتمكن بشكل افضل من تمثيل العمليات الرياضيه الإضافيه في النموذج الخطي.

### تعريف نموذج خطي بسيط

نحن مهتمون بتوقع قيمة الإكراميه بناءًا على مجموع الفاتوره. لنجعل $ y $ تمثل مجموع الإكراميه، المتغير الذي نريد ان يتوقعه النموذج. و $ x $ تمثل مجموع الفاتوره، المتغير الذي نريد استخدامه للتوقع.

نقوم بتعريف النموذج الخطي $ f_\boldsymbol\theta^* $ الذي يعتمد على $ x $:

$$ f_\boldsymbol\theta^* (x) = \theta_1^* x + \theta_0^* $$

نتعامل مع $ f_\boldsymbol\theta^* (x) $ على انها الدالة التي أنشأت البيانات.

تفترض $ f_\boldsymbol\theta^* (x) $ ان $ y $ لديها علاقه خطيه كامله مع $ x $. ولكن، في بياناتنا لا يظهر لنا خط مستقيم بسبب وجود بعض البيانات العشوائية المزعجه $ \epsilon $. رياضياً، نأخذ بعين الإعتبار هذه المشكله بإضافة المصطلح:

$$ y = f_\boldsymbol\theta^* (x) + \epsilon $$

أذا كانت الإفتراضيه ان $ y $ لديها علاقخ خطية كامله مع $ x $، وتمكنا بطريقة ما من توقع القيمة الصحيحه ل $ \theta_1^* $ و $ \theta_0^* $، وبشكل غير اعتيادي لم يكنا لدينا أي عشوائية في البيانات، سنتمكن من التوقع بشكل دقيق قيمة الإكرامية التي سيحصل عليها النادل من جميع الزبائن، وبشكل دائم. بالطبع، لا يمكننا تلبية جميع المعايير في الحياه الواقعية. بدلاً من ذلك، نتوقع $ \theta_1^* $ و $ \theta_0^* $ بإستخدام البيانات لجعل توقعاتنا اقرب دقه للواقع.

#### توقع النموذج الخطي

بما أننا لا نستطيع إيجاد قيمة $ \theta_1^* $ و $ \theta_0^* $ بشكل دقيق، سنفترض ان بياناتنا تتوقع قيمة هذه المتغيرات. نشير للتوقعات ب $ \theta_1 $ و $ \theta_0 $، وتوقعنا الذي يتم ضبطه بالنموذج ب $ \hat{\theta_1} $ و $ \hat{\theta_0} $، و النموذج:

$$ f_\boldsymbol\theta (x) = \theta_1 x + \theta_0 $$

أحياناً سترى $ h(x) $ بدلاً من $ f_\hat{\boldsymbol\theta} (x) $؛ ال $ h $ تعني الفرضية *Hypothesis*، كون $ f_\hat{\boldsymbol\theta} (x) $ هي فرضيتنا ل $ f_{\boldsymbol\theta^*} (x) $.

من أجل تحديد قيمة $ \hat{\theta_1} $ و $ \hat{\theta_0} $، نقوم بإختيار دالة تكلفة وتقليلها بإستخدام النزول الإشتقاقي.

## ضبط النموذج الخطي بإستخدام النزول الإشتقاقي

نريد ضبط نموذج خطي يتنبأ بقيمة الإكراميه من مجموع الفاتوره:

$$ f_\boldsymbol\theta (x) = \theta_1 x + \theta_0 $$

ولتسحين قيم $ \theta_1 $ و $ \theta_0 $، نحتاج أولاً لإختيار دالة خسارة. سنختار دالة خسارة الخطأ التربيعي المتوسط MSE:

$$ \begin{split}
\begin{aligned}
L(\boldsymbol\theta, \textbf{x}, \textbf{y})
&= \frac{1}{n} \sum_{i = 1}^{n}(y_i - f_\boldsymbol\theta (x_i))^2\\
\end{aligned}
\end{split} $$

لاحظ اننا عدلنا على دالة الخساره لتوضيح إضافتنا لمتغير في النموذج. الآن، $ \textbf{x} $ هي مصفوفه أحادية البعد تحتوي على جميع الفواتير، و $ \textbf{y} $ هي مصفوفة أحادية البعد تحتوي على قيمة كل إكراميه، و $ \boldsymbol\theta $ هي مصفوفة تحتوي على التالي: $ \boldsymbol\theta = [ \theta_1, \theta_0 ] $.

يطلق على إستخدام النموذج الخطي مع دالة خسارة الخطأ التربيعي المتوسط بإسم الإنحدار الخطي للمربعات الصغرى Least-squares Linear Regression. يمكننا استخدام النزول الإشتقاقي لإيجاد قيمة $ \boldsymbol\theta $ التي تقلل الخساره. [📝][LinearLeastSquares]

#### ملاحظة عن إستخدام العلاقات

اذا سبق أن رأيت الإنحدار الخطي للمربعات الصغرى، قد تلاحظ اننا نستطيع حساب معامل الإرتباط وإستخدامه لتحديد قيمة $ \theta_1 $ و $ \theta_0 $. هذه طريقة اسهل واسرع للحساب بدلاً من إستخدام النزول الإشتقاقي في أي معادله، تماماً كما يكون اسهل لنا حساب المتوسط بدلاً من حساب النزول الإشتقاقي لضبط النموذج الثابت. على أية حال، سنستخدم النزول الإشتقاقي لأنها طريقة عامله لتقليل الخساره وستعمل معنا لاحقاً عندما نتعرف على نماذج لا يمكن حساب خسارتها إحصائياً. بالأصح، في كثير من المشاكل في العالم الحقيقي، سنستخدم النزول الإشتقاقي حتى ولو كانت هناك طرق إحصائيه تحليله لأن حسابها يأخذ وقتاً اطول من النزول الإشتقاقي، خاصة عندما تكون البيانات ذات حجم كبير.

### مشتقة خسارة الخطأ التربيعي المتوسط MSE

لإستخدام النزول الإشتقاقي، نحتاج لحساب مشتقة خسارة الخطأ التربيعي المتوسط بالنسبة ل $ \boldsymbol\theta $. الآن بما ان $ \boldsymbol\theta $ عبارة عن مصفوفه ذات طول 2 وليست قيمة عددية مدرجة Scalar، و $ \nabla_{\boldsymbol\theta} L(\boldsymbol\theta, \textbf{x}, \textbf{y}) $ ايضاً مصفوفه من الحجم 2. [📝][Scalar]

$$ \begin{split}
\begin{aligned}
\nabla_{\boldsymbol\theta} L(\boldsymbol\theta, \textbf{x}, \textbf{y})
&= \nabla_{\boldsymbol\theta} \left[ \frac{1}{n} \sum_{i = 1}^{n}(y_i - f_\boldsymbol\theta (x_i))^2 \right] \\
&= \frac{1}{n} \sum_{i = 1}^{n}2 (y_i - f_\boldsymbol\theta (x_i))(- \nabla_{\boldsymbol\theta} f_\boldsymbol\theta (x_i))\\
&= -\frac{2}{n} \sum_{i = 1}^{n}(y_i - f_\boldsymbol\theta (x_i))(\nabla_{\boldsymbol\theta} f_\boldsymbol\theta (x_i))\\
\end{aligned}
\end{split} $$

نعرف ان:

$$ f_\boldsymbol\theta (x) = \theta_1 x + \theta_0 $$

نريد حساب قيمة $ \nabla_{\boldsymbol\theta} f_\boldsymbol\theta (x_i) $ والتي هي مصفوفه طولها 2:

$$ \begin{split}
\begin{aligned}
\nabla_{\boldsymbol\theta} f_\boldsymbol\theta (x_i)
&= \begin{bmatrix}
     \frac{\partial}{\partial \theta_0} f_\boldsymbol\theta (x_i)\\
     \frac{\partial}{\partial \theta_1} f_\boldsymbol\theta (x_i)
   \end{bmatrix} \\
&= \begin{bmatrix}
     \frac{\partial}{\partial \theta_0} [\theta_1 x_i + \theta_0]\\
     \frac{\partial}{\partial \theta_1} [\theta_1 x_i + \theta_0]
   \end{bmatrix} \\
&= \begin{bmatrix}
     1 \\
     x_i
   \end{bmatrix} \\
\end{aligned}
\end{split} $$

اخيراً، نعوضها في معادلتنا الأساسيه لنحصل على التالي:

$$ \begin{split}
\begin{aligned}
\nabla_{\boldsymbol\theta} L(\theta, \textbf{x}, \textbf{y})
&= -\frac{2}{n} \sum_{i = 1}^{n}(y_i - f_\boldsymbol\theta (x_i))(\nabla_{\boldsymbol\theta} f_\boldsymbol\theta (x_i))\\
&= -\frac{2}{n} \sum_{i = 1}^{n} (y_i - f_\boldsymbol\theta (x_i)) \begin{bmatrix} 1 \\ x_i \end{bmatrix} \\
&= -\frac{2}{n} \sum_{i = 1}^{n} \begin{bmatrix}
    (y_i - f_\boldsymbol\theta (x_i)) \\
    (y_i - f_\boldsymbol\theta (x_i)) x_i
    \end{bmatrix} \\
\end{aligned}
\end{split} $$

هذه مصفوفة من طولها 2 كون $ (y_i - f_\boldsymbol\theta (x_i)) $ قيمة عددية مدرجه.

### تطبيق النزول الإشتقاقي

الآن، لنقوم بضبط النموذج الخطي على بيانات الإكراميات لتوقع قيمة الإكرامية من مجموع الفاتورة.

أولاً، نقوم بتعريف دالة في بايثون لحساب الخساره:

```python
def simple_linear_model(thetas, x_vals):
    '''نتيجة هذه الداله هي القيمه المتوقعه من النموذج الخطي'''
    return thetas[0] + thetas[1] * x_vals

def mse_loss(thetas, x_vals, y_vals):
    return np.mean((y_vals - simple_linear_model(thetas, x_vals)) ** 2)
```

ثم نعرف طالة تقوم بحساب خطية الخساره:

```python
def grad_mse_loss(thetas, x_vals, y_vals):
    n = len(x_vals)
    grad_0 = y_vals - simple_linear_model(thetas, x_vals)
    grad_1 = (y_vals - simple_linear_model(thetas, x_vals)) * x_vals
    return -2 / n * np.array([np.sum(grad_0), np.sum(grad_1)])
```

سنقوم بإستخدام الدالة `minimize` التي سبق ان عرفناها لتطبيق النزول الإشتقاقي:

```python
def minimize(loss_fn, grad_loss_fn, x_vals, y_vals,
             alpha=0.0005, progress=True):
        '''
    تستخدم النزول الإشتقاقي للتقليل من دالة الخساره loss_fn.
    تنتج لنا الداله القيمه الصغرى ل theta_hat (θ^) عندما يكون
    التغيير اقل من 0.001 بين التكرارات.
    '''
    theta = np.array([0., 0.])
    loss = loss_fn(theta, x_vals, y_vals)
    while True:
        if progress:
            print(f'theta: {theta} | loss: {loss}')
        gradient = grad_loss_fn(theta, x_vals, y_vals)
        new_theta = theta - alpha * gradient
        new_loss = loss_fn(new_theta, x_vals, y_vals)
        
        if abs(new_loss - loss) < 0.0001:
            return new_theta
        
        theta = new_theta
        loss = new_loss
```

والآن نقوم بتطيبق النزول الإشتقاقي:

```python
thetas = minimize(mse_loss, grad_mse_loss, tips['total_bill'], tips['tip'])
```

```ruby
theta: [0. 0.] | cost: 10.896283606557377
theta: [0.   0.07] | cost: 3.8937622006094705
theta: [0.  0.1] | cost: 1.9359443267168215
theta: [0.01 0.12] | cost: 1.388538448286097
theta: [0.01 0.13] | cost: 1.235459416905535
theta: [0.01 0.14] | cost: 1.1926273731479433
theta: [0.01 0.14] | cost: 1.1806184944517062
theta: [0.01 0.14] | cost: 1.177227251696266
theta: [0.01 0.14] | cost: 1.1762453624313751
theta: [0.01 0.14] | cost: 1.1759370980989148
theta: [0.01 0.14] | cost: 1.175817178966766
```

نلاحظ ان النزول الإشتقاقي يقترب لقيمة $ \hat\theta_0 = 0.01 $ و $ \hat\theta_0 = 0.14 $. نموذجنا الخطي الآن:

$$ y = 0.14x + 0.01 $$

يمكننا استخدام النتيجة السابقه لرسم توقعاتنا بجانب البيانات الحقيقيه:

```python
x_vals = np.array([0, 55])
sns.lmplot(x='total_bill', y='tip', data=tips, fit_reg=False)
plt.plot(x_vals, simple_linear_model(thetas, x_vals), c='goldenrod')
plt.title('Tip amount vs. Total Bill')
plt.xlabel('Total Bill')
plt.ylabel('Tip Amount');
```

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter13/linear_grad_16_0.png'>
</p>

نلاحظ انه عندما تكون قيمة الفاتوره $ \\$10 $، فأن نموذجنا يتوقع ان النادل سيحصل على إكرامية بحوالي $ \\$1.50 $. بنفس الطريقة، اذا كانت قيمة الفاتوره $ \\$40 $ فأن النموذج يتوقع حصول النادل على إكرامية بقيمة $ \\$6.00 $.

## الإنحدار الخطي المتعدد

نموذجنا الخطي البسيط لميزة إضافية عن النموذج الثابت، الميزه هي استخدامه البيانات للتوقع. ولكن، لا يزال النموذج محدود كونه يستخدم متغير واحد من بياناتنا. الكثير من البيانات تحتوي على اكثر من متغير مهم ومفيد للإستخدام، ويمكن للإنحدار الخطي المتعدد الإستفاده من ذلك. مثلاً، لنأخذ البيانات التاليه لأنواع السيارات ومعلومات صرف الوقود بالميل لكل جالون (Milage Per Gallon MPG):

```python
mpg = pd.read_csv('mpg.csv').dropna().reset_index(drop=True)
mpg
```

| car name                  | origin | model year | \.\.\. | displacement | cylinders | mpg    |        |
|:-------------------------:|:------:|:----------:|:------:|:------------:|:---------:|:------:|:------:|
| chevrolet chevelle malibu | 1      | 70         | \.\.\. | 307          | 8         | 18     | 0      |
| buick skylark 320         | 1      | 70         | \.\.\. | 350          | 8         | 15     | 1      |
| plymouth satellite        | 1      | 70         | \.\.\. | 318          | 8         | 18     | 2      |
| \.\.\.                    | \.\.\. | \.\.\.     | \.\.\. | \.\.\.       | \.\.\.    | \.\.\. | \.\.\. |
| dodge rampage             | 1      | 82         | \.\.\. | 135          | 4         | 32     | 389    |
| ford ranger               | 1      | 82         | \.\.\. | 120          | 4         | 28     | 390    |
| chevy s\-10               | 1      | 82         | \.\.\. | 119          | 4         | 31     | 391    |


```ruby
392 rows × 9 columns
```

> لتحميل قاعدة البيانات mpg.csv [اضغط هنا]({{ site.baseurl }}/files/chapter13/mpg.csv).

يبدو لنا أن اكثر من متغير يأثر على صرف السياره للوقود. مثلاً، يبدو ان صرف الوقد يقل عندما تزيد قوة الحصان للسياره:

```python
sns.lmplot(x='horsepower', y='mpg', data=mpg);
```

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter13/linear_multiple_6_0.png'>
</p>

ولكن، السيارات التي في السنوات الأخيره لديها صرف وقود افضل بشكل عام عن السيارات القديمه:

```python
sns.lmplot(x='model year', y='mpg', data=mpg);
```

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter13/linear_multiple_8_0.png'>
</p>

يظهر أن بأمكاننا الحصول على نتائج اكثر دقة للنموذج إذا استطعنا إستخدام قوة الحصان وسنة صناعة السياره للتنبؤ عن كمية صرف الوقود بالميل لكل جالون MPG. بالأصح، يبدو ان النموذج المثالي يأخذ بعين الإعتبار جميع المتغيرات الرقميه في بياناتنا. يمكننا توسيع نموذجنا الخطي ذو المتغير الواحد ليتمكن من التنبؤ بناءًا على أي عدد من المتغيرات.

ذكرنا ان تعريف النموذج كالتالي:

$$ f_\boldsymbol\theta (\textbf{x}) = \theta_0 + \theta_1 x_1 + \ldots + \theta_p x_p $$

فيها $ \textbf{x} $ تمثل متّجهه Vector تحتوي على عدد $ p $ من المتغيرات لسياره واحده. النموذج السابق يقول التالي، "خذ أكثر من متغير عن السياره، اضربهم بوزن Weight معين، ثم اجمعهم معاً للقيام بتوقع صرفية السياره للوقود بالميل لكل جالون".

> توجد انواع متعددة من طرق تمثيل الأرقام: [📝][Vector]
> <p align="center"> 
> <img src='{{ site.baseurl }}/img/chapter13/scalar-vector-matrix.png'>
> </p>
> - عدد Scalar: رقم صحيح مثلاً $ 7, -4, 0.345 $.
> - المتّجه Vector: هي مصفوفة أرقام احادية الأبعاد، تكون اما من صف واحد أو عامود واحد.
> - مصفوفه Matrix: مصفوفه أرقام تحتوي على اكثر من صف أو عامود.
>


مثلاً، إذا اردنا اجراء توقع لأول سياره في بيانتنا بإستخدام قوة الحصان، الوزن، وسنة الصناعه، فسيكون شكل المتّجهه $ \textbf{x} $ كالتالي:

| model year |  weight | horsepower    |        |
|:-------------------------:|:------:|:----------:|:------:|
| 70 | 3504.0      | 130.0         | 0 |

في هذا المثال ابقينا اسماء الأعمدة للتوضيح، ولكن تذكر ان $ \textbf{x} $ تحتوي فقط على القيم الرقمية من الجدول السابق: $ \textbf{x} = [130.0, 3504.0, 70] $.

الآن، سنقوم بتعريف طريقه حسابيه ستسهل العمليات الحسابيه القادمه. سنقوم بإضافة الرقم $ 1 $ إلى المتجهه $ \textbf{x} $، وسيكون شكل الصف كالتالي:

| model year |  weight | horsepower    |  bias  ||
|:-------------------------:|:------:|:----------:|:------:|:--:|
| 70 | 3504.0      | 130.0         | 1 |0|

الآن، لاحظ ما سيحدث للعملية السحابيه لنموذجنا:

$$ \begin{split}
\begin{aligned}
f_\boldsymbol\theta (\textbf{x})
&= \theta_0 + \theta_1 x_1 + \ldots + \theta_p x_p \\
&= \theta_0 (1) + \theta_1 x_1 + \ldots + \theta_p x_p \\
&= \theta_0 x_0 + \theta_1 x_1 + \ldots + \theta_p x_p \\
f_\boldsymbol\theta (\textbf{x}) &= \boldsymbol\theta \cdot \textbf{x}
\end{aligned}
\end{split} $$

فيها $ \boldsymbol\theta \cdot \textbf{x} $ هي متّجه لحاصل ضرب $ \boldsymbol\theta $ و $ \textbf{x} $. صُممت المتّجهات والمصفوفات لكتابة التركيبات الخطيه ولذلك فهي  مناسبة جداً لنموذجنا الخطي. ولكن، يجب عليك من الآن وصاعدًا تذكر ان $ \boldsymbol\theta \cdot \textbf{x} $ هي حاصل ضرب متّجه بأخرى. يمكن أيضاً لنزع الشك، توسيع عملية ضرب المتجهتين  إلى عملية جمع وضرب مبسطه.

الآن، نقوم بتعريف المصفوفه $ \textbf{X} $ والتي ستكون المصفوفة التي تحتوي على جميع انواع السيارات كصفوف، وأول عامود هو قيمة التحيز Bias. مثلاً، هذه أول خمس اسطر من المصفوفه $ \textbf{X} $:

| model year |  weight | horsepower    |  bias  ||
|:-------------------------:|:------:|:----------:|:------:|:--:|
| 70 | 3504.0      | 130.0         | 1 |0|
| 70 | 3693.0      | 165.0         | 1 |1|
| 70 | 3436.0	      | 150.0         | 1 |2|
| 70 | 3433.0	      | 150.0         | 1 |3|
| 70 | 3449.0	     | 140.0         | 1 |4|

للتذكير مره أخرى، المصفوفة الحقيقه $ \textbf{X} $ فقط تحتوي على القيم الرقميه من الجدول السابق.

لاحظ ان $ \textbf{X} $ تحتوي على أكثر من مُتجهه $ \textbf{x} $ فوق بعضها البعض. ليكون الوصف واضحاً، نقوم بتعريف $ \textbf{X}\_{i} $ والتي ترمز للمتّجه في الصف رقم $ i $ في المصفوفه $ \textbf{X} $. نقوم بتعريف $ X_{i,j} $ والتي تمثل القيمه ذات الرقم $ j $ في الصف ذو الرقم $ i $ في المصفوفه $ \textbf{X} $. لذا، $ \textbf{X}\_{i} $ هي متّجه ذات ابعاد $ p $ و $ $ \textbf{X}\_{i,j} $ هي عدد. $ $ \textbf{X} $ هي مصفوفة $ n \times p $، فيها $ n $ هي عدد السيارات لدينا و $ p $ هي عدد المتغيرات لكل سياره.

مثلاً، في الجدول السابق لدينا $ \textbf{X}\_4 = [1, 140, 3449, 70] $ و $ X_{4,1} = 140 $ لذا الرموز هي مهمه عند تعريف دوال الخساره لأننا سنحتاج إلى كلا القيمتين $ \textbf{X} $، مصفوفة البيانات المدخله للنموذج، و $ y $، متّجه صرف الوقود بالميل لكل جالون.

> - $ \textbf{X} $ هي مصفوفه Matrix.
> - $ \textbf{x} $ هي متّجه Vector وهي هنا كل صف على حده. مثلاً السطر الثاني: $ [1, 165.0, 3693.0, 70] $.
> - $ j $ هي رقم صحيح Scalar مثلاً وزن السياره في الصف الثالث $ 3436.0 $.

### خسارة الخطأ التربيعي المتوسط وإنحدارها

دالة خسارة الخطأ التربيعي المتوسط تأخذ متّجه بوزن $ \boldsymbol\theta $ و مدخلات على شكل مصفوفه $ \textbf{X} $، و متّحه لصرف الوقود بالميل لكل جالون لكل سياره $ \textbf{y} $:

$$ \begin{split}
\begin{aligned}
L(\boldsymbol\theta, \textbf{X}, \textbf{y})
&= \frac{1}{n} \sum_{i}(y_i - f_\boldsymbol\theta (\textbf{X}_i))^2\\
\end{aligned}
\end{split} $$

اوجدنا مسبقاً مشتقة دالة خسارة الخطأ التربيعي المتوسط بالنسبة ل $ \boldsymbol\theta $:

$$ \begin{split}
\begin{aligned}
\nabla_{\boldsymbol\theta} L(\boldsymbol\theta, \textbf{X}, \textbf{y})
&= -\frac{2}{n} \sum_{i}(y_i - f_\boldsymbol\theta (\textbf{X}_i))(\nabla_{\boldsymbol\theta} f_\boldsymbol\theta (\textbf{X}_i))\\
\end{aligned}
\end{split} $$

نعرف أيضاً ان:

$$ \begin{split}
\begin{aligned}
f_\boldsymbol\theta (\textbf{x}) &= \boldsymbol\theta \cdot \textbf{x} \\
\end{aligned}
\end{split} $$

لنقوم بحساب $ \nabla_{\boldsymbol\theta} f_\boldsymbol\theta (\textbf{x}) $. عملية الحساب اسهل من المتوقع لأن $ \boldsymbol\theta \cdot \textbf{x} = \theta_0 x_0 + \ldots + \theta_p x_p $ إذاً $ \frac{\partial}{\partial \theta_0}(\boldsymbol\theta \cdot \textbf{x}) = x_0 $ و $ \frac{\partial}{\partial \theta_1}(\boldsymbol\theta \cdot \textbf{x}) = x_1 $ إلى آخره:

$$ \begin{split}
\begin{aligned}
\nabla_{\boldsymbol\theta} f_\boldsymbol\theta (\textbf{x})
&= \nabla_{\boldsymbol\theta} [ \boldsymbol\theta \cdot \textbf{x} ] \\
&= \begin{bmatrix}
     \frac{\partial}{\partial \theta_0} (\boldsymbol\theta \cdot \textbf{x}) \\
     \frac{\partial}{\partial \theta_1} (\boldsymbol\theta \cdot \textbf{x}) \\
     \vdots \\
     \frac{\partial}{\partial \theta_p} (\boldsymbol\theta \cdot \textbf{x}) \\
   \end{bmatrix} \\
&= \begin{bmatrix}
     x_0 \\
     x_1 \\
     \vdots \\
     x_p
   \end{bmatrix} \\
\nabla_{\boldsymbol\theta} f_\boldsymbol\theta (\textbf{x}) &= \textbf{x}
\end{aligned}
\end{split} $$

اخيراً، نقوم بإدخال النتيجه لحساب الخطيه:

$$ \begin{split}
\begin{aligned}
\nabla_{\boldsymbol\theta} L(\boldsymbol\theta, \textbf{X}, \textbf{y})
&= -\frac{2}{n} \sum_{i}(y_i - f_\boldsymbol\theta (\textbf{X}_i))(\nabla_{\boldsymbol\theta} f_\boldsymbol\theta (\textbf{X}_i))\\
&= -\frac{2}{n} \sum_{i}(y_i - \boldsymbol\theta \cdot \textbf{X}_i)(\textbf{X}_i)\\
\end{aligned}
\end{split} $$

تذكر أنه بما ان $ y_i - \boldsymbol\theta \cdot \textbf{X}\_i $ هي عدد و $ \textbf{X}_i $ هي متّجه ذات $ p $ ابعاد، الخطيه $ \nabla\_{\boldsymbol\theta} L(\boldsymbol\theta, \textbf{X}, \textbf{y}) $ هي ايضاً متّجه ذات $ p $ ابعاد.

رأينا نفس هذا النوع من النتائج عندما قمنا بحساب خطية الإنحدار الخطي ووجدنا انها ثنائية الأبعاد بما ان $ \boldsymbol\theta $ كانت ثنائية الأبعاد.

### ضبط النموذج الخطي مع النزول الإشتقاقي

يمكننا الآن إدخال الخساره ومشتقتها إلى دالة النزول الإشتقاقي. كالعاده، سنقوم بتعريف النموذج، دالة الخساره و دالة النزول الإشتقاقيمشتقتها في بايثون:

```python
def linear_model(thetas, X):
    '''Returns predictions by a linear model on x_vals.'''
    return  X @ thetas

def mse_loss(thetas, X, y):
    return np.mean((y - linear_model(thetas, X)) ** 2)

def grad_mse_loss(thetas, X, y):
    n = len(X)
    return -2 / n * (X.T @ y  - X.T @  X @ thetas)
```

> استخدم الكاتب الرمز `@` وهي علامة ضرب بين المصفوفات في Numpy، لذا يحتاج أن تكون `X` و `thetas` هي مصفوفات في Numpy كي يعمل الرمز `@`.

الآن، ببساطه يمككنا ادخال دوالنا إلى النزول الإشتقاقي:

```python
X = (mpg_mat
     .loc[:, ['bias', 'horsepower', 'weight', 'model year']]
     .to_numpy())
y = mpg_mat['mpg'].to_numpy()

thetas = minimize(mse_loss, grad_mse_loss, X, y)
print(f'theta: {thetas} | loss: {mse_loss(thetas, X, y):.2f}')
```

```ruby
theta: [ 0.  0.  0.  0.] | cost: 610.47
theta: [ 0.    0.    0.01  0.  ] | cost: 178.95
theta: [ 0.01 -0.11 -0.    0.55] | cost: 15.78
theta: [ 0.01 -0.01 -0.01  0.58] | cost: 11.97
theta: [-4.   -0.01 -0.01  0.63] | cost: 11.81
theta: [-13.72  -0.    -0.01   0.75] | cost: 11.65
theta: [-13.72  -0.    -0.01   0.75] | cost: 11.65
```

> استخدم الكاتب في الكود البرمجي السابق الدالة `minimize` وهي مختلفه قليلاً عن السابقه، اجري عليها التعديلات التاليه:
>
> ```python
> from scipy.optimize import minimize as sci_min
> def minimize(loss_fn, grad_loss_fn, X, y, progress=True):
>     '''
>     تستخدم دالة من مكتبة scipy للتقليل من خسارة الداله loss_fun 
>     بإستخدام نموذج من النزول الإشتقاقي
>     '''
>    theta = np.zeros(X.shape[1])
>    iters = 0
>    
>    def objective(theta):
>        return loss_fn(theta, X, y)
>    def gradient(theta):
>        return grad_loss_fn(theta, X, y)
>    def print_theta(theta):
>        nonlocal iters
>        if progress and iters % progress == 0:
>            print(f'theta: {theta} | loss: {loss_fn(theta, X, y):.2f}')
>        iters += 1
>        
>    print_theta(theta)
>    return sci_min(
>        objective, theta, method='BFGS', jac=gradient, callback=print_theta,
>        tol=1e-7
>    ).x
> ```

بناءًا على النزول الإشتقاقي، فأن نموذجنا الخطي هو كالتالي:

$$ y = -13.72 - 0.01x_2 + 0.75x_3 $$

### الرسم البياني لتنبؤاتنا

كيف أدى نموذجنا؟ نلاحظ ان الخساره قلت بشكل كبير (من 610 حتى 11.6). يمكننا طباعة نتائج توقع النموذج بجانب النتائج الحقيقه:

```python
reordered = ['predicted_mpg', 'mpg', 'horsepower', 'weight', 'model year']
with_predictions = (
    mpg
    .assign(predicted_mpg=linear_model(thetas, X))
    .loc[:, reordered]
)
with_predictions
```

| model year | weight | horsepower | mpg    | predicted\_mpg |        |
|:----------:|:------:|:----------:|:------:|:--------------:|:------:|
| 70         | 3504   | 130        | 18     | 15\.447125     | 0      |
| 70         | 3693   | 165        | 15     | 14\.053509     | 1      |
| 70         | 3436   | 150        | 18     | 15\.785576     | 2      |
| \.\.\.     | \.\.\. | \.\.\.     | \.\.\. | \.\.\.         | \.\.\. |
| 82         | 2295   | 84         | 32     | 32\.4569       | 389    |
| 82         | 2625   | 79         | 28     | 30\.354143     | 390    |
| 82         | 2720   | 82         | 31     | 29\.726608     | 391    |

```ruby
392 rows × 5 columns
```

بما اننا أوجدنا $ \boldsymbol\theta $ من النزول الإشتقاقي، يمكننا ان نتأكد من اول سطر في بياناتنا ان $ \boldsymbol\theta \cdot \textbf{X}_0 $ تطابق توقعنا السابق:

```python
print(f'Prediction for first row: '
      f'{thetas[0] + thetas[1] * 130 + thetas[2] * 3504 + thetas[3] * 70:.2f}')
```

```ruby
Prediction for first row: 15.45
```

يمكننا رسم الفرق بين توقعنا والنتيجة الحقيقه (النتيجة الحقيقه - التوقع):

```python
resid = y - linear_model(thetas, X)
plt.scatter(np.arange(len(resid)), resid, s=15)
plt.title('Residuals (actual MPG - predicted MPG)')
plt.xlabel('Index of row in data')
plt.ylabel('MPG');
```

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter13/linear_multiple_34_0.png'>
</p>

يبدو واضحاً ان نموذجنا يعطي توقعات منطقيه لكثير من السيارات، على الرغم ان بعض النتائج كان الفرق فيها أكثر من 10 ميل لكل جالون (بعض السيارات لديها اقل من 10!). قد يهمنا اكثر معرفة نسبة الخط بين التوقع والنتيجة الصحيحه لصرف الوقود:

```python
resid_prop = resid / with_predictions['mpg']
plt.scatter(np.arange(len(resid_prop)), resid_prop, s=15)
plt.title('Residual proportions (resid / actual MPG)')
plt.xlabel('Index of row in data')
plt.ylabel('Error proportion');
```

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter13/linear_multiple_36_0.png'>
</p>

يظهر لنا ان النموذج عادةً ابعد بحدود 20% من القيمه الصحيحه.

### استخدام كامل البيانات

لاحظ ان في مثالنا حتى الآن، المصفوفه $ \textbf{X} $ تحتوي على اربع أعمدة: أولها يحتوي على القيمه 1 في جميع الصفوف، عامود قوة السياره بالحصان، وزنها، وسنة الصناعه. ولكن، يسمح لنا النموذج بإستخدام اكثر من هذا العدد:

$$ \begin{aligned}
f_\boldsymbol\theta (\textbf{x}) &= \boldsymbol\theta \cdot \textbf{x}
\end{aligned} $$

عندما نضيف المزيد من الأعمدة لمصفوفتنا، نقوم بتوسيع $ \boldsymbol\theta $ لتحتوي على متغير لكل عامود في $ \textbf{x} $. بدلاً من اختيار فقط 3 أعمدة رقميه لإجراء التنبؤ، لماذا لا نستخدم جميع الأعمدة السبعه؟

```python
cols = ['bias', 'cylinders', 'displacement', 'horsepower',
        'weight', 'acceleration', 'model year', 'origin']
X = mpg_mat[cols].to_numpy()
mpg_mat[cols]
```

| origin | model year | acceleration | weight | horsepower | displacement | cylinders | bias   |        |
|:------:|:----------:|:------------:|:------:|:----------:|:------------:|:---------:|:------:|:------:|
| 1      | 70         | 12           | 3504   | 130        | 307          | 8         | 1      | 0      |
| 1      | 70         | 11\.5        | 3693   | 165        | 350          | 8         | 1      | 1      |
| 1      | 70         | 11           | 3436   | 150        | 318          | 8         | 1      | 2      |
| \.\.\. | \.\.\.     | \.\.\.       | \.\.\. | \.\.\.     | \.\.\.       | \.\.\.    | \.\.\. | \.\.\. |
| 1      | 82         | 11\.6        | 2295   | 84         | 135          | 4         | 1      | 389    |
| 1      | 82         | 18\.6        | 2625   | 79         | 120          | 4         | 1      | 390    |
| 1      | 82         | 19\.4        | 2720   | 82         | 119          | 4         | 1      | 391    |

```ruby
392 rows × 8 columns
```

```python
thetas_all = minimize(mse_loss, grad_mse_loss, X, y, progress=10)
print(f'theta: {thetas_all} | loss: {mse_loss(thetas_all, X, y):.2f}')
```

```ruby
theta: [0. 0. 0. 0. 0. 0. 0. 0.] | loss: 610.47
theta: [-0.5  -0.81  0.02 -0.04 -0.01 -0.07  0.59  1.3 ] | loss: 11.22
theta: [-17.23  -0.49   0.02  -0.02  -0.01   0.08   0.75   1.43] | loss: 10.85
theta: [-17.22  -0.49   0.02  -0.02  -0.01   0.08   0.75   1.43] | loss: 10.85
```

وفقاً لنتيجة النزول الإشتقاقي، فأن النموذج الخطي يمكننا تعريفه كالتالي:

$$ y = -17.22 - 0.49x_1 + 0.02x_2 - 0.02x_3 - 0.01x_4 + 0.08x_5 + 0.75x_6 + 1.43x_7 $$

نلاحظ ان خسارتنا قلت من 11.6 بإستخدام ثلاث أعمدة إلى 10.85 بإستخدام جميع الأعمدة الرقمية السبعه في بياناتنا. سنرى نسبة الخطأ في الرسم البياني لكل التوقعين السابق (بإستخدام ثلاث أعمدة) والجديد (بإستخدام سبع أعمدة):

```python
resid_prop_all = (y - linear_model(thetas_all, X)) / with_predictions['mpg']
plt.figure(figsize=(10, 4))
plt.subplot(121)
plt.scatter(np.arange(len(resid_prop)), resid_prop, s=15)
plt.title('Residual proportions using 3 columns')
plt.xlabel('Index of row in data')
plt.ylabel('Error proportion')
plt.ylim(-0.7, 0.7)

plt.subplot(122)
plt.scatter(np.arange(len(resid_prop_all)), resid_prop_all, s=15)
plt.title('Residual proportions using 7 columns')
plt.xlabel('Index of row in data')
plt.ylabel('Error proportion')
plt.ylim(-0.7, 0.7)

plt.tight_layout();
```

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter13/linear_multiple_42_0.png'>
</p>

على الرغم ان الفرق بسيط، نلاحظ ان الفرق اقل عندما نستخدم السبع أعمدة. كلا النموذجين افضل من النموذج الثابت، كما يوضح الرسم البياني التالي:

```python
constant_resid_prop = (y - with_predictions['mpg'].mean()) / with_predictions['mpg']
plt.scatter(np.arange(len(constant_resid_prop)), constant_resid_prop, s=15)
plt.title('Residual proportions using constant model')
plt.xlabel('Index of row in data')
plt.ylabel('Error proportion')
plt.ylim(-1, 1);
```

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter13/linear_multiple_44_0.png'>
</p>

استخدام النموذج الثابت وصلت فيه نتائج الخطأ إلى أكثر من 75% لكثير من السيارات!

### ملخص الإنحدار الخطي المتعدد

تعرفنا على النموذج الخطي للإنحدار. على عكس النموذج الثابت، الإنحدار الخطي يأخذ خصائص من البيانات بالحسبان عند اجراء التوقعات، مما يجعله اكثر فائدة عندما يكون لدينا علاقات في بياناتنا.

خطوات ضبط النموذج من المفترض ان تكون واضحه الآن:
- اختيار النموذج.
- اختيار دالة الخسارة.
- تقليل دالة الخساره بإستخدام النزول الإشتقاقي.

من المفيد معرفة ان بإمكاننا التعديل على احد المكونات دون الأخرى. في هذا الجزء، تعرفنا على النموذج الخطي دون التغير في دالة الخساره أو استخدام خوارزميات تقليل اخرى. على الرغم ان النمذجه قد تكون معقده، يكون اسهل التركيز على مكون واحد فقط في كل مره، ثم جمع المكونات مع بعضها البعض.

## المربعات الصغرى - منظور هندسي

لنتذكر اننا اوجدنا المتغيرات الرياضيه المثاليه للنموذج الخطي بواسطة التحسين من دالة الخساره بإستخدام النزول الإشتقاقي. ذكرنا أيضاً ان الإنحدار الخطي للمربعات الصغرى يمكن حسابه تحليلياً. على الرغم ان النزول الإشتقاقي اكثر عملياً، هذا المنظور الهندسي سيساعدك على فهم الإنحدار الخطي بشكل اكبر.

يتوقع من القارئ ان يكون على علم بفضاء المتجهات  Vector Space وطريقة القيام بالعمليات الحسابيه عليها.

لنفترض اننا نبحث عن النموذج الخطي للبيانات التاليه:

**y**|**x**
:-----:|:-----:
2|3
1|0
-2|-1

```python
data = pd.DataFrame(
    [
        [3,2],
        [0,1],
        [-1,-2]
    ],
    columns=['x', 'y']
)

sns.regplot(x='x', y='y', data=data, ci=None, fit_reg=False);
```

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter13/linear_projection_5_0.png'>
</p>

لنفترض أن النموذج المثالي هو النموذج بأقل خساره، وان خطأ المربعات الصغرى هي اداة مقبولة للقياس.

### المربعات الصغرى: النموذج الثابت

كما فعلنا في بيانات الإكراميات، لنبدأ بالنموذج الثابت: نموذج يتوقع رقم واحد فقط.

$$ \theta = C $$

نعمل نحن فقط مع قيم $ y $:

|**y**|
|:-----:|
|2|
|1|
|-2|

وهدفنا هو إيجاد قيمة $ \theta $ التي تنتج لنا خطاً يقلل من الخساره التربيعيه:

$$ \begin{split} L(\theta, \textbf{y}) = \sum_{i = 1}^{n}(y_i - \theta)^2\\ \end{split} $$

لنتذكر ان للنموذج الثابت، قيمة $ \theta $ التي تقلل الخساره في دالة الخطأ التربيعي المتوسط MSE هي $ \bar{\textbf{y}} $، متوسط قيم $ \textbf{y} $. يمكن إيجاد العملية الحسابية الكامله في درس [دوال الخساره](/chapter10/#%D8%AF%D8%A7%D9%84%D8%A9%20%D8%A7%D9%84%D8%AE%D8%B3%D8%A7%D8%B1%D9%87) في فصل [النماذج والتوقعات](/ds-100-ar/chapter10/). 

لاحظ ان دالة الخساره لدينا هي مجمع التربيع. القاعدة L2 للمتجهات هي ايضاً مجموع التربيع، ولكن مع الجذر التربيعي:

$$ \Vert \textbf{v} \Vert = \sqrt{v_1^2 + v_2^2 + \dots + v_n^2} $$

إذا جعلنا $ y_i - \theta = v_i $:

$$ \begin{split}
\begin{aligned}
L(\theta, \textbf{y}) 
&= v_1^2 + v_2^2 + \dots + v_n^2 \\
&= \Vert \textbf{v} \Vert^2
\end{aligned}
\end{split} $$

يعني ذلك ان بإمكاننا تعريف الخساره على انها تربيع القاعدة L2 لمتّجه $ \textbf{v} $. يمكننا وصف $ v_i $ كالتالي $ y_i - \theta \quad \forall i \in [1,n] $ إذاً ذلك في كتعريف ديكارتي:

$$ \begin{split}
\begin{aligned}
\textbf{v} \quad &= \quad \begin{bmatrix} y_1 - \theta \\ y_2 - \theta \\ \vdots \\ y_n - \theta \end{bmatrix} \\
&= \quad \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n  \end{bmatrix} \quad - \quad 
\begin{bmatrix} \theta \\ \theta \\ \vdots \\ \theta \end{bmatrix} \\
&= \quad \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n  \end{bmatrix} \quad - \quad 
\theta \begin{bmatrix} 1 \\ 1 \\ \vdots \\ 1 \end{bmatrix}
\end{aligned}
\end{split} $$

إذاً، يمكننا كتابة دالة الخساره كالتالي:

$$ \begin{split} 
\begin{aligned}
L(\theta, \textbf{y})
\quad &= \quad \left \Vert  \qquad   
\begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n  \end{bmatrix} \quad - \quad 
\theta \begin{bmatrix} 1 \\ 1 \\ \vdots \\ 1 \end{bmatrix}
\qquad \right \Vert ^2 \\
\quad &= \quad \left \Vert  \qquad  
\textbf{y} 
\quad - \quad 
\hat{\textbf{y}}
\qquad \right \Vert ^2 \\
\end{aligned}
\end{split} $$

الوصف $$ \theta \begin{bmatrix} 1 \\ 1 \\ \vdots \\ 1 \end{bmatrix} $$ هو مضاعف عددي للأعمدة في المتّجه $ \textbf{1} $، وهي ايضاً نتيجة التوقع، ويرمز لها $ \hat{\textbf{y}} $.

يعطينا ذلك منظوراً جديداً عن معنى تقليل الخساره في خطأ المربعات الصغرى.

قيم $ \textbf{y} $ و $ \textbf{1} $ ثابته، ولكن $ \theta $ يمكن أن تأخذ أي قيمة، لذا $ \hat{\textbf{y}} $ يمكن انت تكون اي مضاعف عددي ل $ \textbf{1} $. نريد إيجاد $ \theta $ لتكون $ \theta \textbf{1} $ اقرب ما تكون إلى $ \textbf{y} $. نستخدم $ \hat{\theta} $ لوصف ذلك الضبط المثالي ل $ \theta $.

### المربعات الصغرى: نموذج خطي بسيط

الآن، لنلقي نظره على نموذج الإنحدار الخطي البسيط. يشبه النموذج بشكل كبير لإشتقاق النموذج الثابت، ولكن لاحظ الفرق وفكر بطريقة عامة لإجراء الإنحدار الخطي المتعدد.

النموذج الخطي البسيط هو:

$$ \begin{split}
\begin{aligned}
f_\boldsymbol\theta (x_i) 
&= \theta_0 + \theta_1 x_i \\
\end{aligned}
\end{split} $$

هدفنا إيجاد $ \boldsymbol\theta $ التي تنتج لنا خطاً بأقل خطأ تربيعي:

$$ \begin{split}
\begin{aligned}
L(\boldsymbol\theta, \textbf{x}, \textbf{y})
&= \sum_{i = 1}^{n}(y_i - f_\boldsymbol\theta (x_i))^2\\
&= \sum_{i = 1}^{n}(y_i - \theta_0 - \theta_1 x_i)^2\\
&= \sum_{i = 1}^{n}(y_i - \begin{bmatrix} 1 & x_i \end{bmatrix}
\begin{bmatrix} 
     \theta_0 \\
     \theta_1
\end{bmatrix} ) ^2
\end{aligned}
\end{split} $$

لمساعدتنا على تحويل شكل جمع الخساره إلى شكل مصفوفه، دعنا نوسع من الخساره ب $ n = 3 $:

$$ \begin{split}
\begin{aligned}
L(\boldsymbol{\theta}, \textbf{x}, \textbf{y})
&=
(y_1 - \begin{bmatrix} 1 & x_1 \end{bmatrix}
\begin{bmatrix} 
     \theta_0 \\
     \theta_1
\end{bmatrix})^2  \\
&+
(y_2 - \begin{bmatrix} 1 & x_2 \end{bmatrix}
\begin{bmatrix} 
     \theta_0 \\
     \theta_1
\end{bmatrix})^2 \\
&+
(y_3 - \begin{bmatrix} 1 & x_3 \end{bmatrix}
\begin{bmatrix} 
     \theta_0 \\
     \theta_1
\end{bmatrix})^2 \\
\end{aligned}
\end{split} $$

مرة أخرى، دالة الخساره هي مجموع التربيع و القاعدة L2 للمتّجه هي الجذر التربيعي لمجموع التربيع:

$$ \Vert \textbf{v} \Vert = \sqrt{v_1^2 + v_2^2 + \dots + v_n^2} $$

إذا جعلنا $$ y_i - \begin{bmatrix} 1 & x_i \end{bmatrix}
\begin{bmatrix} 
     \theta_0 \\
     \theta_1
\end{bmatrix} $$ $ v_i = $: 

$$ \begin{split}
\begin{aligned}
L(\boldsymbol{\theta}, \textbf{x}, \textbf{y}) 
&= v_1^2 + v_2^2 + \dots + v_n^2 \\
&= \Vert \textbf{v} \Vert^2
\end{aligned}
\end{split} $$

كما في السابق، يمكننا وصف خسارتنا على انها تربيع القاعدة L2 للمتّجه $ \textbf{v} $. 

$$ v_i = y_i - \begin{bmatrix} 1 & x_i \end{bmatrix}
\begin{bmatrix} 
     \theta_0 \\
     \theta_1
\end{bmatrix} \quad \forall i \in [1,3] $$

$$ 
\begin{aligned}
L(\boldsymbol{\theta}, \textbf{x}, \textbf{y})
&= \left \Vert  \qquad   
\begin{bmatrix} y_1 \\ y_2 \\ y_3  \end{bmatrix} \quad - \quad 
\begin{bmatrix} 1 & x_1 \\ 1 & x_2 \\ 1 & x_3 \end{bmatrix}
\begin{bmatrix} 
     \theta_0 \\
     \theta_1
\end{bmatrix}
\qquad \right \Vert ^2 \\
&= \left \Vert  \qquad  
\textbf{y} 
\quad - \quad 
\textbf{X}
\begin{bmatrix} 
     \theta_0 \\
     \theta_1
\end{bmatrix}
\qquad \right \Vert ^2 \\
&= \left \Vert  \qquad  
\textbf{y} 
\quad - \quad 
f_\boldsymbol\theta(\textbf{x})
\qquad \right \Vert ^2 \\
&= \left \Vert  \qquad  
\textbf{y} 
\quad - \quad 
\hat{\textbf{y}}
\qquad \right \Vert ^2 \\
\end{aligned}
$$

عملية ضرب المصفوفة $$ \begin{bmatrix} 1 & x_1 \\ 1 & x_2 \\ 1 & x_3 \end{bmatrix}
\begin{bmatrix} 
     \theta_0 \\
     \theta_1
\end{bmatrix} $$ هي تركيبة خطية للأعمدة في $ \textbf{X} $: كل $ \theta_i $ يتم ضربها بعامود واحد من $ \textbf{X} $، يظهر لنا هذا المنظور ان $ f_\boldsymbol\theta $ تركيبة خطية للخصائص في بياناتنا.

$ \textbf{X} $ و $ \textbf{y} $ ثابتين، ولكن $ \theta_0 $ و $ \theta_1 $ يمكن أن يأخذان أي قيمه، لذا $ \hat{\textbf{y}} $ يمكن ان تأخذ اياً من التركيبات الخطية من الأعمدة في $ \textbf{X} $. للحصول على أقل خساره، تريد أن نختار $ \boldsymbol\theta $ التي فيها $ \hat{\textbf{y}} $ أقرب ما تكون إلى $ \textbf{y} $، يرمز لها بالرمز $ \hat{\boldsymbol\theta} $.

### الفكره الهندسيه

الآن، لنحاول تكوين فكرة عن اهمية ان تكون $ \hat{\textbf{y}} $ محدوده للتركيبات الخطية للأعمدة في $ \textbf{X} $. على الرغم ان مدى اي متّجه يحتوي على عدد غير محدود من التركيبات الخطية، غير محدود لا تعني اياً كان، التركيبات الخطية محدوده بواسطة أساس المتّجه.

للتذكير، هذه دالة الخساره ومخطط التشتت:

$$ L(\boldsymbol{\theta}, \textbf{x}, \textbf{y}) \quad = \quad \left \Vert  \quad  
\textbf{y} 
\quad - \quad 
\textbf{X} \boldsymbol\theta
\quad \right \Vert ^2 $$

```python
sns.regplot(x='x', y='y', data=data, ci=None, fit_reg=False);
```

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter13/linear_projection_19_0.png'>
</p>

من خلال مشاهدتنا لمخطط التشتت، نلاحظ انه لا يوجد خط مثالي للنقاط، لذا لن نستطيع الوصول إلى خساره تساوي صفر. نعرف ان $ \textbf{y} $ ليست على مستوى خطي مع $ \textbf{x} $ و $ \textbf{1}:

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter13/proj1.png'>
</p>

بما ان حساب الخساره يكون بالمسافه، يمكننا ملاحظة انه لتقليل الخساره $ L(\boldsymbol\theta, \textbf{x}, \textbf{y}) = \left \Vert  \textbf{y} - \textbf{X} \boldsymbol\theta \right \Vert ^2 $، فنريد ان تكون $ \textbf{X} \boldsymbol\theta $ اقرب ما تكون إلى $ \textbf{y} $.

رياضياً، نحنى نرى توقع $ \textbf{y} $ في فضاء المتّجه الممتد بواسطة الأعمدة في $ \textbf{X} $، لأن التوقع لأي متّجه هي اقرب نقطة في $ Span(\textbf{X})$ للمتّجه. لذا، إختيار $ \boldsymbol\theta $ يكون فيها $ \hat{\textbf{y}} $ $ \textbf{X}  \boldsymbol\theta = $ $ \mathit{proj}_{Span(\textbf{X})}  \textbf{y} = $ هو الحل المثالي.

> proj = projection = التوقع

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter13/proj2.png'>
</p>

لنعرف لماذا، لنأخذ بالإعتبار النقاط الأخرى في فضاء المتّجه، النقاط باللون البنفسجي:

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter13/proj3.png'>
</p>

بناءًا على نظرية فيثاغورس، اي نقطة على السطح هي ابعد عن $ \textbf{y} $ من $ \hat{\textbf{y}} $. طول العامود المقابل ل $ \hat{\textbf{y}} $ هو خطأ المربعات الصغرى. 

### الجبر الخطي

تحدثنا بشكل كبير عن الجبر الخطي، ما تبقى لنا الآن هو حل $ \hat{\boldsymbol\theta} $ التي تكون لنا ما نبحث عنه $ \hat{\textbf{y}} $

بعض النقاط نأخذها بالإعتبار:

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter13/proj4.png'>
</p>

- $ \hat{\textbf{y}} + \textbf{e} = \textbf{y} $
- $ \textbf{e} $ متوازيه عامودياً مع $ \textbf{x} $ و $ \textbf{1} $
- $ \hat{\textbf{y}} = \textbf{X} \hat{\boldsymbol\theta} $ هي المتّجه الأقرب إلى $ \textbf{y} $ في فضاء المتّجهات الممتدة من $ \textbf{x} $ و $ \textbf{1} $

ولذا، تنتج لنا المعادله التاليه:

$$ \textbf{X}  \hat{\boldsymbol\theta} + \textbf{e} = \textbf{y} $$

ضرب الجهه اليسرى لكل القيم ب $ \textbf{X}^T $ ينتج لنا التالي:

$$ \textbf{X}^T \textbf{X}  \hat{\boldsymbol\theta} + \textbf{X}^T \textbf{e} = \textbf{X}^T \textbf{y} $$

بما أن $ \textbf{e} $ متعامده مع الأعمدة في $ \textbf{X} $، فأن $ \textbf{X}^T \textbf{e} $ هو عامود متّجه يحتوي على اصفار. لذا، نصل إلى المعادلة التاليه:

$$ \textbf{X}^T \textbf{X}  \hat{\boldsymbol\theta} = \textbf{X}^T \textbf{y} $$

من هنا، يمكننا بسهولة الحل لإيجاد $ \hat{\boldsymbol\theta} $ بواسطة ضرب الجهه اليسرى في كلا الجانبين ب $ (\textbf{X}^T \textbf{X})^{-1} $:

$$ \hat{\boldsymbol\theta} = (\textbf{X}^T \textbf{X})^{-1} \textbf{X}^T \textbf{y} $$

ملاحظة: يمكننا الحصول على نفس النتجية بواسطة التقليل بإستخدام متجهات التفاضل والتكامل، ولكن بالنسبة لخطأ المربعات الصغرى، متجهات التفاضل والتكامل ليست ضروريه. لدوال الخساره الأخلى، سنحتاج لإستخدام متجهات التفاضل والتكامل للحصول على النتيجة التحليليه.

### انهاء الدراسه

لنعود لتجربتنا، لنطبق ما تعلمناه، ونشرح إجابتنا:

$$ \begin{split}
\textbf{y} = \begin{bmatrix} 2 \\ 1 \\ -2  \end{bmatrix} \qquad \textbf{X} = \begin{bmatrix} 1 & 3 \\ 1 & 0 \\ 1 & -1 \end{bmatrix}
\end{split} $$

$$ \begin{split}
\begin{align}
\hat{\boldsymbol\theta} 
&= 
\left(
\begin{bmatrix} 1 & 1 & 1 \\ 3 & 0 & -1 \end{bmatrix}
\begin{bmatrix} 1 & 3 \\ 1 & 0 \\ 1 & -1 \end{bmatrix}
\right)^{-1}
\begin{bmatrix} 1 & 1 & 1 \\ 3 & 0 & -1 \end{bmatrix}
\begin{bmatrix} 2 \\ 1 \\ -2  \end{bmatrix} \\
&= 
\left(
\begin{bmatrix} 3 & 2\\ 2 & 10 \end{bmatrix}
\right)^{-1}
\begin{bmatrix} 1 \\ 8 \end{bmatrix} \\
&=
\frac{1}{30-4}
\begin{bmatrix} 10 & -2\\ -2 & 3 \end{bmatrix}
\begin{bmatrix} 1 \\ 8 \end{bmatrix} \\
&=
\frac{1}{26}
\begin{bmatrix} -6 \\ 22 \end{bmatrix}\\
&=
\begin{bmatrix} - \frac{3}{13} \\ \frac{11}{13} \end{bmatrix}
\end{align}
\end{split} $$

قمنا تحليلياً بإيجاد النموذج المثالي لإنحدار المربعات الصغرى وهو $ f_\boldsymbol{\boldsymbol\theta}(x_i) = - \frac{3}{13} + \frac{11}{13} x_i $. نعرف ان اختيارنا ل $ \boldsymbol\theta $ صحيح بإستخدام الخاصيه الرياضية التي تقول توقع $ \textbf{y} $ لأمتداد الأعمدة في $ \textbf{X} $ ينتج لنا اقرب نقطة في فضاء المتّجه ل $ \textbf{y} $. تحت القيود الخطيه بإستخدام خسارة المربعات الصغرى، الحل ل $ \hat{\boldsymbol\theta} $ ينتج لنا توقع مضمون انه الحل الأفضل.

### عندما تعتمد المتغيرات خطياً

لكل متغير اضافي، نضيف عامود جديد إلى $ \textbf{X} $. امتداد أعمدة ل $ \textbf{X} $ هو التركيب الخطي لأعمدة المتّجهات، لذا إضافة أعمدة تغير من الإمتداد فقط اذا كانت مستقلة خطياً عن بقية الأعمدة الموجوده مسبقاً.

عندما يكون العامود المضاف غير مستقل خطياً، يمكن وصفة كتركيب خطي من احد الأعمدة الأخرى، لذا، لن يكون لنا اي متّجه جديده في الفضاء الجزئي.

لنتذكر ان امتداد $ \textbf{X} $ مهم لأنه الفضاء الجزئي الذي نريد ان نتوقع $ \textbf{y} $ فيه. إذا لم يتغير هذا الفضاء، فأن التوقع لن يتغير.

مثلاً، عندما عرفنا $ \textbf{x} $ على النموذج الثابت لنحصل على النموذج الخطي البسيط، عرفنا دالة مستقله. $$ \textbf{x} = \begin{bmatrix} 3 \\ 0 \\ -1 \end{bmatrix} $$ لا يمكن وصفها كعدديه من $$ \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix} $$. لذا، انتقلنا من إيجاد توقع ل $ \textbf{y} $ في الخط:

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter13/1dprojection.png'>
</p>

إلى ايجاد توقع ل $ \textbf{y} $ على السطح:

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter13/proj1.png'>
</p>

الآن، لنعرف متغير آخر، $ \textbf{z} $، يكون بشكل واضح هذا العامود متحيز:

**y**|**x**|**1**|**z**
:-----:|:-----:|:-----:|:-----:
2|3|1|4
1|0|1|1
-2|-1|1|0

لاحظ ان $ \textbf{z} = \textbf{1} + \textbf{x} $. بما ان $ \textbf{z} $ متركبة خطياً من $ \textbf{1} $ و $ \textbf{x} $، فأنها تقع في $ Span(\textbf{X}) $. الآن، $ \textbf{z} $ معتمده خطياً على $ \\{\textbf{1} ,\textbf{x}\\} $ ولا تغيّر في $ Span(\textbf{X}) $. لذا، توقع $ \textbf{y} $ في الفضاء الجزئي الممتد بواسطة $ \textbf{1} $، $ \textbf{x} $ و $ \textbf{z} $ سيكون مطابق لتوقع $ \textbf{y} $ في الفضاء الجزئي الممتد بواسطة $ \textbf{1} $ و $ \textbf{x} $.

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter13/dependent_variablesz.png'>
</p>

يمكننا ايضاً ملاحظة ذلك عند تقليل دالة الخساره:

$$ \begin{split} 
\begin{aligned}
L(\boldsymbol\theta, \textbf{d}, \textbf{y})
&= \left \Vert  \qquad   
\begin{bmatrix} y_1 \\ y_2 \\ y_3  \end{bmatrix} \quad - \quad 
\begin{bmatrix} 1 & x_1 & z_1 \\ 1 & x_2 & z_2\\ 1 & x_3 & z_3\end{bmatrix}
\begin{bmatrix} 
     \theta_0 \\
     \theta_1 \\
     \theta_2
\end{bmatrix}
\qquad \right \Vert ^2
\end{aligned}
\end{split} $$

الحل المتوقع لنا سيكون كالشكل التالي $ \theta_0 \textbf{1} + \theta_1 \textbf{x} + \theta_2 \textbf{z} $.

بما ان $ \textbf{z} = \textbf{1} + \textbf{x} $، ايضاً كانت $ \theta_0 $، $ \theta_1 $ و $ \theta_2 $، القيم المتوقه يمكن إعادة كتابتها كالتالي:

$$ \begin{split}
\begin{aligned}
\theta_0 \textbf{1} + \theta_1 \textbf{x} + \theta_2 (\textbf{1} + \textbf{x})
&= 
(\theta_0 + \theta_2) \textbf{1} + (\theta_1 + \theta_2) \textbf{x} \\
\end{aligned}
\end{split} $$

إذاً، اضافة $ \textbf{z} $ لن تغير اي شيء. الفرق الوحيد هو ان بإمكاننا وصف هذا التوقع بعدة أشكال. لنتذكر اننا اوجدنا توقع $ \textbf{y} $ على امتداد $ \textbf{1} $ و $ \textbf{x} $ وكان:

$$ \begin{split} \begin{bmatrix} \textbf{1} & \textbf{x} \end{bmatrix}  \begin{bmatrix} - \frac{3}{13} \\ \frac{11}{13} \end{bmatrix} = - \frac{3}{13} \textbf{1} + \frac{11}{13} \textbf{x}\end{split} $$

لكن، مع التعرف على $ \textbf{z} $، يمكننا وصف توقع المتّجه بأكثر من طريقه.

بما ان $ \textbf{1} = \textbf{z} - \textbf{x} $، ف $ \hat{\textbf{y}} $ يمكن وصفها كالتالي:

$$ - \frac{3}{13} (\textbf{z} - \textbf{x}) + \frac{11}{13} \textbf{x} = - \frac{3}{13} \textbf{z} + \frac{14}{13} \textbf{x} $$

بما ان $ \textbf{x} = \textbf{z} + \textbf{1} $، ف $ \hat{\textbf{y}} $ يمكن وصفها كالتالي:

$$ - \frac{3}{13} \textbf{1} + \frac{11}{13} (\textbf{z} + \textbf{1}) = \frac{8}{13} \textbf{1} + \frac{11}{13} \textbf{z} $$

ولكن جميع الأوصاف الثلاثه تقدم نفس التوقع.

في الختام، اضافة عامود غير مستقل خطياً إلى $ \textbf{x} $ لا يغير في $ Span(\textbf{X}) $، ولذلك لن يغير في التوقع والحل لمشكلة المربعات الصغرى.

### طريقتين للتفكير

اضفنا مخططات التشتت مرتين في هذا الدرس. اول مره للتذكير انه كما في السابق، نحاول ايجاد الخط المثالي للبيانات. المره الثانية كانت تأكد انه لا يوجد خط مثالي لجميع النقاط. بصرف النظر هن ذلك، حاولنا عدم تخريب رسم فضاء المتّجه بمخططات التشتت. ذلك لأن مخططات التشتت تتوافق مع نظرية مساحة الصف في مشكلة المربعات الصغرى: النظر لكل نقاط البيانات ومحاولة التقليل في المسافه بين توقعنا و كل نقطة. في هذا الدرس، تعرفنا على نطريقة مساحة العامود: كل متغير كان متّجه، تكون لنا فضاءًا من الإجابات المُحتملة (التوقعات).

## تطبيق عملي للإنحدار الخطي

في هذا الجزء، سنقوم بتطبيق عملي لنموذج الإنحدار الخطي على بيانات. لدى البيانات التي سنعمل عليها العديد من الخصائص، مثل الطول والحجم لحيوان الِحمار.

مهمتنا هي توقع الوزن بإستخدام الإنحدار الخطي.

### نظره عامه على البيانات

سنبدأ أولاً بقراءة البيانات وإخذ نظره سريعه على محتواها:

> لتحميل البيانات، [اضغط هنا]({{ site.baseurl }}/files/chapter13/donkeys.csv).

```python
import pandas as pd 

donkeys = pd.read_csv("donkeys.csv")
donkeys.head()
```

**WeightAlt**|**Weight**|**Height**|**...**|**Sex**|**Age**|**BCS**| 
:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:
NaN|77|90|...|stallion|2>|3|0
NaN|100|94|...|stallion|2>|2.5|1
NaN|74|95|...|stallion|2>|1.5|2
NaN|116|96|...|female|2>|3|3
NaN|91|91|...|female|2>|2.5|4

```ruby
5 rows × 8 columns
```

فكرة جيدة دائماً هي النظر *لعدد* البيانات لدينا عن طريق عرض مقاسات الت DataFrame. عندما يكون حجم البيانات لدينا كبير، طباعة كامل البيانات قد يسبب مشاكل للمتصفح وتتسبب بإغلاقه:

```python
donkeys.shape
```

```ruby
(544, 8)
```

البيانات قليله نسبياً، لدينا فقط 544 سطر و 8 أعمدة. لنرى ما هي الأعمدة المتوفره لدينا:

```python
donkeys.columns.values
```

```ruby
array(['BCS', 'Age', 'Sex', 'Length', 'Girth', 'Height', 'Weight',
       'WeightAlt'], dtype=object)
```

فهم البيانات لدينا يساعدنا على توجيه عملية تحليلنا لها، يجب علينا فهم ما يحتويه كل عامود. بعض هذه الأعمدة واضحه من مسمياتها، ولكن بعضها يحتاج شرحاً اكثر:
- `BCS` (Body Condition Score): حالة الجسم (تقييم لصحة جسد الحيوان).
- `Grith`: مقياس لمتوسط جسم الحيوان.
- `WeightAlt`: وزن آخر (من بين البيانات لدينا تم وزن بعضهم مرتين وعددهم 31، تم وزنهم مرتين للتأكد من دقة الوزن).

فكرة مناسبة الآن هو تحديد اي الأعمدة تحتوي على بيانات كمية وأيها يحتوي على بيانات أسميه.

الكمية: `Length`، `Girth`، `Height`، `Weight` و `WeightAlt`.  
الأسمية: `BCS`، `Age` و `Sex`.

### تنظيف البيانات

في هذا الجزء، سنتحقق من البيانات اذا كانت تحتوي على اي شذوذ ونتعامل معها.

عن طريق التحقق من عامود `WeightAlt`، يمكننا التأكد من دقة الوزن بأخذ الفرق بين الوزنين ورسمها النتائج:

```python
difference = donkeys['WeightAlt'] - donkeys['Weight']
sns.distplot(difference.dropna());
```

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter13/linear_case_study_11_0.png'>
</p>

الأوزان تبدو معقولة بفارق 1 أو أقل كيلو قرام فيما بينها.

الآن، يمكننا النظر إلى قيم غريبه والتي قد تظر ان لدينا مشاكل أو أخطاء. يمكننا استخدام الدالة [Quantile](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.quantile.html) للكشف عن القيم الشاذه:

```python
donkeys.quantile([0.005, 0.995])
```

**WeightAlt**|**Weight**|**Height**|**Girth**|**Length**|**BCS**| 
:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:
98.75|71.715|89|90|71.145|1.5|0.005
192.8|214|112|131.285|111|4|0.995

> شرح مبسط للداله quantile:  
> تقوم الداله بإظهار نتائج بناءًا على النسبه المعطاه لها، في المثال السابق طلبنا نتائج من هم في النسبه 0.005 وأقل و منهم في النسبه 0.995 وأقل.  
> **مثلاً** في العامود Height، يظهر لنا أن 0.5% أو أقل يبلغ طولهم 89.0 بينما 99.5% أو أقل يبلغ طولهم 112.0 في بياناتنا.

لكل الأعمدة الكمية، يمكن أن نبحث عن اي سطر تكون نتيجته خارج هذه الأعداد، لأننا نريد أن يطبق النموذج على اي حيوان صحته ممتازه وناضح.

أولاً، لنرى العامود `BCS`:

```python
donkeys[(donkeys['BCS'] < 1.5) | (donkeys['BCS'] > 4)]['BCS']
```

```ruby
291    4.5
445    1.0
Name: BCS, dtype: float64
```

ايضاً عند النظر على المخطط الشريطي للعامود `BCS`:

```python
plt.hist(donkeys['BCS'], density=True)
plt.xlabel('BCS');
```

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter13/linear_case_study_17_0.png'>
</p>

بالأخذ بالأعتبار ان العامود `BCS` يظهر لنا مدى صحة ونضوج الحيوان، حصول الحيوان على 1 يبين انه هزيل وحصوله على 4.5 يبين ان وزنه زائد. أيضاً بالنظر إلى المخطط الشريطي، فط يظهر لدينا اثنين بقيم شاذه في العامود `BCS`. لذا سنقوم بحذف هذه البيانات.

الآن، لنلقي نظره على الأعمدة `Length`، `Girth` و `Height`:

```python
donkeys[(donkeys['Length'] < 71.145) | (donkeys['Length'] > 111)]['Length']
```

```ruby
8       46
22      68
26      69
216    112
Name: Length, dtype: int64
```

```python
donkeys[(donkeys['Girth'] < 90) | (donkeys['Girth'] > 131.285)]['Girth']
```

```ruby
8       66
239    132
283    134
523    134
Name: Girth, dtype: int64
```

```python
donkeys[(donkeys['Height'] < 89) | (donkeys['Height'] > 112)]['Height']
```

```ruby
8       71
22      86
244    113
523    116
Name: Height, dtype: int64
```

بالنسبه لهذه الأعمدة الثلاثه، يبدو ان السطر 8 يحتوي على قيم ادنى من باقي البيانات بينما الأسطر الأخرى قريبه جداً من باقي البيانات ولا يحتاج أن نحذفها.

أخيراً، لنرى العامود `Weight`:

```python
donkeys[(donkeys['Weight'] < 71.715) | (donkeys['Weight'] > 214)]['Weight']
```

```ruby
8       27
26      65
50      71
291    227
523    230
Name: Weight, dtype: int64
```

أو سطرين وآخر سطرين بعيدة جداً عن بقية البيانات وعلى الأرجح سنقوم بحذفها. يمكننا أن نبقي السطر في المنتصف كونه قريباً جدناً من بقية البيانات.

بما ان العامود `WeightAlt` مشابه للعامود `Weight`، لن نقوم بالبحث عن شواذ فيه. ولتجميع ما فهمناه، هذا ما سنقوم بفلترته في بياناتنا:
- الأبقاء على البيانات بين 1.5 و 4 في العامود `BCS`.
- الأبقاء على الأسطر التي بياناتها بين 71 و 214 في العامود `Weight`.

```python
donkeys_c = donkeys[(donkeys['BCS'] >= 1.5) & (donkeys['BCS'] <= 4) &
                         (donkeys['Weight'] >= 71) & (donkeys['Weight'] <= 214)]
```

### فصل بيانات التدريب والإختبار

قبل أن نبدأ بتحليل البيانات، نريد أن نقسم البيانات إلى تقسيم 80/20، نستخدم فيها 80% من البيانات لتدريب النموذج، ونترك 20% لتقييم وإختبار النموذج:

```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(donkeys_c.drop(['Weight'], axis=1),
                                                    donkeys_c['Weight'],
                                                    test_size=0.2,
                                                   random_state=42)
X_train.shape, X_test.shape
```

```ruby
((431, 7), (108, 7))
```

> أستخدم الكاتب دالة train_test_split والتي تأتي من مكتبة sklearn وهي مكتبة متخصصه بأدوات تحليل البيانات وتعلم الآله.  

نقوم أيضاً بإنشاء دالة تقييم التوقع على بيانات الإختبار، لنستخدم الخطأ التربيعي المتوسط:

```python
def mse_test_set(predictions):
    return float(np.sum((predictions - y_test) ** 2))
```

### استكشاف البيانات وتصويرها

كالمعتاد، سنتحقق من البيانات قبل محاولة ضبط النموذج عليها.

أولاً، لنطلع على البيانات الإسميه بإستخدام مخطط الصندوق:

```python
sns.boxplot(x=X_train['BCS'], y=y_train);
```

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter13/linear_case_study_33_0.png'>
</p>

يبدو أن متوسط الوزن يزداد مع `BCS`، وليس خطياً:

```python
sns.boxplot(x=X_train['Sex'], y=y_train,
            order = ['female', 'stallion', 'gelding']);
```

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter13/linear_case_study_35_0.png'>
</p>

يظهر أن الجنس لا يتأثر كثيراً بالوزن

```python
sns.boxplot(x=X_train['Age'], y=y_train, 
            order = ['<2', '2-5', '5-10', '10-15', '15-20', '>20']);
```

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter13/linear_case_study_37_0.png'>
</p>

بعمر 5 أو أكثر، توزيع الأوزان لا يظهر أي تغير كبير.

الآن، لنلقي نظره على البيانات الكمية. يمكننا رسم كل واحدٍ منها مع المتغير الذي نريد التنبؤ عنه:

```python
X_train['Weight'] = y_train
sns.regplot('Length', 'Weight', X_train, fit_reg=False);
```

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter13/linear_case_study_39_0.png'>
</p>

```python
sns.regplot('Girth', 'Weight', X_train, fit_reg=False);
```

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter13/linear_case_study_40_0.png'>
</p>

```python
sns.regplot('Height', 'Weight', X_train, fit_reg=False);
```

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter13/linear_case_study_41_0.png'>
</p>

جميع البيانات الكمية لديها علاقة خطية مع المتغير الذي نريد توقعه `Weight`، لذا لا نحتاج للقيام بأي تعديلات على البيانات التي سندرب النموذج عليها.

أيضاً تبدو فكرة جيده إذا رأينا بين كل متغير وآخر إذا كانت العلاقه بينهم خطية. سنقوم برسم أثنين:

```python
sns.regplot('Height', 'Length', X_train, fit_reg=False);
```

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter13/linear_case_study_43_0.png'>
</p>

```python
sns.regplot('Height', 'Girth', X_train, fit_reg=False);
```

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter13/linear_case_study_44_0.png'>
</p>

من هذه الرسوم، نلاحظ أيضاً ان المتغيرات التي تساعدنا على التنبؤ لديها علاقة خطية قويه فيما بينها. يصعب ذلك من عملية تفسير نتائج النموذج، لذا لنتذكر لذلك بعد إنشاء النموذج.

### نماذج خطية أبسط

بدلاً من إستخدام كل البيانات مره واحدة، لنجرب ضبط النموذج على متغير أو أثنين أولاً.

في الأسفل ثلاث نماذج إنحدار خطي فقط بإستخدام متغير كمي واحد. أي هذه النماذج يبدو الأفضل؟

```python
sns.regplot('Length', 'Weight', X_train, fit_reg=True);
```

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter13/linear_case_study_46_0.png'>
</p>

```python
from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(X_train[['Length']], X_train['Weight'])
predictions = model.predict(X_test[['Length']])
print("MSE:", mse_test_set(predictions))
```

```ruby
MSE: 26052.58007702549
```

```python
sns.regplot('Girth', 'Weight', X_train, fit_reg=True);
```

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter13/linear_case_study_48_0.png'>
</p>

```python
model = LinearRegression()
model.fit(X_train[['Girth']], X_train['Weight'])
predictions = model.predict(X_test[['Girth']])
print("MSE:", mse_test_set(predictions))
```

```ruby
MSE: 13248.814105932383
```

```python
sns.regplot('Height', 'Weight', X_train, fit_reg=True);
```

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter13/linear_case_study_50_0.png'>
</p>

```python
model = LinearRegression()
model.fit(X_train[['Height']], X_train['Weight'])
predictions = model.predict(X_test[['Height']])
print("MSE:", mse_test_set(predictions))
```

```ruby
MSE: 36343.308584306134
```

بالنظر إلى مهطط التشتت ونتيجة MSE، يبدو لنا ان `Girth` هي الأفضل لتوقع الوزن وحدها كون لديها علاقة خطية قوية مع `Weight` ولديها أقل قيمة للخطأ التربيعي المتوسط.

هل سنحصل على أداء أفضل عند إستخدام مُتغيرين؟ لنجرب ضبط النموذج بإستخدام `Girth` و `Length`. على الرغم انه من الصعب رسم هذا النموذج، يمكننا رؤية نتيجة الخطأ التربيعي المتوسط:

```python
model = LinearRegression()
model.fit(X_train[['Girth', 'Length']], X_train['Weight'])
predictions = model.predict(X_test[['Girth', 'Length']])
print("MSE:", mse_test_set(predictions))
```

```ruby
MSE: 9680.902423377258
```

رائع! يبدو أن MSE تم تقليلها لدينا من حوالي 13000 بإستخدام `Girth` وحدها إلى 10000 بإستخدام `Girth` و `Length`. أضافة متغير أخر حسن من نموذجنا.

يمكننا أيضاً إستخدام المتغيرات الأسميه في نموذجنا. لنرى الآن كيف نستخدم النموذج الخطي مع البيانات الأسميه في العامود `Age`. هذا الرسم البياني ل `Age` و `Weight`:

```python
sns.stripplot(x='Age', y='Weight', data=X_train, order=['<2', '2-5', '5-10', '10-15', '15-20', '>20']);
```

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter13/linear_case_study_55_0.png'>
</p>

بما أن البيانات في العامود `Age` أسميه، فنحتاج لإستخدام بيانات رقمية مطابقه لها لنتمكن من تطبيق نموذج الإنحدار الخطي عليه:

```python
just_age_and_weight = X_train[['Age', 'Weight']]
with_age_dummies = pd.get_dummies(just_age_and_weight, columns=['Age'])
model = LinearRegression()
model.fit(with_age_dummies.drop('Weight', axis=1), with_age_dummies['Weight'])

just_age_and_weight_test = X_test[['Age']]
with_age_dummies_test = pd.get_dummies(just_age_and_weight_test, columns=['Age'])
predictions = model.predict(with_age_dummies_test)
print("MSE:", mse_test_set(predictions))
```

```ruby
MSE: 41398.515625
```

> استخدم الكاتب الدالة `get_dummies` لتحويل المتغيرات الإسميه إلى بيانات وهوية كمية تتكون من 0 و 1، يطلق على هذه الطريقه ب One-hot encoding ويمكن وصفها في الصوره التاليه:
>
> <p align="center"><a href="https://alioh.github.io/DSND-Notes-2/"><img src='{{ site.baseurl }}/img/chapter13/one-hot-encoding.jpeg'></a>
> </p> 
>

نتيجة 40000 أسوء بكثير من ما حصلنا عليه بإستخدام متغير كمي واحد، ولكن هذا المتغير قد يثبت أهميته في نموذجنا الخطي.

لنحاول تفسير هذا النموذج. لاحظ أن أي حِمار عمره لنقل بين 2 و 5 سنوات، سيحصل على نفس التوقع لأن لديهم نفس المدخلات: 1 للعامود الي يحدد العمر بين 2-5 سنوات. و 0 في بقية الأعمدة. لذا، يمكننا تفسير المتغيرات الأسميه ببساطه بتغير القيم في النموذج لأن المتغيرات الأسميه تفصلهم إلى مجموعات وتعطي توقع واحد لجميع من في هذه المجموعه.

خطوتنا القادمه هي بناء النموذج النهائي بإستخدام المتغيرات الأسميه وأكثر من متغير كمي.

### تحويل المتغيرات

لنتذكر من رسمات الصندوق السابقه أن العامود `Sex` لم يكن مفيداً، لذا سنتخلص منه. سنقوم أيضاً بحذف العامود `WeightAlt` لأن لدينا فقط 31 منها. أخيراً، إستخدام `get_dummies`، لتحويل البيانات الأسميه فيه `BCS` و `Age` إلى بيانات وهمية كمية لنتمكن من إدخالها إلى النموذج:

```python
# هذا الخيار لزيادة عدد الأعمدة التي تظهر في جوبتر
pd.set_option('max_columns', 15)

X_train.drop(['Sex', 'WeightAlt'], axis=1, inplace=True)
X_train = pd.get_dummies(X_train, columns=['BCS', 'Age'])
X_train.head()
```

**Age\_>20**|**Age\_<2**|**Age\_5-10**|**Age\_2-5**|**Age\_15-20**|**Age\_10-15**|**BCS\_4.0**|**BCS\_3.5**|**BCS\_3.0**|**BCS\_2.5**|**BCS\_2.0**|**BCS\_1.5**|**Height**|**Girth**|**Length**| 
:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:
0|0|0|1|0|0|0|0|1|0|0|0|99|113|98|465
0|0|0|0|0|1|0|0|1|0|0|0|101|119|101|233
0|0|0|0|0|1|0|0|0|1|0|0|103|125|106|450
0|0|0|1|0|0|0|0|0|1|0|0|100|120|93|453
0|0|1|0|0|0|0|0|0|1|0|0|108|120|98|452

لنتذكر أننا لاحظنا توزيع الوزن لمن أعمارهم أكبر من 5 وعدم وجود فارق كبير. لذا، سنجمع الأعمدة `Age_10-15`، `Age_15-20`، و `Age_>20` إلى عامود واحد:

```python
age_over_10 = X_train['Age_10-15'] | X_train['Age_15-20'] | X_train['Age_>20']
X_train['Age_>10'] = age_over_10
X_train.drop(['Age_10-15', 'Age_15-20', 'Age_>20'], axis=1, inplace=True)
```

لأننا لا نريد ان تكون مصفوفتنا تحتوي على متغيرات كثيره، لنقوم بحذف واحد من كلا الأعمده الأسمية `BCS` و `Age`:

```python
X_train.drop(['BCS_3.0', 'Age_5-10'], axis=1, inplace=True)
X_train.head()
```

**Age\_>10**|**Age\_<2**|**Age\_2-5**|**BCS\_4.0**|**BCS\_3.5**|**BCS\_2.5**|**BCS\_2.0**|**BCS\_1.5**|**Height**|**Girth**|**Length**| 
:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:
0|0|1|0|0|0|0|0|99|113|98|465
1|0|0|0|0|0|0|0|101|119|101|233
1|0|0|0|0|1|0|0|103|125|106|450
0|0|1|0|0|1|0|0|100|120|93|453
0|0|0|0|0|1|0|0|108|120|98|452

يجب علينا إضافة عامود جديد لإظهار الإنحياز في النموذج:

```python
# اضافة عامود bias
X_train = X_train.assign(bias=1)
X_train = X_train.reindex(columns=['bias'] + list(X_train.columns[:-1]))
X_train.head()
```

**Age\_>10**|**Age\_<2**|**Age\_2-5**|**BCS\_4.0**|**BCS\_3.5**|**BCS\_2.5**|**BCS\_2.0**|**BCS\_1.5**|**Height**|**Girth**|**Length**|**bias** 
:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:
0|0|1|0|0|0|0|0|99|113|98|1|465
1|0|0|0|0|0|0|0|101|119|101|1|233
1|0|0|0|0|1|0|0|103|125|106|1|450
0|0|1|0|0|1|0|0|100|120|93|1|453
0|0|0|0|0|1|0|0|108|120|98|1|452

### نموذج الإنحدار الخطي المتعدد

نحن الآن جاهزون لضبط النموذج بإستخدام جميع المتغيرات التي قررنا انها الأهم وبعد أن حولناها لشكل مناسب للنموذج.

يمكننا تعريف نموذجنا كالتالي:

$$ f_\theta (\textbf{x}) = \theta_0 + \theta_1 (Length) + \theta_2 (Girth) + \theta_3 (Height) + ... + \theta_{11} (Age\_>10) $$

هذه هي الدوال التي عرفناها في درس الإنحدار الخطي المتعدد، والتي سنستخدمها مره أخرى:

```python
def linear_model(thetas, x):
    return  X @ thetas

def mse_loss(thetas, x_vals, y_vals):
    return np.mean((y - linear_model(thetas, X)) ** 2)

def grad_mse_loss(thetas, x_vals, y_vals):
    n = len(x_vals)
    return -2 / n * (X.T @ y  - X.T @ X @ thetas)
```

للقدره على إستخدام الدوال السابقه، نريد `X` و `y`. يمكننا أن نحصل عليهما من بياناتنا. تذكر ان `X` و `y` يجب ان تكون مصفوفات في NumPy لتتمكن من القيام بعملية الضرب بإستخدام الرمز `@`:

```python
X_train = X_train.values
y_train = y_train.values
```

الآن، فقط نحتاج لإستخدام الدالة `minimize` التي عرفناها في الجزء السابق:

```python
thetas = minimize(mse_cost, grad_mse_cost, X_train, y_train)
```

```ruby
theta: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] | cost: 23979.72
theta: [0.01 0.53 0.65 0.56 0.   0.   0.   0.   0.   0.   0.   0.  ] | cost: 1214.03
theta: [-0.07  1.84  2.55 -2.87 -0.02 -0.13 -0.34  0.19  0.07 -0.22 -0.3   0.43] | cost: 1002.46
theta: [-0.25 -0.76  4.81 -3.06 -0.08 -0.38 -1.11  0.61  0.24 -0.66 -0.93  1.27] | cost: 815.50
theta: [-0.44 -0.33  4.08 -2.7  -0.14 -0.61 -1.89  1.02  0.4  -1.06 -1.57  2.09] | cost: 491.91
theta: [-1.52  0.85  2.   -1.58 -0.52 -2.22 -5.63  3.29  1.42 -2.59 -5.14  5.54] | cost: 140.86
theta: [-2.25  0.9   1.72 -1.3  -0.82 -3.52 -7.25  4.64  2.16 -2.95 -7.32  6.61] | cost: 130.33
theta: [ -4.16   0.84   1.32  -0.78  -1.65  -7.09 -10.4    7.82   4.18  -3.44
 -12.61   8.24] | cost: 116.92
theta: [ -5.89   0.75   1.17  -0.5   -2.45 -10.36 -11.81  10.04   6.08  -3.6
 -16.65   8.45] | cost: 110.37
theta: [ -7.75   0.67   1.13  -0.35  -3.38 -13.76 -11.84  11.55   8.2   -3.8
 -20.     7.55] | cost: 105.74
theta: [ -9.41   0.64   1.15  -0.31  -4.26 -16.36 -10.81  11.97  10.12  -4.33
 -21.88   6.15] | cost: 102.82
theta: [-11.08   0.66   1.17  -0.32  -5.18 -18.28  -9.43  11.61  11.99  -5.37
 -22.77   4.69] | cost: 100.70
theta: [-12.59   0.69   1.16  -0.32  -6.02 -19.17  -8.53  10.86  13.54  -6.65
 -22.89   3.73] | cost: 99.34
theta: [-14.2    0.72   1.14  -0.3   -6.89 -19.35  -8.29  10.03  14.98  -7.99
 -22.74   3.14] | cost: 98.30
theta: [-16.14   0.73   1.11  -0.26  -7.94 -19.03  -8.65   9.3   16.47  -9.18
 -22.59   2.76] | cost: 97.35
theta: [-18.68   0.73   1.1   -0.21  -9.27 -18.29  -9.42   8.76  18.14 -10.04
 -22.55   2.39] | cost: 96.38
theta: [-21.93   0.72   1.1   -0.17 -10.94 -17.19 -10.25   8.5   19.92 -10.36
 -22.66   1.99] | cost: 95.35
theta: [-26.08   0.7    1.13  -0.14 -13.03 -15.78 -10.79   8.54  21.78 -10.05
 -22.83   1.59] | cost: 94.18
theta: [-31.35   0.69   1.17  -0.13 -15.59 -14.12 -10.69   8.9   23.61  -9.19
 -22.93   1.32] | cost: 92.84
theta: [-37.51   0.7    1.21  -0.13 -18.44 -12.47  -9.79   9.52  25.14  -8.06
 -22.78   1.38] | cost: 91.40
theta: [-43.57   0.72   1.23  -0.12 -21.06 -11.3   -8.4   10.2   25.98  -7.16
 -22.24   1.87] | cost: 90.06
theta: [-48.96   0.74   1.23  -0.1  -23.13 -10.82  -7.13  10.76  26.06  -6.79
 -21.34   2.6 ] | cost: 88.89
theta: [-54.87   0.76   1.22  -0.05 -25.11 -10.88  -6.25  11.22  25.55  -6.8
 -20.04   3.41] | cost: 87.62
theta: [-63.83   0.78   1.21   0.02 -27.82 -11.42  -5.83  11.68  24.36  -6.96
 -17.97   4.26] | cost: 85.79
theta: [-77.9    0.8    1.22   0.13 -31.81 -12.47  -6.17  12.03  22.29  -6.98
 -14.93   4.9 ] | cost: 83.19
theta: [-94.94   0.81   1.26   0.23 -36.3  -13.73  -7.37  11.98  19.65  -6.47
 -11.73   4.88] | cost: 80.40
theta: [-108.1     0.81    1.34    0.28  -39.34  -14.55   -8.72   11.32   17.48
   -5.47   -9.92    4.21] | cost: 78.34
theta: [-115.07    0.81    1.4     0.29  -40.38  -14.75   -9.46   10.3    16.16
   -4.47   -9.7     3.5 ] | cost: 77.07
theta: [-119.8     0.81    1.44    0.28  -40.43  -14.6    -9.61    9.02   15.09
   -3.67  -10.25    3.05] | cost: 76.03
theta: [-125.16    0.82    1.47    0.3   -40.01  -14.23   -9.3     7.48   13.79
   -3.14  -11.09    2.94] | cost: 74.96
theta: [-131.24    0.83    1.48    0.33  -39.39  -13.76   -8.71    6.21   12.41
   -3.16  -11.79    3.17] | cost: 74.03
theta: [-137.42    0.84    1.48    0.39  -38.62  -13.25   -8.11    5.57   11.18
   -3.67  -12.11    3.47] | cost: 73.23
theta: [-144.82    0.85    1.47    0.46  -37.36  -12.53   -7.56    5.47    9.93
   -4.57  -12.23    3.56] | cost: 72.28
theta: [-155.48    0.86    1.48    0.54  -34.88  -11.3    -6.98    5.95    8.38
   -5.92  -12.27    3.13] | cost: 70.91
theta: [-167.86    0.88    1.52    0.62  -31.01   -9.63   -6.53    7.03    6.9
   -7.3   -12.29    1.91] | cost: 69.33
theta: [-176.09    0.89    1.57    0.64  -27.32   -8.32   -6.41    8.07    6.31
   -7.84  -12.29    0.44] | cost: 68.19
theta: [-178.63    0.9     1.6     0.62  -25.15   -7.88   -6.5     8.52    6.6
   -7.51  -12.19   -0.39] | cost: 67.59
theta: [-179.83    0.91    1.63    0.6   -23.4    -7.84   -6.6     8.61    7.27
   -6.83  -11.89   -0.72] | cost: 67.08
theta: [-182.79    0.91    1.66    0.58  -20.55   -8.01   -6.68    8.49    8.44
   -5.7   -11.11   -0.69] | cost: 66.27
theta: [-190.23    0.93    1.68    0.6   -15.62   -8.38   -6.68    8.1    10.26
   -4.1    -9.46    0.01] | cost: 65.11
theta: [-199.13    0.93    1.69    0.67  -11.37   -8.7    -6.55    7.67   11.53
   -3.17   -7.81    1.13] | cost: 64.28
theta: [-203.85    0.93    1.68    0.72  -10.03   -8.78   -6.42    7.5    11.68
   -3.25   -7.13    1.86] | cost: 64.01
theta: [-204.24    0.93    1.67    0.74  -10.33   -8.74   -6.39    7.52   11.46
   -3.52   -7.17    1.97] | cost: 63.98
theta: [-204.06    0.93    1.67    0.74  -10.48   -8.72   -6.39    7.54   11.39
   -3.59   -7.22    1.95] | cost: 63.98
theta: [-204.03    0.93    1.67    0.74  -10.5    -8.72   -6.39    7.54   11.39
   -3.6    -7.22    1.95] | cost: 63.98
theta: [-204.03    0.93    1.67    0.74  -10.5    -8.72   -6.39    7.54   11.39
   -3.6    -7.22    1.95] | cost: 63.98
theta: [-204.03    0.93    1.67    0.74  -10.5    -8.72   -6.39    7.54   11.39
   -3.6    -7.22    1.95] | cost: 63.98
theta: [-204.03    0.93    1.67    0.74  -10.5    -8.72   -6.39    7.54   11.39
   -3.6    -7.22    1.95] | cost: 63.98
theta: [-204.03    0.93    1.67    0.74  -10.5    -8.72   -6.39    7.54   11.39
   -3.6    -7.22    1.95] | cost: 63.98
theta: [-204.03    0.93    1.67    0.74  -10.5    -8.72   -6.39    7.54   11.39
   -3.6    -7.22    1.95] | cost: 63.98
```

ونموذجنا يبدو كالتالي:

$$ y = -204.03 + 0.93x_1 + ... -7.22x_{9} + 1.95x_{11} $$

لنقارن هذه المعادله التي حصلنا عليها مع أخرى قد نحصل عليها من نموذج الإنحدار الخطي في مكتبة `sklearn`:

```python
model = LinearRegression(fit_intercept=False)
model.fit(X_train[:, :14], y_train)
print("Coefficients", model.coef_)
```

```ruby
Coefficients [-204.03    0.93    1.67    0.74  -10.5    -8.72   -6.39    7.54   11.39
   -3.6    -7.22    1.95]
```

يظهر المعامل مطابقاً لما حصلنا عليه! دوالنا التي كتبناها انتجت نفس النموذج  كما لو انتجته مكتبة بايثون!

تمكنا بنجاح من ضبط النموذج الخطي!

### تقييم نموذجنا

خطوتنا التاليه هي تقييم نتائج النموذج بإستخدام بيانات الإختبار. نحتاج لتطبيق جميع ما فعلناه على بيانات التدريب في بيانات الإختبار قبل ان ندخلها إلى النموذج للتوقع: 

```python
X_test.drop(['Sex', 'WeightAlt'], axis=1, inplace=True)
X_test = pd.get_dummies(X_test, columns=['BCS', 'Age'])
age_over_10 = X_test['Age_10-15'] | X_test['Age_15-20'] | X_test['Age_>20']
X_test['Age_>10'] = age_over_10
X_test.drop(['Age_10-15', 'Age_15-20', 'Age_>20'], axis=1, inplace=True)
X_test.drop(['BCS_3.0', 'Age_5-10'], axis=1, inplace=True)
X_test = X_test.assign(bias=1)
X_test = X_test.reindex(columns=['bias'] + list(X_test.columns[:-1]))

X_test
```

**Age\_>10**|**Age\_<2**|**Age\_2-5**|**BCS\_4.0**|**BCS\_3.5**|**BCS\_2.5**|**BCS\_2.0**|**BCS\_1.5**|**Height**|**Girth**|**Length**|**bias**| 
:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:
1|0|0|0|0|1|0|0|103|119|98|1|490
0|0|1|0|0|0|0|0|105|114|86|1|75
1|0|0|0|0|0|0|0|101|114|94|1|352
...|...|...|...|...|...|...|...|...|...|...|...|...
0|0|1|0|0|0|0|0|102|114|94|1|182
0|0|0|0|0|1|0|0|105|113|104|1|334
0|0|0|0|0|0|0|0|110|124|104|1|543

```ruby
108 rows × 12 columns
```

نقوم بإدخال `X_test` إلى `predict` في نموذجنا الخطي `LinearRegression`:

```python
X_test = X_test.values
predictions = model.predict(X_test)
```

الآن لنطلع على نتيجة الخطأ التربيعي المتوسط:

```python
mse_test_set(predictions)
```

```ruby
7261.974205350604
```

بهذه التوقعات، يمكننا رسم نتيجة الفارق بين التوقع مقارنه بالنتائج الحقيقه Residual plot:

```python
y_test = y_test.values
resid = y_test - predictions
resid_prop = resid / y_test
plt.scatter(np.arange(len(resid_prop)), resid_prop, s=15)
plt.axhline(0)
plt.title('Residual proportions (resid / actual Weight)')
plt.xlabel('Index of row in data')
plt.ylabel('Error proportion');
```

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter13/linear_case_study_91_0.png'>
</p>

يبدو ان النموذج يؤدي بشكل رائع! نتيجة الفارق بين التوقع والنتائج الحقيقه تظهر ان توقعنا كان بحد أعلى 15% من النتيجة الحقيقه.

[LinearLeastSquares]: https://towardsdatascience.com/linear-regression-using-least-squares-a4c3456e8570
[Scalar]: https://www.youtube.com/watch?v=rcDXQ-5H8mk
[Vector]: https://www.mathsisfun.com/algebra/scalar-vector-matrix.html