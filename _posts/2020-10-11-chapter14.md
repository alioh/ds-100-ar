---
title: هندسة الخصائص
show_title: true
chapter_number: 14
chapter_text: الفصل الرابع عشر
chapter_lessons: [[0, 'مقدمة'], [1, 'بيانات وول مارت'], [2, 'التنبؤ بتقييم الآيس كريم']]
chapter_sublessons: [
    [],
    ['ضبط النموذج بإستخدام مكتبة Scikit-learn', 'استخدام One-Hot Encoding', 'طريقة One-Hot Encoding في مكتبة Scikit-learn', 'ضبط النموذج بإستخدام البيانات المُعدله', 'تقييم النموذج', 'ملخص بيانات وول مارت'],
    ['خصائص متعددة الحدود', 'الإنحدار متعدد الحدود', 'زيادة الدرجة', 'ملخص التنبؤ بتقييم الآيس كريم'],
]
layout: default
---

## مقدمة

**هندسة الخصائص Feature Engineering** تعني إنشاء وإضافة خصائص جديده للبيانات لزيادة دقة وتعقيد النموذج. 

حتى الآن، قمنا فقط بتطبيق الإنحدار الخطي بإستخدام خصائص كميه كمُدخلات، استخدمنا القيمه الكميه لمجموع الفاتوره للتنبؤ بمبلغ الإكراميه. ولكن، احتوت بيانات الإكراميات على بيانات اسمية، مثل ايام الأسبوع ونوع الوجبه. هندسة الخصائص تسمح لنا بتحويل البيانات الأسميه إلى كميه لإستخدامها في الإنحدار الخطي.

تسمح لنا هندسة الخصائص ايضاً بإستخدام نموذج الإنحدار الخطي لإجراء انحدار متعدد الحدود عن طريق إنشاء متغيرات جديده في بياناتنا.

## بيانات وول مارت

في عام 2014، قامت ول مارت بنشر بعض بيانات بيعها كجزء من مسابقه للتنبؤ بالمبيعات الأسبوعيه في فروعها. أخذنا جزء من هذه البيانات وإستخدمناها في المثال التالي:

```python
walmart = pd.read_csv('walmart.csv')
walmart
```

**MarkDown**|**Unemployment**|**Fuel\_Price**|**Temperature**|**IsHoliday**|**Weekly\_Sales**|**Date**| 
:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:
No Markdown|8.106|2.572|42.31|No|24924.5|05-02-10|0
No Markdown|8.106|2.548|38.51|Yes|46039.49|12-02-10|1
No Markdown|8.106|2.514|39.93|No|41595.55|19-02-10|2
...|...|...|...|...|...|...|...
MarkDown2|6.573|3.601|62.99|No|22764.01|12-10-12|140
MarkDown2|6.573|3.594|67.97|No|24185.27|19-10-12|141
MarkDown1|6.573|3.506|69.16|No|27390.81|26-10-12|142


```ruby
143 rows × 7 columns
```

> وول مارت Walmart هي أحدى أكبر متاجر التجزئة والمنتجات اليوميه في الولايات المتحدة الأمريكية.  
> لتحميل البيانات walmart.csv [اضغط هنا]({{ site.baseurl }}/files/chapter14/walmart.csv).


تحتوي البيانات على عدد من الخصائص المثيره للإعجاب، إضافة إلى معلومات ما إذا كان الأسبوع يحتوي على إجازة في العامود `IsHoliday`، ومعدل البطاله في الأسبوع `Unemployment` وأي من العروض الخاصه التي قدمها المتجر في ذلك الأسبوع `MarkDown`.

الهدف هو بناء نموذج يتوقع المتغير `Weekly_Sales` بإستخدام باقي المعلومات في جدول البيانات. بإستخدام نموذج الإنحدار الخطي يمكننا بشكل مباشر إستخدام الأعمدة `Temperature`، `Fuel_Price` و `Unemployment` كونها تحتوي على بيانات كمية.

### ضبط النموذج بإستخدام مكتبة Scikit-learn

في الفصول السابقه تعلمنا كيف نوجد مشتقة دالة التكلفه وإستخدام النزول الإشتقاقي لضبط النموذج. لفعل ذلك، كان علينا تعريف دوال في بايثون لبناء النموذج، دالة التكلفة، مشتقة دالة التكلفة، وخوارزمية النزول الإشتقاقي. رغم أهمية ذلك لعرض وفهم المفاهيم في بناء النماذج، في هذا الفصل سنستخدم مكتبة متخصصه بتعلم الآله أسمها `scikit-learn` والتي تسمح لنا بضبط النماذج بإستخدام أكواد برمجية أقل.

مثلاً، لضبط نموذج إنحدار خطي متعدد بإستخدام البيانات الكمية في قاعدة بيانات وول مارت، نقوم أولاً ببناء مصفوفة ثنائية الأبعاد بإستخدام NumPy تحتوي على المتغيرات المستخدمه لبناء نموذج التنبؤ و مصفوفة أحادية الأبعاد تحتوي على القيم التي نريد التنبؤ عنها:

```python
numerical_columns = ['Temperature', 'Fuel_Price', 'Unemployment']
X = walmart[numerical_columns].to_numpy()
X
```

```ruby
array([[ 42.31,   2.57,   8.11],
       [ 38.51,   2.55,   8.11],
       [ 39.93,   2.51,   8.11],
       ..., 
       [ 62.99,   3.6 ,   6.57],
       [ 67.97,   3.59,   6.57],
       [ 69.16,   3.51,   6.57]])
```

```python
y = walmart['Weekly_Sales'].to_numpy()
y
```

```ruby
array([ 24924.5 ,  46039.49,  41595.55, ...,  22764.01,  24185.27,
        27390.81])
```

ثم نستدعي [كلاس](/ds-100-ar/translation-reference/) الإنحدار الخطي `LinearRegression` من مكتبة `scikit-learn` ([مزيد من التفاصيل اضغط هنا](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression))، نعرّف بالنموذج، ثم نستدعي دالة الضبط `fit` بإستخدام المتغيرات `X` للتنبؤ بقيمة `y`.

لاحظ أننا في السابق أحتجنا لإضافة عامود جديد يدوياً يحتوي على القيمة $ 1 $ للمصفوفه `X` "التحيز" لنقوم بتطبيق الإنحدار الخطي. هذه المره، `scikit-learn` ستتكفل بفعل ذلك لتختصر علينا الوقت:

```python
from sklearn.linear_model import LinearRegression

simple_classifier = LinearRegression()
simple_classifier.fit(X, y)
```

```ruby
LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)
```

رائع! عندما نستدعي `.fit`، مكتبة `scikit-learn` أوجدت المتغيرات التي تقلل من دالة تكلفة المربعات الصغرى. يمكننا الإطلاع على المتغيرات كالتالي:

```python
simple_classifier.coef_, simple_classifier.intercept_
```

```ruby
(array([ -332.22,  1626.63,  1356.87]), 29642.700510138635)
```

لحساب تكلفة المتوسط التربيعي، يمكننا أن نطلب من المصنف Classifier التنبؤ للقيم المدخله في `X` ومقارنة النتيجة مع البيانات الحقيقه في `y`:

```python
predictions = simple_classifier.predict(X)
np.mean((predictions - y) ** 2)
```

```ruby
74401210.603607252
```

يظهر أن الخطأ التربيعي المتوسط يبدو مرتفعاً جداً. على الأرجح يكون سبب ذلك هو المتغيرات الثلاثه الكميه التي استخدمناها وأرتباطها الضعيف بالمبيعات الأسبوعيه.

لدينا متغيران آخران من الممكن أن يكون لهما فائده في التنبؤ: عامود `IsHoliday` و `MarkDown`. مخطط الصندوق في الأسفل يوضح أن الإجازات قد يكون لها أرتباط مع المبياعات الأسبوعيه:

```python
sns.pointplot(x='IsHoliday', y='Weekly_Sales', data=walmart);
```

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter14/feature_one_hot_14_0.png'>
</p>

يظهر ان هناك رابط بين الأنواع المختلفه للعروض في العامود `MarkDown` وبين كمية المبيعات في الأسابيع المختلفه:

```python
markdowns = ['No Markdown', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']
plt.figure(figsize=(7, 5))
sns.pointplot(x='Weekly_Sales', y='MarkDown', data=walmart, order=markdowns);
```

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter14/feature_one_hot_16_0.png'>
</p>

ولكن، كلا الأعمدة `IsHoliday` و `MarkDown` هي بيانات أسميه، وليست كميه، لذا لا يمكن أن نستخدمها كما هي الآن في الأنحدار.

### استخدام One-Hot Encoding

لحسن الحظ، يمكننا أن نستخدم **One-Hot Encoding** على هذه البيانات الأسمية لتحويلها لكمية. التحويل يتم كالتالي: ننشأ عامود جديد لكل قيمه مختلفه في العامود الأسمي. يحتوي كل عامود على الرقم $ 1 $ إذا كانت العامود هو تلك القيمه الموجوده في العامود الأسمي، وغير ذاك تكون القيمه $ 0 $، مثلاً في العامود `MarkDown`:

```python
walmart[['MarkDown']]
```

**MarkDown**| 
:-----:|:-----:
No Markdown|0
No Markdown|1
No Markdown|2
...|...
MarkDown2|140
MarkDown2|141
MarkDown1|142

```ruby
143 rows × 1 columns
```

يحتوي العامود على ست قيم مختلفه وهي:  No Markdown، MarkDown1، MarkDown2، MarkDown3، MarkDown4، و MarkDown5. نقوم بإنشاء عامود لكل قيمه وينتج لنا ست أعمدة جديده. ثم نقوم بتعبئة كل عامود بصفر أو واحد بنفس الطريقة التي شرحناها مسبقاً.

```python
from sklearn.feature_extraction import DictVectorizer

items = walmart[['MarkDown']].to_dict(orient='records')
encoder = DictVectorizer(sparse=False)
pd.DataFrame(
    data=encoder.fit_transform(items),
    columns=encoder.feature_names_
)
```

**MarkDown=No Markdown**|**MarkDown=MarkDown5**|**MarkDown=MarkDown4**|**MarkDown=MarkDown3**|**MarkDown=MarkDown2**|**MarkDown=MarkDown1**| 
:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:
1|0|0|0|0|0|0
1|0|0|0|0|0|1
1|0|0|0|0|0|2
...|...|...|...|...|...|...
0|0|0|0|1|0|140
0|0|0|0|1|0|141
0|0|0|0|0|1|142

```ruby
143 rows × 6 columns
```

لاحظ أن أول سطر في البيانات هي No Markdown، لذا فقط آخر عامود في الجدول الذي تم أنشاءه تحتوي على الرقم $ 1 $. وأيضاً، آخر سطر في البيانات يحتوي على MarkDown1 مما جعل أو عامود يحتوي على $ 1 $.

كل سطر في الجدول يحتوي على عامود واحد لدية الرقم $ 1 $، والبقيه ستحتوي على $ 0 $. الأسم "One-Hot" يعني أن عامود واحد سيكون ذو قيمه (يحتوي على 1).


> مثال آخر تم ذكره في الفصل السابق:
>
> <p align="center"><a href="https://alioh.github.io/DSND-Notes-2/"><img src='{{ site.baseurl }}/img/chapter13/one-hot-encoding.jpeg'></a>
> </p> 
>

### طريقة One-Hot Encoding في مكتبة Scikit-learn

لنقوم بعملية One-Hot Encoding يمكننا إستخدام الكلاس [DictVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html) من مكتبة `scikit-learn`. لنستخدم هذه الكلاس، نحتاج لتحويل ال DataFrame إلى مصفوفه تحتوى على قواميس. الكلاس DictVectorizer يقوم بشكل تلقائي بعملية One-Hot Encoding للأعمدة الأسميه (والتي يجب أن تكون نصوص) ولا يقوم بتغير أي عامود يحتوي على قيم كميه: 

```python
from sklearn.feature_extraction import DictVectorizer

all_columns = ['Temperature', 'Fuel_Price', 'Unemployment', 'IsHoliday',
               'MarkDown']

records = walmart[all_columns].to_dict(orient='records')
encoder = DictVectorizer(sparse=False)
encoded_X = encoder.fit_transform(records)
encoded_X
```

```ruby
array([[  2.57,   1.  ,   0.  , ...,   1.  ,  42.31,   8.11],
       [  2.55,   0.  ,   1.  , ...,   1.  ,  38.51,   8.11],
       [  2.51,   1.  ,   0.  , ...,   1.  ,  39.93,   8.11],
       ..., 
       [  3.6 ,   1.  ,   0.  , ...,   0.  ,  62.99,   6.57],
       [  3.59,   1.  ,   0.  , ...,   0.  ,  67.97,   6.57],
       [  3.51,   1.  ,   0.  , ...,   0.  ,  69.16,   6.57]])
```

لتتبين لك الفكره والشكل الجديد للبيانات، يمكننا عرضها مع أسماء الأعمدة:

```python
pd.DataFrame(data=encoded_X, columns=encoder.feature_names_)
```

**Unemployment**|**Temperature**|**MarkDown=No Markdown**|**MarkDown=MarkDown5**|**...**|**MarkDown=MarkDown1**|**IsHoliday=Yes**|**IsHoliday=No**|**Fuel\_Price**| 
:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:
8.106|42.31|1|0|...|0|0|1|2.572|0
8.106|38.51|1|0|...|0|1|0|2.548|1
8.106|39.93|1|0|...|0|0|1|2.514|2
...|...|...|...|...|...|...|...|...|...
6.573|62.99|0|0|...|0|0|1|3.601|140
6.573|67.97|0|0|...|0|0|1|3.594|141
6.573|69.16|0|0|...|1|0|1|3.506|142

```ruby
143 rows × 11 columns
```

الأعمدة الكمية `Fuel price`، `Temperature` و `Unemployment` بقيت كما هي كأرقام. الأعمدة الأسميه `IsHoliday` و `MarkDown` تم تطبيق ال One-Hot Encoding عليها. عندما نستخدم المصفوفة الجديده للبيانات لضبط نموذج الإنحدار الخطي، سنقوم بإيجاد متغير جديد لكل عامود في البيانات. بما أن المصفوفه تحتوي على 11 أعمدة، النموذج سيتنبأ ب 12 متغير بما أننا اضفنا متغير إضافي للتحيز.

### ضبط النموذج بإستخدام البيانات المُعدله

يمكننا الآن إستخدام `encoded_X` في نموذج الإنحدار الخطي:

```python
clf = LinearRegression()
clf.fit(encoded_X, y)
```

```ruby
LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)
```

كما ذكرنا مُسبقاً، ستكون النتيجة مكونه من 12 قيمه لكل عامود:

```python
clf.coef_, clf.intercept_
```

```ruby
(array([ 1622.11,    -2.04,     2.04,   962.91,  1805.06, -1748.48,
        -2336.8 ,   215.06,  1102.25,  -330.91,  1205.56]), 29723.135729284979)
```

يمكننا مقارنة نتائج التوقع بين كلا النماذج لنرى إذا كان هناك فرق كبير بينهما:

```python
walmart[['Weekly_Sales']].assign(
    pred_numeric=simple_classifier.predict(X),
    pred_both=clf.predict(encoded_X)
)
```

**pred\_both**|**pred\_numeric**|**Weekly\_Sales**| 
:-----:|:-----:|:-----:|:-----:
30766.79021|30768.87804|24924.5|0
31989.4104|31992.2795|46039.49|1
31460.28001|31465.22016|41595.55|2
...|...|...|...
24447.34898|23492.26265|22764.01|140
22788.04955|21826.41479|24185.27|141
21409.36746|21287.92854|27390.81|142

```ruby
143 rows × 3 columns
```

> في الجدول السابق، العامود `pred_numeric` يمثل نتائج النموذج `simple_classifier` والذي استخدم فقط الأعمدة الكميه في بياناتنا. بينما `pred_both` استخدم كامل الأعمدة.

نلاحظ ان نتائج التوقع قريبه بين النموذجان. يمكن رسم مخطط التشتت لكلا الأعمدة لتوضيح ذلك:

```python
plt.scatter(simple_classifier.predict(X), clf.predict(encoded_X))
plt.title('Predictions using all data vs. numerical features only')
plt.xlabel('Predictions using numerical features')
plt.ylabel('Predictions using all features');
```

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter14/feature_one_hot_35_0.png'>
</p>

### تقييم النموذج

لماذا ظهرت لنا النتائج هكذا؟ يمكننا عرض المتغيرات التي اعتمدها النموذجان. الجدول في الأسفل يوضح [الأوزان](/ds-100-ar/translation-reference/) المناسبه التي تعلم عليها المُصنف بإستخدام الأعمدة الكمية فقط:

```python
def clf_params(names, clf):
    weights = (
        np.append(clf.coef_, clf.intercept_)
    )
    return pd.DataFrame(weights, names + ['Intercept'])

clf_params(numerical_columns, simple_classifier)
```

**0**| 
:-----:|:-----:
-332.22118|Temperature
1626.625604|Fuel\_Price
1356.868319|Unemployment
29642.70051|Intercept

الجدول في الأسفل يوضح الأوزان المناسبه التي تعلم عليها المُصنف بعد تطبيق One-Hot Encoding على البيانات:

```python
pd.options.display.max_rows = 13
display(clf_params(encoder.feature_names_, clf))
pd.options.display.max_rows = 7
```

**0**| 
:-----:|:-----:
1622.106239|Fuel\_Price
-2.041451|IsHoliday=No
2.041451|IsHoliday=Yes
962.908849|MarkDown=MarkDown1
1805.059613|MarkDown=MarkDown2
-1748.475046|MarkDown=MarkDown3
-2336.799791|MarkDown=MarkDown4
215.060616|MarkDown=MarkDown5
1102.24576|MarkDown=No Markdown
-330.912587|Temperature
1205.564331|Unemployment
29723.13573|Intercept

> في الكود البرمجي السابق، عرف الكاتب دالة `clf_params` والتي تستقبل اسم الأعمدة و النموذج، وتنتج لنا DataFrame بقيم الأوزان لكل عامود. العامود `Intercept` هنا هو [الإنحياز](/ds-100-ar/translation-reference/).

نلاحظ أنه حتى بعد ضبط نموذج الإنحدار الخطي بإستخدام One-Hot Encoding الأوزان للأعمدة `Fuel price`، `Temperature` و `Unemployment` مشابهه بشكل كبير للبيانات قبل التعديل. جميع قيم الأوزان قليله مقارنة بالإنحياز `Intercept`، يشير ذلك إلى أن أكثر المتغيرات لا يزال ارتباطها قليل مع القيم الحقيقه للمبيعات. في الحقيقه، وزن العامود `IsHoliday` في النموذج قليل جداً مما يجعله لا يشكل فرقاً في التنبؤ. على الرغم أن بعض الأوزان في العامود `MarkDown` تبدو مرتفعه جداً، الكثير منها لم يظهر إلا قليلاً:

```python
walmart['MarkDown'].value_counts()
```

```ruby
No Markdown    92
MarkDown1      25
MarkDown2      13
MarkDown5       9
MarkDown4       2
MarkDown3       2
Name: MarkDown, dtype: int64
```

يشير ذلك أننا نحتاج لجمع المزيد من البيانات ليكون للعامود `MarkDown` تأثير على قيمة المبيعات. ( في الحقيقه، البيانات هنا هي جزء صغير من [البيانات الحقيقه الكاملة](https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting) والتي نشرتها وول مارت. سيكون تدريباً مناسباً لو قمت بتجربة ضبط النموذج وتدريبه على كامل البيانات بدلاً من جزء بسيط منها).

###  ملخص بيانات وول مارت

تعلمنا كيف نستخدم One-Hot Encoding، طريقة مناسبه لتطبيق الإنحدار الخطي على البيانات الإسميه. على الرغم أن في هذا المثال تطبيق ذلك لم يأثر كثيراً على النموذج، عملياً تستخدم هذه الطريقه بشكل كبير للتعامل مع البيانات الإسميه. One-Hot Encoding تعتبر أحد المبادئ العامه في هندسة الخصائص، تأخذ مصفوفه/عامود من البيانات وتحولها إلى مصفوفات/أعمدة ذات أهميه أكبر.

## التنبؤ بتقييم الآيس كريم

لنفترض أننا نريد صناعة نكهة جديده وتكسب شهره من الآيس كريم. نحن مهتمون بحل مشكلة الإنحدار التالية: بناءًا على نسبة حلاوة نكهة الآيس كريم، نريد التنبؤ عن التقييم العام للآيس كريم من 7.

```python
ice = pd.read_csv('icecream.csv')
ice
```

> لتحميل البيانات icecream.csv [اضغط هنا]({{ site.baseurl }}/files/chapter14/icecream.csv).

**overall**|**sweetness**| 
:-----:|:-----:|:-----:
3.9|4.1|0
5.4|6.9|1
5.8|8.3|2
...|...|...
5.9|11|6
5.5|11.7|7
5.4|11.9|8

```ruby
9 rows × 2 columns
```

على الرغم من توقعنا أن نكهات الآيس كريم التي لا تعتبر ذات حلاوة عاليه ستحصل على تقييم أقل، نتوقع أيضاً ان النكهات ذات الحلاوة العالية ستحصل أيضاً على تقييم أقل. يتبين لنا ذلك في رسمة مخطط التشتت للتقييم مقارنة بحلاوة الآيس كريم:

```python
sns.lmplot(x='sweetness', y='overall', data=ice, fit_reg=False)
plt.title('Overall taste rating vs. sweetness');
```

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter14/feature_polynomial_6_0.png'>
</p>

للأسف، لا يمكن للنموذج الخطي وحدة من التعامل مع الزيادة والنقصان الواضحه في الرسم البياني؛ في النموذج الخطي، التقييم يزداد أو ينقص مع حلاوة الآيس كريم. نرى أن أستخدام النموذج الخطي يظهر لنا نتائج سيئة.

```python
sns.lmplot(x='sweetness', y='overall', data=ice)
plt.title('Overall taste rating vs. sweetness');
```

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter14/feature_polynomial_8_0.png'>
</p>

طريقة مفيدة لحل مثل هذه المشاكل هي بإستخدام المنحينات متعددة الحدود بدلاً من الخط. مثل هذه المنحنيات تساعدنا على بناء نموذج يتعامل مع الزيادة في التقييم حتى الوصول لنقطة معينة من الحلاوة، ثم النقص بالتقييم مع الزيادة في الحلاوة.

بإستخدام تقنيات معينة في هندسة الخصائص، يمكننا أن ننشأ أعمدة جديده في البيانات لتساعدنا على إستخدام النموذج الخطي للإنحدار متعدد الخطوط.

### خصائص متعددة الحدود

لنتذكر أن في الإنحدار الخطي نقوم بضبط [وزن](/ds-100-ar/translation-reference/) واحد لكل عامود في مصفوفتنا $ X $. في هذه الحالة، في مصفوفتنا $ X $ سيكون لدينا عامودان: عامود يحتوي على الرقم 1 و آخر يحتوي على مستوى حلاوة الآيس كريم:

```python
from sklearn.preprocessing import PolynomialFeatures

first_X = PolynomialFeatures(degree=1).fit_transform(ice[['sweetness']])
pd.DataFrame(data=first_X, columns=['bias', 'sweetness'])
```

**sweetness**|**bias**| 
:-----:|:-----:|:-----:
4.1|1.0|0
6.9|1.0|1
8.3|1.0|2
...|...|...
11|1.0|6
11.7|1.0|7
11.9|1.0|8

```ruby
9 rows × 2 columns
```

بذلك يمكننا تعريف النموذج كالتالي:

$$ f_\hat{\theta} (x) = \hat{\theta_0} + \hat{\theta_1} \cdot \text{sweetness} $$

يمكننا إنشاء عامود جديد في $ X $ يحتوي على تربيع العامود `sweetness`:

```python
second_X = PolynomialFeatures(degree=2).fit_transform(ice[['sweetness']])
pd.DataFrame(data=second_X, columns=['bias', 'sweetness', 'sweetness^2'])
```

**sweetness^2**|**sweetness**|**bias**| 
:-----:|:-----:|:-----:|:-----:
16.81|4.1|1.0|0
47.61|6.9|1.0|1
68.89|8.3|1.0|2
...|...|...|...
121|11|1.0|6
136.89|11.7|1.0|7
141.61|11.9|1.0|8

```ruby
9 rows × 3 columns
```

بما أن النموذج سيتعلم الحصو لعلى وزن واحد لكل عامود في المصفوفه المعطاه له، سيكون نموذجنا كالتالي:

$$ f_\hat{\theta} (x) = \hat{\theta_0}
    + \hat{\theta_1} \cdot \text{sweetness}
    + \hat{\theta_2} \cdot \text{sweetness}^2 $$

يمكن لنموذجنا الآن أن يضبط متعددة الخطوط من الدرجة الثانية. يمكننا أن نضبط درجات أعلى من متعددة الخطوط بإضافة أعمدة جديده لحلاوة الآيس كريم $ \text{sweetness}^3 $ ، $ \text{sweetness}^4 $ وهكذا.

لاحظ أن هذا النموذج لا يزال نموذج خطي، **لأن  [مُعلماته](/ds-100-ar/translation-reference/) خطية**، كل $ \hat{\theta_i} $ هي قيمة عددية من الدرجة الأولى. ولكن، النموذج **متعدد الحدود بخصائصة**  لأن البيانات تحتوي على عامود  متعدد الحدود تم إيجادة من عامود آخر.

### الإنحدار متعدد الحدود

لتطبيق الإنحدار متعدد الحدود، نستخدم نموذج خطي مع خصائص متعددة الحدود. لذا، نقوم بإستدعاء النموذج [`LinearRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression) و [`PolynomialFeatures`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html#sklearn.preprocessing.PolynomialFeatures) من مكتبة `scikit-learn`.

```python
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
```

تحتوي مصفوفتنا الأصليه $ X $ على القيم التاليه. تذكر أن البيانات الأصلية لا تحتوي على الرقم التسلسلي ومسمى العواميد، المصفوفة الأصلية $ X $ تحتوي على الأرقام فقط:

```python
ice[['sweetness']]
```

**sweetness**| 
:-----:|:-----:
4.1|0
6.9|1
8.3|2
...|...
11|6
11.7|7
11.9|8

```ruby
9 rows × 1 columns
```

نستخدم أولاً الكلاس `PolynomialFeatures` لتحويل البيانات، نضيف الخصائص متعددة الحدود من الدرجة الثانية:

```python
transformer = PolynomialFeatures(degree=2)
X = transformer.fit_transform(ice[['sweetness']])
X
```

```ruby
array([[  1.  ,   4.1 ,  16.81],
       [  1.  ,   6.9 ,  47.61],
       [  1.  ,   8.3 ,  68.89],
       ...,
       [  1.  ,  11.  , 121.  ],
       [  1.  ,  11.7 , 136.89],
       [  1.  ,  11.9 , 141.61]])
```

الآن، نقوم بضبط  النموذج الخطي على بيانات هذه المصفوفة:

```python
clf = LinearRegression(fit_intercept=False)
clf.fit(X, ice['overall'])
clf.coef_
```

```ruby
array([-1.3 ,  1.6 , -0.09])
```

المتغيرات السابقه تظهر أن لهذه البيانات، أفضل قيمة لضبط النموذج هي:

$$ f_\hat{\theta} (x) = -1.3 + 1.6 \cdot \text{sweetness} - 0.09 \cdot \text{sweetness}^2 $$

الآن، يمكننا مقارنة توقعات هذا النموذج مع البيانات الأصليه:

```python
sns.lmplot(x='sweetness', y='overall', data=ice, fit_reg=False)
xs = np.linspace(3.5, 12.5, 1000).reshape(-1, 1)
ys = clf.predict(transformer.transform(xs))
plt.plot(xs, ys)
plt.title('Degree 2 polynomial fit');
```

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter14/feature_polynomial_26_0.png'>
</p>

هذا النموذج يظهر أنه أفضل بكثير من النموذج الخطي، يمكننا التأكد أيضاً من أن تكلفة المتوسط التربيعي لمتعددة الحدود من الدرجة الثانية أقل بكثير من من التكلفة للخطي:

```python
y = ice['overall']
pred_linear = (
    LinearRegression(fit_intercept=False).fit(first_X, y).predict(first_X)
)
pred_quad = clf.predict(X)

# دالة لحاسب المتوسط
def mse_cost(pred, y):
    return np.mean((pred - y) ** 2)

print(f'MSE cost for linear reg:     {mse_cost(pred_linear, y):.3f}')
print(f'MSE cost for deg 2 poly reg: {mse_cost(pred_quad, y):.3f}')
```

```ruby
MSE cost for linear reg:     0.323
MSE cost for deg 2 poly reg: 0.032
```

### زيادة الدرجة

كما ذكرنا سابقاً، يمكننا زيادة درجة متعددة الحدود بإضافة المزيد من الخصائص للبيانات. مثلاً، يمكننا بسهولة انشاء خصائص من الدرجة الخامسه لمتعددة الحدود وستكون كالتالي:

```python
second_X = PolynomialFeatures(degree=5).fit_transform(ice[['sweetness']])
pd.DataFrame(data=second_X,
             columns=['bias', 'sweetness', 'sweetness^2', 'sweetness^3',
                      'sweetness^4', 'sweetness^5'])
```

**sweetness^5**|**sweetness^4**|**sweetness^3**|**sweetness^2**|**sweetness**|**bias**| 
:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:
1158.56201|282.5761|68.921|16.81|4.1|1|0
15640.31349|2266.7121|328.509|47.61|6.9|1|1
39390.40643|4745.8321|571.787|68.89|8.3|1|2
...|...|...|...|...|...|...
161051|14641|1331|121|11|1|6
219244.8036|18738.8721|1601.613|136.89|11.7|1|7
238635.366|20053.3921|1685.159|141.61|11.9|1|8

```ruby
9 rows × 6 columns
```

ضبط النموذج الخطي بإستخدام هذه الخصائص سينتج لنا الإنحدار متعدد الحدود من الدرجة الخامسه:

```python
trans_five = PolynomialFeatures(degree=5)
X_five = trans_five.fit_transform(ice[['sweetness']])
clf_five = LinearRegression(fit_intercept=False).fit(X_five, y)

sns.lmplot(x='sweetness', y='overall', data=ice, fit_reg=False)
xs = np.linspace(3.5, 12.5, 1000).reshape(-1, 1)
ys = clf_five.predict(trans_five.transform(xs))
plt.plot(xs, ys)
plt.title('Degree 5 polynomial fit');
```

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter14/feature_polynomial_32_0.png'>
</p>

الرسم البياني يظهر ان متعددة الحدود من الدرجة الخامسه نتائجها مشابهه بشكل كبير لمتعددة الحدود من الدرجة الثانية. في الحقيقه، تكلفة المتوسط التربيعي لمتعددة الحدود ذات الدرجة الخامسه بلغت نصف تكلفة المتوسط التربيعي لمتعددة الحدود من الدرجة الثانية:

```python
pred_five = clf_five.predict(X_five)

print(f'MSE cost for linear reg:     {mse_cost(pred_linear, y):.3f}')
print(f'MSE cost for deg 2 poly reg: {mse_cost(pred_quad, y):.3f}')
print(f'MSE cost for deg 5 poly reg: {mse_cost(pred_five, y):.3f}')
```

```ruby
MSE cost for linear reg:     0.323
MSE cost for deg 2 poly reg: 0.032
MSE cost for deg 5 poly reg: 0.017
```

يعني ذلك أننا قد نحصل على نتائج أفضل لو قمنا بزايدة الدرجة. لماذا لا نجرب متعددة الحدود من الدرجة العاشره؟

```python
trans_ten = PolynomialFeatures(degree=10)
X_ten = trans_ten.fit_transform(ice[['sweetness']])
clf_ten = LinearRegression(fit_intercept=False).fit(X_ten, y)

sns.lmplot(x='sweetness', y='overall', data=ice, fit_reg=False)
xs = np.linspace(3.5, 12.5, 1000).reshape(-1, 1)
ys = clf_ten.predict(trans_ten.transform(xs))
plt.plot(xs, ys)
plt.title('Degree 10 polynomial fit')
plt.ylim(3, 7);
```

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter14/feature_polynomial_36_0.png'>
</p>

نتائج تكلفة المتوسط التربيعي لجميع نماذج الإنحدار التي عرضناها:

```python
pred_ten = clf_ten.predict(X_ten)

print(f'MSE cost for linear reg:      {mse_cost(pred_linear, y):.3f}')
print(f'MSE cost for deg 2 poly reg:  {mse_cost(pred_quad, y):.3f}')
print(f'MSE cost for deg 5 poly reg:  {mse_cost(pred_five, y):.3f}')
print(f'MSE cost for deg 10 poly reg: {mse_cost(pred_ten, y):.3f}')
```

```ruby
MSE cost for linear reg:      0.323
MSE cost for deg 2 poly reg:  0.032
MSE cost for deg 5 poly reg:  0.017
MSE cost for deg 10 poly reg: 0.000
```

متعددة الحدود من الدرجة العاشره تكلفتها كانت صفر! سنفهم ذلك إذا أمعنا النظر في الرسم البياني؛ تمكنت متعددة الحدود من الدرجة العاشره من المرور بشكل صحيح على مكان كل نقطة في البيانات.

ولكن، يجب أن تتردد من إستخدام الدرجة العاشره من متعددة الحدود لتوقع تقييم الآيس كريم. متعددة الحدود من الدرجة العاشره تظهر أنها مضبوطة للبيانات التي لدينا بشكل متقن. إذا حصلنا على بيانات جديده ورسمنا النتائج على مخطط التشتت، نتوقع أن تكون النتائج قريبة من البيانات الأصلية. لو جربنا ذلك على متعددة الحدود من الدرجة العاشرة، فأن النتائج تظهر أسوء بكثير من متعددة الحدود من الدرجة الثانية:

```python
# انشاء بيانات عشوائية ذات حجم مشابه
# للبيانات الأصليه في المصفوفة ice
# https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html
np.random.seed(1)
x_devs = np.random.normal(scale=0.4, size=len(ice))
y_devs = np.random.normal(scale=0.4, size=len(ice))

plt.figure(figsize=(10, 5))

# الدرجة 10 على البيانات الجديده
plt.subplot(121)
ys = clf_ten.predict(trans_ten.transform(xs))
plt.plot(xs, ys)
plt.scatter(ice['sweetness'] + x_devs,
            ice['overall'] + y_devs,
            c='g')
plt.title('Degree 10 poly, second set of data')
plt.ylim(3, 7);

# الدرجة 2 على البيانات الجديده
plt.subplot(122)
ys = clf.predict(transformer.transform(xs))
plt.plot(xs, ys)
plt.scatter(ice['sweetness'] + x_devs,
            ice['overall'] + y_devs,
            c='g')
plt.title('Degree 2 poly, second set of data')
plt.ylim(3, 7);
```

<p align="center"> 
<img src='{{ site.baseurl }}/img/chapter14/feature_polynomial_40_0.png'>
</p>

نرى في هذه الحالة، أن متعددة الحدود من الدرجة الثانية حصلت على نتائج أفضل من النموذج بدون تغير في الخصائص و متعددة الحدود من الدرجة العاشره.

ذلك يجعلنا نتسائل: بشكل عام، متى نحدد الضبط المناسب لدرجة متعددة الحدود؟ على الرغم من ترددنا من إستخدام التكلفة على بيانات التدريب لإختيار أفضل متعددة الحدود، رأينا أن استخدام التكلفة قد يجعلنا نختار نماذج معقدة جداً. بدلاً من ذلك، نريد تقييم النموذج على بيانات لم تستخدم في ضبط متغيراته.

### ملخص التنبؤ بتقييم الآيس كريم

في هذا الجزء، تعرفنا على طريقة أخرى لهندسة الخصائص: إضافة خصائص متعددة الحدود للبيانات من أجل تطبيق الإنحدار متعدد الحدود. كما في One-Hot Encoding، إضافة خصائص متعددة الحدود يسمح لنا بإستخدام نموذج الإنحدار الخطي بشكل فعال على أنواع متعددة من البيانات.

رأينا أيضاً مشكلة أساسية تواجهنا عندما نقوم بهندسة الخصائص. إضافة الكثير من الخصائص يعطي نموذجنا تكلفة أقل على البيانات الأصلية ولكن عادة ما يعطي نتائج غير صحيحه عند تطبيق النموذج على بيانات جديدة.