---
title: ุงููุฒูู ุงูุงุดุชูุงูู ูุชุญุณูู ุงููุชุงุฆุฌ ุงููููุฉ
show_title: true
chapter_number: 11
chapter_text: ุงููุตู ุงูุญุงุฏู ุนุดุฑ
chapter_lessons: [[0, 'ููุฏูุฉ'], [1, 'ุชูููู ุงูุฎุณุงุฑุฉ ุจุงุณุชุฎุฏุงู ุจุฑูุงูุฌ'], [2, 'ุงููุฒูู ุงูุงุดุชูุงูู'], [3, 'ุงูุชุญุฏุจ'], [4, 'ุงููุฒูู ุงูุงุดุชูุงูู ุงูุนุดูุงุฆู']]
chapter_sublessons: [
    [],
    ['ูุดุงูู simple_minimize'],
    ['ุงูููุฑุฉ', 'ุชุญููู ุงููุฒูู ุงูุงุดุชูุงูู', 'ุชุนุฑูู ุฏุงูุฉ minimize', 'ุชูููู ุฎุณุงุฑุฉ Huber', 'ููุฎุต ุงููุฒูู ุงูุงุดุชูุงูู'],
    ['ุงูุชุดุงู ุงูุญุฏูุฏ ุงูุฏููุง ุจุงููุฒูู ุงูุงุดุชูุงูู', 'ุชุนุฑูู ุงูุชุญุฏุจ', 'ููุฎุต ุงูุชุญุฏุจ'],
    [['ุงุณุชุฎุฏุงู ุฏุงูุฉ ุงูุฎุณุงุฑุฉ MSE'], 'ุณููู ุงููุฒูู ุงูุงุดุชูุงูู ุงูุนุดูุงุฆู', 'ุชุนุฑูู ุฏุงูุฉ ูููุฒูู ุงูุงุดุชูุงูู ุงูุนุดูุงุฆู', 'ูุฒูู ุงุดุชูุงูู ุจุฏูุนุงุช ุตุบูุฑุฉ', 'ุชุนุฑูู ุฏุงูุฉ ูููุฒูู ุงูุงุดุชูุงูู ุจุฏูุนุงุช ุตุบูุฑุฉ', ููุฎุต ุงููุฒูู ุงูุงุดุชูุงูู ุงูุนุดูุงุฆู']
]
layout: default
---

## ููุฏูุฉ

ูุงุณุชุฎุฏุงู ูุงุนุฏุฉ ุจูุงูุงุช ููุชูุจุค ูุงูุชููุนุ ูุฌุจ ุนูููุง ุชูููู ูููุฐุฌูุง ุจุดูู ุฏููู ูุงุฎุชูุงุฑ ุฏุงูุฉ ุฎุณุงุฑุฉ. ูุซูุงูุ ุจูุงูุงุช ุงูุฅูุฑุงููุงุชุ ูููุฐุฌูุง ุชููุน ุฃู ูุณุจุฉ ุงูุฅูุฑุงููุฉ ุซุงุจุชุฉ ูุง ุชุชุบูุฑ. ุซู ูุฑุฑูุง ุงุณุชุฎุฏุงู ุฏุงูุฉ ุงูุฎุทุฃ ุงูุชุฑุจูุนู ุงููุชูุณุท MSE ููุฌุฏูุง ุงููููุฐุฌ ุงูุฃูู ุฎุณุงุฑุฉ.

ูุฌุฏูุง ุฃูุถุงู ุฃู ููุงู ูุตูุงู ุฃุจุณุท ูุฏุงูุชู ุงูุฎุณุงุฑุฉ ุงูุฎุทุฃ ุงูุชุฑุจูุนู ุงููุชูุณุท ู ูุชูุณุท ุงูุฎุทุฃ ุงูุญุชูู ููู: ุงููุชูุณุท ูุงููุณูุท. ููููุ ูููุง ูุงู ูููุฐุฌูุง ูุฏุงูุฉ ุงูุฎุณุงุฑุฉ ุฃูุซุฑ ุชุนููุฏุงู ูู ูุณุชุทูุน ุฅูุฌุงุฏ ูุตูุงู ุฑูุงุถูุงู ููุงุณุจ. ูุซูุงูุ ุฏุงูุฉ Huber ูุฏููุง ุฎุตุงุฆุต ูููุฏู ูููู ุตุนุจ ุชูููุฒูุง.

ูููููุง ุงุณุชุฎุฏุงู ุงูููุจููุชุฑ ูุญู ูุฐู ุงููุดูู ุจูุงุณุทุฉ ุงููุฒูู ุงูุงุดุชูุงููุ ุทุฑููู ุญุณุงุจูุฉ ูุชูููู ุฏูุงู ุงูุฎุณุงุฑุฉ.

## ุชูููู ุงูุฎุณุงุฑุฉ ุจุงุณุชุฎุฏุงู ุจุฑูุงูุฌ

ููุนูุฏ ูููููุฐุฌ ูู ุงููุตู ุงูุณุงุจู:

$$ \theta = C $$

ุณูุณุชุฎุฏู ุฏุงูุฉ ุงูุฎุณุงุฑุฉ MSE:

$$ \begin{split}
\begin{aligned}
L(\theta, \textbf{y})
&= \frac{1}{n} \sum_{i = 1}^{n}(y_i - \theta)^2\\
\end{aligned}
\end{split} $$

ููุชุจุณูุทุ ุณูุณุชุฎุฏู ุงูุจูุงูุงุช ุงูุชุงููุฉ: $ \textbf{y} = [ 12, 13, 15, 16, 17 ] $. ูุนูู ูู ุฎูุงู ุชุญููููุง ููุจูุงูุงุช ูู ุงููุตู ุงูุณุงุจู ุฃู ูููุฉ $ \theta $ ูุฏุงูุฉ ุงูุฎุณุงุฑุฉ MSE ูู ุงููุชูุณุท $ \text{mean}(\textbf{y}) = 14.6 $. ููุฑู ุงุฐุง ูุงู ุจุฅููุงููุง ุงูุญุตูู ุนูู ูุชูุฌุฉ ุนูุฏ ูุชุงุจุชูุง ูุจุฑูุงูุฌ ููุฌุฏูุง.

ุฅุฐุง ูููุง ุจูุชุงุจุฉ ุจุฑูุงูุฌ ุจุดูู ูุชููุ ูุจุฅููุงููุง ุงุณุชุฎุฏุงู ููุณ ุงูุจุฑูุงูุฌ ุนูู ุฃู ุฏุงูุฉ ุฎุณุงุฑุฉ ูุฅูุฌุงุฏ ุฃูู ูููุฉ ู $ \theta $ุ ูุดูู ุฐูู ุฏุงูุฉ Huber ุงููุนูุฏุฉ ุฑูุงุถูุงู:

$$ \begin{split}
L_\alpha(\theta, \textbf{y}) = \frac{1}{n} \sum_{i=1}^n \begin{cases}
    \frac{1}{2}(y_i - \theta)^2 &  | y_i - \theta | \le \alpha \\
    \alpha ( |y_i - \theta| - \frac{1}{2}\alpha ) & \text{otherwise}
\end{cases}
\end{split} $$

ุฃููุงูุ ูููู ุจุฑุณู ุชุฎุทูุทู ููุจูุงูุงุช. ุจุงูุฌุงูุจ ุงูุฃููู ูู ุงูุฑุณู ูููู ุจุฑุณู ุฏุงูุฉ ุงูุฎุณุงุฑุฉ MSE ูููู ูุฎุชููุฉ ู $ \theta $:

```python
pts = np.array([12, 13, 15, 16, 17])
points_and_loss(pts, (11, 18), mse)
```

> ูู ุงูููุฏ ุงูุจุฑูุฌู ุงูุณุงุจู ุงุณุชุฎุฏู ุงููุงุชุจ ุฏุงูุฉ ุนุฑููุง ูุณุจูุงู ุจุฃุณู `points_and_loss`ุ ุชูุจู ุงูุฏุงูุฉ ุซูุงุซ ูุชุบูุฑุงุชุ ุงูุฃููู ูู ุงูุจูุงูุงุช. ุงููุชุบูุฑ ุงูุซุงูู ูู ููุงุณุงุช ุฃุจุนุงุฏ x-axis ุ ูุงููููุฉ ุงูุฃุฎูุฑุฉ ูู ููุน ุฏุงูุฉ ุงูุฎุณุงุฑุฉุ ูุงูุชู ูู ุนุจุงุฑุฉ ุนู ุฏุงูุฉ ุฃุฎุฑู ุนุฑููุง ุฃูุถุงู ุจุฃุณู `mse`. ุชุนุฑูู ููุง ุงูุฏุงูุชูู ูู ูุงูุชุงูู:
>
> ```python
> def mse(theta, y_vals):
>     return np.mean((y_vals - theta) ** 2)
> 
> def points_and_loss(y_vals, xlim, loss_fn):
>     thetas = np.arange(xlim[0], xlim[1] + 0.01, 0.05)
>     losses = [loss_fn(theta, y_vals) for theta in thetas]
>     
>     plt.figure(figsize=(9, 2))
>     
>     ax = plt.subplot(121)
>     sns.rugplot(y_vals, height=0.3, ax=ax)
>     plt.xlim(*xlim)
>     plt.title('Points')
>     plt.xlabel('Tip Percent')
>     
>     ax = plt.subplot(122)
>     plt.plot(thetas, losses)
>     plt.xlim(*xlim)
>     plt.title(loss_fn.__name__)
>     plt.xlabel(r'$ \theta $')
>     plt.ylabel('Loss')
>     plt.legend()
> ```
> 

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_basics_3_0.png'>
</p>

ููู ุจุฅููุงููุง ุจุฑูุฌุฉ ุจุฑูุงูุฌ ูููู ุจุฅูุฌุงุฏ ุฃูู ูููุฉ ู $ \theta $ ุฃูุชููุงุชูููุงูุ ุงูุทุฑููุฉ ุงูุฃุณูู ูู ุจุญุณุงุจ ูููุฉ ุงูุฎุณุงุฑุฉ ูุฃูุซุฑ ูู ูููู ู $ \theta $ุ ุซู ููุฌุฏ ูููุฉ $ \theta $ ุฐุง ุงูุฃูู ุฎุณุงุฑุฉ.

ุนุฑููุง ุฏุงูุฉ ุจุฃุณู `simple_minimize` ูุงูุชู ุชูุจู ูุชุบูุฑุงุช ูู ุฏุงูุฉ ุงูุฎุณุงุฑุฉุ ูุตูููุฉ ุงูุจูุงูุงุชุ ููุตูููุฉ ุจููู $ \theta $:

```python
def simple_minimize(loss_fn, dataset, thetas):
    '''
    ุงููููู ุงูููุงุฆูู ููุฐู ุงูุฏุงูู ูู ูููุฉ ฮธ ูู ุจูู ุนุฏุฉ ููู ูู ฮธ ุฐุงุช ุงูุฃูู ุฎุณุงุฑุฉ
    '''
    losses = [loss_fn(theta, dataset) for theta in thetas]
    return thetas[np.argmin(losses)]
```

ุซู ูุนุฑู ุฏุงูุฉ ูุฅูุฌุงุฏ ูููุฉ MSE ูุงุณุชุฎุฏุงููุง ูู ุฏุงูุฉ `simple_minimize`:

```python
def mse(theta, dataset):
    return np.mean((dataset - theta) ** 2)

dataset = np.array([12, 13, 15, 16, 17])
thetas = np.arange(12, 18, 0.1)

simple_minimize(mse, dataset, thetas)
```

```ruby
14.599999999999991
```

ุงููุชูุฌุฉ ูุฐู ูุฑูุจู ูููููุฉ ุงููุชููุนุฉ:

```python
# ุงูุฌุงุฏ ุงููููู ุจุงุณุชุฎุฏุงู ุงููุชูุณุท
np.mean(dataset)
```

```ruby
14.6
```

ุงูุขู ูููููุง ูุชุงุจุฉ ุฏุงูุฉ ูุญุณุงุจ ุฎุณุงุฑุฉ Huber ูุฑุณููุง:

```python
def huber_loss(theta, dataset, alpha = 1):
    d = np.abs(theta - dataset)
    return np.mean(
        np.where(d < alpha,
                 (theta - dataset)**2 / 2.0,
                 alpha * (d - alpha / 2.0))
    )

points_and_loss(pts, (11, 18), huber_loss)
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_basics_12_0.png'>
</p>

ุนูู ุงูุฑุบู ุฃู ุงูููู ุงูุฏููุง ู $ \theta $ ูุฌุจ ุฃู ุชููู ุฃูุฑุจ ุฅูู 15ุ ููุณ ูุฏููุง ุทุฑููู ูุชุญููู ูุฅูุฌุงุฏ ูููุฉ $ \theta $ ูุฏุงูุฉ ุงูุฎุณุงุฑุฉ Huber. ุจุฏูุงูุ ูู ุฐููุ ุณูุณุชุฎุฏู ุงูุฏุงูุฉ `simple_minimize`:

```python
simple_minimize(huber_loss, dataset, thetas)
```

```ruby
14.999999999999989
```

ุงูุขูุ ููุนูุฏ ูุจูุงูุงุช ูุณุจุฉ ุงูุฅูุฑุงููุงุช ูููุฌุฏ ุฃูุถู ูููู ู $ \theta $ ุจุงุณุชุฎุฏุงู Huber:

```python
tips = sns.load_dataset('tips')
tips['pcttip'] = tips['tip'] / tips['total_bill'] * 100
tips.head()
```

**pcttip**|**size**|**time**|**day**|**smoker**|**sex**|**tip**|**total\_bill**| 
:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:
5.944673|2|Dinner|Sun|No|Female|1.01|16.99|0
16.054159|3|Dinner|Sun|No|Male|1.66|10.34|1
16.658734|3|Dinner|Sun|No|Male|3.5|21.01|2
13.978041|2|Dinner|Sun|No|Male|3.31|23.68|3
14.680765|4|Dinner|Sun|No|Female|3.61|24.59|4

<br>
```python
points_and_loss(tips['pcttip'], (11, 20), huber_loss)
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_basics_17_0.png'>
</p>

```python
simple_minimize(huber_loss, tips['pcttip'], thetas)
```

```ruby
15.499999999999988
```

ููุงุญุธ ุฃู ุนูุฏ ุงุณุชุฎุฏุงู ุฏุงูุฉ ุฎุณุงุฑุฉ Huber ูุงูุช ุงููุชูุฌุฉ $ \hat{\theta} = 15.5 $ . ูููููุง ุงูุขู ููุงุฑูุฉ ูุฐู ุงููุชูุฌุฉ ูุน MSE ู MAE:

```python
print(f"               MSE: theta_hat = {tips['pcttip'].mean():.2f}")
print(f"               MAE: theta_hat = {tips['pcttip'].median():.2f}")
print(f"        Huber loss: theta_hat = 15.50")
```

```ruby
            MSE: theta_hat = 16.08
            MAE: theta_hat = 15.48
    Huber loss: theta_hat = 15.50
```

ููุงุญุธ ุฃู ุฏุงูุฉ Huber ุฃูุฑุจ ุฅูู MAE ููููุง ูุง ุชุชุฃุซุฑ ุจุดูู ูุจูุฑ ุจุณุจุจ ุงูููู ุงูุดุงุฐุฉ ุนูู ุงูุฌุงูุจ ุงูุฃููู ูู ุงูุฑุณู ุงูุจูุงูู ุงูุชุงูู ูุชูุฒูุน ุจูุงูุงุช ุงูุฅูุฑุงููุงุช:

```python
sns.distplot(tips['pcttip'], bins=50);
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_basics_22_0.png'>
</p>

### ูุดุงูู simple_minimize

ุนูู ุงูุฑุบู ูู ุฃู ุฏุงูุฉ `simple_minimize` ุชุณุงุนุฏูุง ุนูู ุชูููู ุฏุงูุฉ ุงูุฎุณุงุฑุฉุ ุฅูุง ุฃู ูุฏููุง ุจุนุถ ุงููุดุงูู ุงูุชู ุชุฌุนููุง ุบูุฑ ูููุฏู ููุงุณุชุฎุฏุงู ุจุดูู ุนุงู. ูุดููุชูุง ุงูุฃูู ูู ุฃููุง ุชุนูู ููุท ูุน ููู $ \theta $. ูุซูุงูุ ูู ุงูููุฏ ุงูุจุฑูุฌู ุงูุชุงููุ ุงูุฐู ุณุจู ุฃู ุงุณุชุฎุฏููุงู ูู ุงูุฃุนููุ ุงุญุชุฌูุง ูุชุนุฑูู ููู $ \theta $ ูุฏููุงู ูู 12 ุฅูู 18:

```python
dataset = np.array([12, 13, 15, 16, 17])
thetas = np.arange(12, 18, 0.1)

simple_minimize(mse, dataset, thetas)
```

ููู ูุฌุฏูุง ุฃูู ุนูููุง ุงูุชุญูู ูู ุงูููู ุจูู 12 ู 18ุ ุงุญุชุฌูุง ููุฑุงุฌุนุฉ ุงูุฑุณู ุงูุจูุงูู ูุฏุงูุฉ ุงูุฎุณุงุฑุฉ ููุฌุฏูุง ุฃู ุงูููู ุงูุฏููุง ุจูู ุชูู ุงููููุชูู. ูุฐู ุงูุทุฑููุฉ ุบูุฑ ุนูููุฉ ูุฃููุง ูููุง ุจุฅุถุงูุฉ ุฎุทูู ูุนูุฏุฉ ุฌุฏูุฏู ูููุงุฐุฌูุง. ุจุงูุฅุถุงูุฉ ูุฐููุ ูููุง ุจุชุนุฑูู ููู ุงูุฒูุงุฏุฉ 0.1 ุจุดูู ูุฏูู. ููููุ ุฅุฐุง ูุงูุช ุงููููุฉ ุงููุซูู ู $ \theta $ ูู 12.043ุ ุณุชููู ุงูุฏุงูุฉ `simple_minimize` ุจุชูุฑูุจ ุงููุชูุฌุฉ ุฅูู 12.00 ููููุง ุงูุฃูุฑุจ ููุถุงุนูุงุช 0.1

ูููููุง ุญู ุชูู ุงููุดุงูู ุจุทุฑููู ูุงุญุฏุฉ ุจุงุณุชุฎุฏุงู ูุง ูุณูู ุจู *ุงููุฒูู ุงูุงุดุชูุงูู Gradient Descent*.

## ุงููุฒูู ุงูุงุดุชูุงูู

ูุญู ูููุชููู ูุจูุงุก ุฏุงูุฉ ุชุณุชุทูุน ุงูุชูููู ูู ุฏุงูุฉ ุงูุฎุณุงุฑุฉ ุฏูู ุชูููุฏ ุงููุณุชุฎุฏู ูุชุญุฏูุฏ ููู ูุณุจูุฉ ู $ \theta $ ููุชุฌุฑุจุฉ ุนูููุง. ุจูุนูู ุฃุตุญุ ุจูุง ุฃู ุฏุงูุฉ `simple_minimize` ุดูููุง ูุงูุชุงูู: [๐][GradientDescent]

```python
simple_minimize(loss_fn, dataset, thetas)
```

ูุฑูุฏ ุฏุงูุฉ ูุฏููุง ุงูุดูู ุงูุชุงูู

```python
minimize(loss_fn, dataset)
```

> ูุงุญุธ ูู ุงูุดูู ุงูุฐู ูุจุญุซ ุนููุ ูุง ูุญุชุงุฌ ุงููุณุชุฎุฏู ูุฅุถุงูุฉ ูููู ูุณุจูุฉ ู $ \theta $ ูู ุงููุชุบูุฑุงุช ุงููุทููุจุฉ ูุฏุงูุฉ ุชูููู ุงูุฎุณุงุฑุฉ `minimize`.

ุชุญุชุงุฌ ูุฐู ุงูุฏุงูุฉ ูุฅูุฌุงุฏ ููู $ \theta $ ุงูุฃูู ุฎุณุงุฑุฉ ุฃูุชููุงุชูููุงู ุฃูุงู ูุงู ุญุฌููุง. ุณูุณุชุฎุฏู ุทุฑููุฉ ุชุณูู ุจุงููุฒูู ุงูุงุดุชูุงูู ูุจูุงุก ุงูุฏุงูุฉ ุงูุฌุฏูุฏุฉ ุงููุณูุงุฉ `minimize`.

### ุงูููุฑุฉ

ููุง ูู ุฏูุงู ุงูุฎุณุงุฑุฉุ ุณูุชุญุฏุซ ุนู ููุฑุฉ ุงููุฒูู ุงูุงุดุชูุงูู ุฃููุงูุ ุซู ูุชุนุฑู ููููู ุงูุนูููุฉ ุงูุฑูุงุถูุฉ ูููุง.

ุจูุง ุฃู ุงูุฏุงูุฉ `minimize` ูุง ููุฏู ููุง ููู ู $ \theta $ ููุชุฌุฑุจุฉ ุนูููุงุ ูููู ุจุงุฎุชูุงุฑ ูููู ู $ \theta $ ุจุฃู ููุงูู. ุซูุ ูููู ุจุดูู ุชูุฑุงุฑู ุจุชุญุณูู ูุชุงุฆุฌ $ \theta $. ูููุชุญุณูู ูู ุงููุชุงุฆุฌุ ูููู ุจููุงุญุธุฉ ุงููููุงู Slope ูุชูู ุงููููุฉ ูู $ \theta $ ุงูุชู ุงุฎุชุฑูุงูุง ูู ุงูุฑุณู ุงูุจูุงูู.

ูุซูุงูุ ุณูุณุชุฎุฏู MSE ุนูู ุงูุจูุงูุงุช ุงูุชุงููุฉ $ \textbf{y} = [ 12.1, 12.8, 14.9, 16.3, 17.2 ] $ ููููุฉ $ \theta $ ุงูุชู ุงุฎุชุฑุงูุงูุง ูู 12:

```python
pts = np.array([12.1, 12.8, 14.9, 16.3, 17.2])
plot_loss(pts, (11, 18), mse)
plot_theta_on_loss(pts, 12, mse)
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_descent_define_5_0.png'>
</p>

> ุงุณุชุฎุฏู ุงููุงุชุจ ุฏุงูุชูู ููุง ูุนู ูุณุจูุงู ูุชุณุงุนุฏู ุนูู ุงูููุงู ุจุงูุนูููุฉ ุงูุญุณุงุจูุฉ ูุงูุฑุณู ุงูุจูุงูู ูููุง `plot_loss` ู `plot_theta_on_loss`. ูุงูููุฏ ุงูุจุฑูุฌู ูู ุงูุฃุณูู ูู ุชุนุฑูู ููุง ุงูุฏุงูุชูู:
>
> ```python
> def plot_loss(y_vals, xlim, loss_fn):
>    thetas = np.arange(xlim[0], xlim[1] + 0.01, 0.05)
>    losses = [loss_fn(theta, y_vals) for theta in thetas]
>
>    plt.figure(figsize=(5, 3))
>    plt.plot(thetas, losses, zorder=1)
>    plt.xlim(*xlim)
>    plt.title(loss_fn.__name__)
>    plt.xlabel(r'$ \theta $')
>    plt.ylabel('Loss')
>   
>   
> def plot_theta_on_loss(y_vals, theta, loss_fn, **kwargs):
>    loss = loss_fn(theta, y_vals)
>    default_args = dict(label=r'$ \theta $', zorder=2,
>                        s=200, c=sns.xkcd_rgb['green'])
>    plt.scatter([theta], [loss], **{**default_args, **kwargs})
> ```
> 

ูุฑูุฏ ุงุฎุชูุงุฑ ูููู ุฌุฏูุฏู ู $ \theta $ ูุชูููู ุงูุฎุณุงุฑุฉ. ููุนูู ุฐููุ ููุง ุฐูุฑูุง ุณุงุจูุงูุ ููุงุญุธ ุงููููุงู ููููุฉ $ \theta= 12 $:

```python
pts = np.array([12.1, 12.8, 14.9, 16.3, 17.2])
plot_loss(pts, (11, 18), mse)
plot_tangent_on_loss(pts, 12, mse)
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_descent_define_7_0.png'>
</p>

ูููุฉ ุงููููุงู ุณูุจูุฉุ ูุนูู ุฐูู ุฃู ุฒูุงุฏุฉ ูููุฉ $ \theta $ ุณูููู ูู ุงูุฎุณุงุฑุฉ.
ุฅุฐุง ูุงูุช $ \theta= 16.5 $ุ ูุฅู ูููุฉ ุงููููุงู ุณุชููู ููุฌุจู:

```python
pts = np.array([12.1, 12.8, 14.9, 16.3, 17.2])
plot_loss(pts, (11, 18), mse)
plot_tangent_on_loss(pts, 16.5, mse)
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_descent_define_9_0.png'>
</p>

ุนูุฏูุง ุชููู ูุชูุฌุฉ ุงููููุงู ุฅูุฌุงุจูุฉุ ูุฅู ุชูููู ูููุฉ $ \theta $ ุณูููู ุงูุฎุณุงุฑุฉ.

ุงููููุงู ูู ุงูุฎุท ูุฎุจุฑูุง ุจุฃู ุงุชุฌุงู ูุฎุชุงุฑ $ \theta $ ูุชูููู ุงูุฎุณุงุฑุฉ. ุฅุฐุง ูุงู ุงููููุงู ูุชูุฌุชู ุณูุจูุฉุ ููุญุชุงุฌ ูุชุญุฑู $ \theta $ ุฅูู ุงูุฌุงูุจ ุงูุฅูุฌุงุจู. ูุฅุฐุง ูุงู ุฅูุฌุงุจูุงูุ ูุนูููุง ุชุญุฑูู $ \theta $ ุฅูู ุงูุฌุงูุจ ุงูุณูุจู. ุฑูุงุถุงูุ ูููู ุงูุชุงูู:

$$ \theta^{(t+1)} = \theta^{(t)} - \frac{\partial}{\partial \theta} L(\theta^{(t)}, \textbf{y}) $$

ููููุง $ \theta^{(t)} $ ูู ุงููููุฉ ุงูุญุงููุฉุ ู $ \theta^{(t+1)} $ ูู ุงููููุฉ ุงูุชุงููุฉ.

ุจุงููุณุจุฉ ู MSEุ ูุณุชููู ูุงูุชุงูู:

$$ \begin{split}
\begin{aligned}
L(\theta, \textbf{y})
&= \frac{1}{n} \sum_{i = 1}^{n}(y_i - \theta)^2\\
\frac{\partial}{\partial \hat{\theta}} L(\theta, \textbf{y})
&= \frac{1}{n} \sum_{i = 1}^{n} -2(y_i - \theta) \\
&= -\frac{2}{n} \sum_{i = 1}^{n} (y_i - \theta) \\
\end{aligned}
\end{split} $$

ุนูุฏูุง ุชููู $ \theta^{(t)} = 12 $ุ ูุงููุชูุฌุฉ ูู $ -\frac{2}{n} \sum_{i = 1}^{n} (y_i - \theta) = -5.32 $ุ ุซู ูุณุชุฎุฏููุง ุจุงููุนุงุฏูุฉ ุงูุณุงุจูุฉ: $ \theta^{(t+1)} = 12 - (-5.32) = 17.32 $

ุฑุณููุง ูู ุงูุฃุณูู ุงููููุฉ ุงูุณุงุจูุฉ ู $ \theta $ ุจุฏุงุฆุฑุฉ ููุฑุบุฉ ุจุญุฏูุฏ ุฎุถุฑุงุก ูุงููููุฉ ุงูุฌุฏูุฏุฉ ููุง ุจุฏุงุฆุฑุฉ ุจุงูููู ุงูุฃุฎุถุฑ:

```python
pts = np.array([12.1, 12.8, 14.9, 16.3, 17.2])
plot_loss(pts, (11, 18), mse)
plot_theta_on_loss(pts, 12, mse, c='none',
                   edgecolor=sns.xkcd_rgb['green'], linewidth=2)
plot_theta_on_loss(pts, 17.32, mse)
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_descent_define_11_0.png'>
</p>

ุนูู ุงูุฑุบู ุฃู $ \theta $ ุงูุชููุช ุฅูู ุงูุฌุงูุจ ุงูุฃูููุ ููููุง ูุง ุฒุงูุช ุจุนูุฏู ุฌุฏุงู ุนู ุงููููุฉ ุงูุฏููุง. ูููููุง ุญู ุฐูู ุนู ุทุฑูู ุถุฑุจ ุงููููุงู ุจูููู ุตุบูุฑุฉ ูุจู ุทุฑุญู ูู $ \theta $. ูุงูุนูููุฉ ุงูุญุณุงุจูุฉ ุงูููุงูุฉ ุณุชุจุฏู ูุงูุชุงูู:

$$ \theta^{(t+1)} = \theta^{(t)} - \alpha \cdot \frac{\partial}{\partial \theta} L(\theta^{(t)}, \textbf{y}) $$

ููููุง $ \alpha $ ูู ูููู ุซุงุจุชุฉ ุตุบูุฑุฉ. ูุซูุงูุ ุฅุฐุง ุญุฏุฏูุง ูููุฉ $ \alpha = 0.3 $ุ ูุฅู ุงููููุฉ ุงูุฌุฏูุฏุฉ ู $ \theta^{(t+1)} $ ุณุชููู:

```python
plot_one_gd_iter(pts, 12, mse, grad_mse)
```

```ruby
old theta: 12
new theta: 13.596
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_descent_define_14_1.png'>
</p>


> ุนุฑู ุงููุงุชุจ ุฏุงูุฉ ุฌุฏูุฏู ุจุฃุณู `plot_one_gd_iter` ุชููู ุจูุชุงุจุฉ ูุฑุณู ุจูุงูู ููููู $ \theta $ ุงูุณุงุจูุฉ ูุงูุฌุฏูุฏุฉ ุจุนุฏ ุงูููุงู ุจุงูุนูููุฉ ุงูุญุณุงุจูุฉ ุงููุดุฑูุญุฉ ูุณุจูุงูุ ุงุณุชุฎุฏู ุงููุงุชุจ ุฃูุถุงู ุฏุงูุฉ ุจุฃุณู `grad_mse` ููู ุชุนุฑูู ูุฏุงูุฉ `mse` ุจุงุณุชุฎุฏุงู ุงููุฒูู ุงูุงุดุชูุงููุ ุงูููุฏ ุงูุจุฑูุฌู ูููุง ุงูุฏุงูุชูู:
>
> ```python
>   def plot_one_gd_iter(y_vals, theta, loss_fn, grad_loss, alpha=0.3):
>       new_theta = theta - alpha * grad_loss(theta, y_vals)
>       plot_loss(pts, (11, 18), loss_fn)
>       plot_theta_on_loss(pts, theta, loss_fn, c='none',
>                          edgecolor=sns.xkcd_rgb['green'], linewidth=2)
>       plot_theta_on_loss(pts, new_theta, loss_fn)
>       print(f'old theta: {theta}')
>       print(f'new theta: {new_theta}')
>
>   def grad_mse(theta, y_vals):
>       return -2 * np.mean(y_vals - theta)
> ```

ูู ุงูุฑุณู ุงูุชุงููุ ููู $ \theta $ ุจุนุฏ ุนุฏุฉ ุชูุฑุงุฑุงุช ุจููุณ ุงูุทุฑููุฉ ุงูุณุงุจูุฉ. ูุงุญุธ ุฃู $ \theta $ ุชุชุบูุฑ ุจุดูู ุจุณูุท ูููุง ุงูุชุฑุจูุง ูู ุงููููุฉ ุงูุฏููุง ูุฃู ุงููููุงู ุฃูุถุงู ุฃุตุจุญุช ูููุชู ุฃูู:

```python
plot_one_gd_iter(pts, 13.60, mse, grad_mse)
```

```ruby
old theta: 13.6
new theta: 14.236
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_descent_define_16_1.png'>
</p>

```python
plot_one_gd_iter(pts, 14.24, mse, grad_mse)
```

```ruby
old theta: 14.24
new theta: 14.492
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_descent_define_17_1.png'>
</p>

```python
plot_one_gd_iter(pts, 14.49, mse, grad_mse)
```

```ruby
old theta: 14.49
new theta: 14.592
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_descent_define_18_1.png'>
</p>

### ุชุญููู ุงููุฒูู ุงูุงุดุชูุงูู

ูุฏููุง ุงูุขู ููุฑุฉ ุนู ุทุฑููุฉ ุนูู ุฎูุงุฑุฒููุฉ ุงููุฒูู ุงูุงุดุชูุงูู:
- ุงุฎุชูุงุฑ ูููู ุฃูููู ู $ \theta $ ( ูู ุงูุนุงุฏุฉ ุชููู 0 ).
- ุฅุฌุฑุงุก ุงูุนูููุฉ ุงูุญุณุงุจูุฉ $ \theta - \alpha \cdot \frac{\partial}{\partial \theta} L(\theta, \textbf{y}) $ ุนูููุง ูุญูุธ ุงููุชูุฌุฉ ููููู ุฌุฏูุฏู ู $ \theta $.
- ุชูุฑุงุฑ ุงูุนูููุฉ ุญุชู ุชุชููู $ \theta $ ุนู ุงูุชุบูุฑ.

ุบุงูุจุงู ุณุชูุงุญุธ ุงุณุชุฎุฏุงู ุฑูุฒ ุงููุฒูู (ุงูุงูุญุฏุงุฑ) $ \nabla_\theta $ ุจุฏูุงู ูู ุงูุงุดุชูุงู ุงูุฌุฒุฆู $ \frac{\partial}{\partial \theta} $. 
ููุง ุงูุฑูุฒูู ูุชุดุงุจูุงูุ ูููู ุจูุง ุฃูุง ุงุณุชุฎุฏุงู ุฑูุฒ ุงููุฒูู ุฃูุซุฑ ุจุดูู ุนุงูุ ูุณูููู ุจุงุณุชุฎุฏุงูู ูู ุงููุนุงุฏูุฉ:

$$ \theta^{(t+1)} = \theta^{(t)} - \alpha \cdot \nabla_\theta L(\theta^{(t)}, \textbf{y}) $$

ููุฑุงุฌุนุฉ ุงูุฑููุฒ:

- $ \theta^{(t)} $ ูู ุงูุชููุน ุงูุญุงูู ู $ \theta^{*} $ ูู ุงูุชูุฑุงุฑ $ t $.
- $ \theta^{(t+1)} $ ุงููููุฉ ุงูุชุงููุฉ ู $ \theta $.
- $ \alpha $ ูุทูู ุนูููุง ูุนุฏู ุงูุชุนููู Learning Rateุ ูุนุงุฏุฉ ูุง ุชููู ุฑูู ุตุบูุฑ ุซุงุจุช. ูู ุจุนุถ ุงููุฑุงุช ูู ุงููููุฏ ุฃู ุชุจุฏุฃ ุจุฑูู ุนุงูู ู $ \alpha $ ูุงูุชูููู ููู. ุฅุฐุง ุชุบูุฑุช ูููุฉ $ \alpha $ ุจูู ุนูููุงุช ุงูุชูุฑุงุฑุ ูุณุชุฎุฏู ุงูุฑูุฒ $ \alpha^t $ ูุชูุถูุญ ุชุบูุฑ $ \alpha $ ูู $ t $.
- $ \nabla_\theta L(\theta^{(t)}, \textbf{y}) $ ูู ุงุดุชูุงู ุฌุฒุฆู ูุฏุงูุฉ ุงูุฎุณุงุฑุฉ ูููุง ูููู ูุชููุนุฉ ู $ \theta $ ูู ุงูุชูุฑุงุฑ $ t $.

ูููููุง ููุงุญุธุฉ ุฃูููุฉ ุงุณุชุฎุฏุงู ุฏุงูุฉ ุฎุณุงุฑุฉ ูุงุจูู ููุชูุงุถู: $ \nabla_\theta L(\theta, \textbf{y}) $ ูู ุฌุฒุก ููู ูู ุฎูุงุฑุฒููุฉ ุงููุฒูู ุงูุงุดุชูุงูู. (ุนูู ุงูุฑุบู ุฃู ุจุงูุฅููุงู ุชููุน ูููุฉ ุงููุฒูู (ุงูุฅูุญุฏุงุฑ) ุจุญุณุงุจ ุงููุฑู ูู ุงูุฎุณุงุฑุฉ ุจูู ูููุชูู $ \theta $ ููุณูุชูุง ุนูู ุงููุณุงูู ุจููููุงุ ููู ุฐูู ูุฒูุฏ ูู ูุฏุฉ ุฅูุฌุงุฏ ุงููุชูุฌุฉ ูููุฒูู ุงูุงุดุชูุงูู ุจุดูู ูุจูุฑ ููุง ูุฌุนููุง ุบูุฑ ูููุฏู ููุงุณุชุฎุฏุงู).

ุฎูุงุฑุฒููุฉ ุงููุฒูู ุงูุงุดุชูุงูู ุจุณูุทู ููููุฏู ุจุดูู ูุจูุฑ ูุฐูู ูุฅู ุจุฅููุงููุง ุงุณุชุฎุฏุงููุง ูู ูุซูุฑ ูู ุงููุงุน ุงูููุงุฐุฌ ูุงููุซูุฑ ูู ุฏูุงู ุงูุฎุณุงุฑุฉ. ูู ุงูุทุฑููุฉ ุงูุญุณุงุจูู ุงูุฃูู ูุถุจุท ุงูููุงุฐุฌุ ุจูุง ูููุง ุงูุฅูุญุฏุงุฑ ุงูุฎุทู ุนูู ุจูุงูุงุช ุจุญุฌู ูุจูุฑ ูุงูุดุจูุงุช ุงูุนุตุจูู.

### ุชุนุฑูู ุฏุงูุฉ `minimize`

ุงูุขู ูุนูุฏ ููููุชูุง ุงูุฃุณุงุณูุฉ: ุชุนุฑูู ุฏุงูุฉ `minimize`. ุณูุญุชุงุฌ ููุชุนุฏูู ููููุงู ูู ุชุนุฑูู ุงูุฏุงูุฉ ููููุง ูุฑูุฏ ุฅูุฌุงุฏ ุงููุฒูู ุงูุงุดุชูุงูู ูุฏุงูุฉ ุงูุฎุณุงุฑุฉ:

```python
def minimize(loss_fn, grad_loss_fn, dataset, alpha=0.2, progress=True):
    '''
    ุชุณุชุฎุฏู ุงููุฒูู ุงูุงุดุชูุงูู ููุชูููู ูู ุฏุงูุฉ ุงูุฎุณุงุฑุฉ loss_fn.
    ุชูุชุฌ ููุง ุงูุฏุงูู ุงููููู ุงูุตุบุฑู ู theta_hat (ฮธ^) ุนูุฏูุง ูููู
    ุงูุชุบููุฑ ุงูู ูู 0.001 ุจูู ุงูุชูุฑุงุฑุงุช.
    '''
    theta = 0
    while True:
        if progress:
            print(f'theta: {theta:.2f} | loss: {loss_fn(theta, dataset):.2f}')
        gradient = grad_loss_fn(theta, dataset)
        new_theta = theta - alpha * gradient
        
        if abs(new_theta - theta) < 0.001:
            return new_theta
        
        theta = new_theta
```

ุซู ูููููุง ุชุนุฑูู ุฏูุงู ุชููู ุจุญุณุงุจ MSE ู ูุฒูููุง (ุงูุญุฏุงุฑูุง):

```python
def mse(theta, y_vals):
    return np.mean((y_vals - theta) ** 2)

def grad_mse(theta, y_vals):
    return -2 * np.mean(y_vals - theta)
```

ุฃุฎูุฑุงูุ ูููููุง ุงุณุชุฎุฏุงู ุงูุฏุงูุฉ `minimize` ูุญุณุงุจ ูููุฉ $ \theta $ ุงูุฃุฏูู ููุจูุงูุงุช ุงูุชุงููุฉ $ \textbf{y} = [12.1, 12.8, 14.9, 16.3, 17.2] $ 

```python
%%time
theta = minimize(mse, grad_mse, np.array([12.1, 12.8, 14.9, 16.3, 17.2]))
print(f'Minimizing theta: {theta}')
print()
```

```ruby
theta: 0.00 | loss: 218.76
theta: 5.86 | loss: 81.21
theta: 9.38 | loss: 31.70
theta: 11.49 | loss: 13.87
theta: 12.76 | loss: 7.45
theta: 13.52 | loss: 5.14
theta: 13.98 | loss: 4.31
theta: 14.25 | loss: 4.01
theta: 14.41 | loss: 3.90
theta: 14.51 | loss: 3.86
theta: 14.57 | loss: 3.85
theta: 14.61 | loss: 3.85
theta: 14.63 | loss: 3.84
theta: 14.64 | loss: 3.84
theta: 14.65 | loss: 3.84
theta: 14.65 | loss: 3.84
theta: 14.66 | loss: 3.84
theta: 14.66 | loss: 3.84
Minimizing theta: 14.658511131035242

CPU times: user 7.88 ms, sys: 3.58 ms, total: 11.5 ms
Wall time: 8.54 ms
```

ููุงุญุธ ุฃู ุงููุฒูู ุงูุงุดุชูุงูู ูุงู ุจุฅูุฌุงุฏ ููุณ ุงููุชูุฌุฉ ุจุดูู ุณุฑูุน ู:

```python
np.mean([12.1, 12.8, 14.9, 16.3, 17.2])
```

```ruby
14.66
```

### ุชูููู ุฎุณุงุฑุฉ Huber

ุงูุขูุ ูููููุง ุชุทุจูู ุงููุฒูู ุงูุงุดุชูุงูู ููุชูููู ูู ุฏุงูุฉ ุงูุฎุณุงุฑุฉ Huber ุนูู ุจูุงูุงุช ุงูุฅูุฑุงููุงุช.

```python
tips = sns.load_dataset('tips')
tips['pcttip'] = tips['tip'] / tips['total_bill'] * 100
```

ุฏุงูุฉ ุงูุฎุณุงุฑุฉ Huber ุชุนุฑู ูุงูุชุงูู:

$$ \begin{split}
L_\delta(\theta, \textbf{y}) = \frac{1}{n} \sum_{i=1}^n \begin{cases}
    \frac{1}{2}(y_i - \theta)^2 &  | y_i - \theta | \le \delta \\
     \delta (|y_i - \theta| - \frac{1}{2} \delta ) & \text{otherwise}
\end{cases}
\end{split} $$

ูุงููุฒูู ุงูุงุดุชูุงูู ูุฏุงูุฉ Huber:

$$ \begin{split}
\nabla_{\theta} L_\delta(\theta, \textbf{y}) = \frac{1}{n} \sum_{i=1}^n \begin{cases}
    -(y_i - \theta) &  | y_i - \theta | \le \delta \\
    - \delta \cdot \text{sign} (y_i - \theta) & \text{otherwise}
\end{cases}
\end{split} $$

(ูุงุญุธ ุฃููุง ูู ุงูุชุนุงุฑูู ุงูุณุงุจูุฉ ูุฏุงูุฉ ุฎุณุงุฑุฉ Huber ุงุณุชุฎุฏููุง ุงููุชุบูุฑ $ \alpha $ ููุฅุดุงุฑุฉ ูููุทุฉ ุงูุงูุชูุงู. ููุฅุจุนุงุฏ ุงูุดู ุจูููุง ูุจูู $ \alpha $ ุงููุณุชุฎุฏูุฉ ูู ุงููุฒูู ุงูุงุดุชูุงููุ ูููุง ุจุชุบูุฑ ุฑูุฒ ููุทุฉ ุงูุงูุชูุงู ูู ุฏุงูุฉ ุงูุฎุณุงุฑุฉ Huber ุฅูู ุงูุฑูุฒ $ \delta $.)

```python
def huber_loss(theta, dataset, delta = 1):
    d = np.abs(theta - dataset)
    return np.mean(
        np.where(d <= delta,
                 (theta - dataset)**2 / 2.0,
                 delta * (d - delta / 2.0))
    )

def grad_huber_loss(theta, dataset, delta = 1):
    d = np.abs(theta - dataset)
    return np.mean(
        np.where(d <= delta,
                 -(dataset - theta),
                 -delta * np.sign(dataset - theta))
    )
```

ููููู ุจุงูุชูููู ูู ุฏุงูุฉ ุงูุฎุณุงุฑุฉ Huber ูู ุจูุงูุงุช ุงูุฅูุฑุงููุฉ:

```python
%%time
theta = minimize(huber_loss, grad_huber_loss, tips['pcttip'], progress=False)
print(f'Minimizing theta: {theta}')
print()
```

```ruby
Minimizing theta: 15.506849531471964

CPU times: user 194 ms, sys: 4.13 ms, total: 198 ms
Wall time: 208 ms
```

### ููุฎุต ุงููุฒูู ุงูุงุดุชูุงูู

ูููุฑ ููุง ุงููุฒูู ุงูุงุดุชูุงูู ุทุฑููู ุนุงููู ููุชูููู ูู ุฏุงูุฉ ุงูุฎุณุงุฑุฉ ุนูุฏูุง ูุง ูุณุชุทูุน ุฅูุฌุงุฏ ุงููููุฉ ุงูุฏููุง ู $ \theta $. ุนูุฏูุง ูููู ูููุฐุฌูุง ูุฏุงูุฉ ุงูุฎุณุงุฑุฉ ุฃูุซุฑ ุชุนููุฏุงูุ ูุณุชุฎุฏู ุงููุฒูู ุงูุงุดุชูุงูู ููุณููุฉ ูุถุจุท ุงูููุงุฐุฌ.

## ุงูุชุญุฏุจ

ูุณุงูู ุงููุฒูู ุงูุงุดุชูุงูู ุจุดูู ุนุงู ุจุงูุชูููู ูู ุฏุงูุฉ ุงูุฎุณุงุฑุฉ. ููุง ุงุธูุฑูุง ุฐูู ูู ุฏุงูุฉ Huber ููุฎุณุงุฑุฉุ ุชููู ูุงุฆุฏุฉ ุงููุฒูู ุงูุงุดุชูุงูู ุนูุฏูุง ูููู ุตุนุจ ุนูููุง ุฅูุฌุงุฏ ุงููููุฉ ุงูุฏููุง.

### ุงูุชุดุงู ุงูุญุฏูุฏ ุงูุฏููุง ุจุงููุฒูู ุงูุงุดุชูุงูู

ููุฃุณูุ ูู ุจุนุถ ุงูุฃุญูุงู ูุง ูููู ูููุฒูู ุงูุงุดุชูุงูู ุฅูุฌุงุฏ ูููุฉ $ \theta $. ูููุชุฑุถ ุงูุชุงูู $ \theta = -21 $:

```python
pts = np.array([0])
plot_loss(pts, (-23, 25), quartic_loss)
plot_theta_on_loss(pts, -21, quartic_loss)
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_convexity_5_0.png'>
</p>

```python
plot_one_gd_iter(pts, -21, quartic_loss, grad_quartic_loss)
```

```ruby
old theta: -21
new theta: -9.944999999999999
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_convexity_6_1.png'>
</p>

```python
plot_one_gd_iter(pts, -9.9, quartic_loss, grad_quartic_loss)
```

```ruby
old theta: -9.9
new theta: -12.641412
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_convexity_7_1.png'>
</p>

```python
plot_one_gd_iter(pts, -12.6, quartic_loss, grad_quartic_loss)
```

```ruby
old theta: -12.6
new theta: -14.162808
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_convexity_8_1.png'>
</p>

```python
plot_one_gd_iter(pts, -14.2, quartic_loss, grad_quartic_loss)
```

```ruby
old theta: -14.2
new theta: -14.497463999999999
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_convexity_9_1.png'>
</p>



> ูู ุงูุฃูุซูุฉ ุงูุณุงุจูุฉ ุงุณุชุฎุฏู ุงููุงุชุจ ุงูุฏูุงู ุงูุชุงููุฉ (ุฏุงูุฉ ุงูุฎุณุงุฑุฉ ุงูุฑุจุงุนูุฉ ููุฒูููุง ุงูุงุดุชูุงูู) ุงูุชู ูู ูุนุฑููุง ูุณุจูุงู:
> 
> ```python
> def quartic_loss(theta, y_vals):
>     return np.mean(1/5000 * (y_vals - theta + 12) * (y_vals - theta + 23)
>                   * (y_vals - theta - 14) * (y_vals - theta - 15) + 7)
> 
> def grad_quartic_loss(theta, y_vals):
>     return -1/2500 * (2 *(y_vals - theta)**3 + 9*(y_vals - theta)**2
>                      - 529*(y_vals - theta) - 327)
> ```
>

ูู ุงููุซุงู ุงูุณุงุจูุ ุฏุงูุฉ ุงูุฎุณุงุฑุฉ ุงูุฑุจุงุนูุฉุ ูุงูุช ูุชูุฌุฉ ุงููุฒูู ุงูุงุดุชูุงูู ูุฑูุจู ุฅูู $ \theta = -14.5 $ุ ูููู ุงููููุฉ ุงูุฏููุง ุงูุนุงูุฉ ูุฏุงูุฉ ุงูุฎุณุงุฑุฉ ูู $ \theta = 18 $ุ ูู ูุฐุง ุงููุซุงู ูุฑู ุฃู ุงููุฒูู ุงูุงุดุชูุงูู ูุจุญุซ ุนู *ุงููููุฉ ุงูุฏููุง ุงููุญููุฉ Local Minimum* ูุงูุชู ููุณุช ุฏุงุฆูุงู ุชุณุงูู ูููุฉ ุงูุฎุณุงุฑุฉ *ูููููุฉ ุงูุฏููุง ุงูุนุงูุฉ Global Minimum*.

ูุญุณู ุงูุญุธุ ุจุนุถ ุฏูุงู ุงูุฎุณุงุฑุฉ ูุฏููุง ููุณ ุงูุฑูู ูููููุฉ ุงูุฏููุง ุงููุญููุฉ ูุงูุนุงูุฉ. ููุฃุฎุฐ ูุซูุงู ุฏุงูุฉ ุงูุฎุทุฃ ุงูุชุฑุจูุนู ุงููุชูุณุท MSE:

```python
pts = np.array([-2, -1, 1])
plot_loss(pts, (-5, 5), mse)
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_convexity_11_0.png'>
</p>

ุชุทุจูู ุงููุฒูู ุงูุงุดุชูุงูู ุนูู ูุฐู ุงูุฏุงูุฉ ุณููุฌุฏ ููุง ุฏุงุฆูุงู ูููู ูุซุงููู ุนุงูู ู $ \theta $ ููู ุงููููุฉ ุงูุฏููุง ุงููุญููุฉ ุงููุญูุฏุฉ ูู ููุณูุง ุงูุนุงูุฉ.

ูุชูุณุท ุงูุฎุทุฃ ุงูุญุชูู ูุฏ ูุญุชูู ุนูู ุฃูุซุฑ ูู ูููู ุฏููุง ูุญููุฉ. ููููุ ูู ุงูููู ุงูุฏููุง ูู ููุณูุง ุงููููุฉ ุงูุฏููุง ุงูุนุงูุฉ.

```python
pts = np.array([-1, 1])
plot_loss(pts, (-5, 5), abs_loss)
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_convexity_13_0.png'>
</p>

> ุชุนุฑูู ุฏุงูุฉ ูุชูุณุท ุงูุฎุทุฃ ุงูุญุชูู
> 
> ```python
> def abs_loss(theta, y_vals):
>     return np.mean(np.abs(y_vals - theta))
> ```
>

ูู ูุฐุง ุงููุซุงูุ ุณุชููู ูููุฉ ุงููุฒูู ุงูุงุดุชูุงูู ุฅุญุฏู ุงูููู ุงููุญููุฉ ุงูุฏููุง ุจูู $ [-1, 1] $ ููู ุฃู ูู ุงูููู ูู ูุฐุง ุงููุทุงู ููู ุฏููุง ูุฏุงูุฉ ุงูุฎุณุงุฑุฉ ูุฐูุ ุณููุชุฑุญ ุงููุฒูู ุงูุงุดุชูุงูู ูููุฉ ุฏููุง ูุซุงููู ุจูู ูุฐู ุงูููุงุท ู $ \theta $.
### ุชุนุฑูู ุงูุชุญุฏุจ

ูู ุจุนุถ ุงูุฏูุงูุ ุฃู ูููู ุฏููุง ูุญููุฉ ูู ููุณูุงุงูุนุงูู. ูุฐู ุงูุฏูุงู ูุทูู ุนูููุง **ุฏูุงู ูุญุฏุจุฉ Convex function** ูุฃููุง ููุญููุฉ ููุฃุนูู. ูุฐูู ุฏุงูุฉ Huber ููุฎุณุงุฑุฉุ ุงููููุฐุฌ ุงูุซุงุจุชุ MSE ู MAE ุฌููุนูุง ูุญุฏุจุฉ.

ูุน ูุนุฏู ุชุนูู Learning Rate ููุงุณุจุ ุงููุฒูู ุงูุงุดุชูุงูู ููุฌุฏ $ \theta $ ุงูุนุงูุฉ ุงููุซุงููุฉ ูุฏุงูุฉ ุงูุฎุณุงุฑุฉ ุงููุญุฏุจุฉ. ูุจุณุจุจ ุฐููุ ููุถู ุถุจุท ููุงุฐุฌูุง ุจุงุณุชุฎุฏุงู ุงูุฏูุงู ุงููุญุฏุจุฉ ุฅูุง ุฅุฐุง ูุงู ูุฏููุง ุณุจุจ ููุงุณุจ ุบูุฑ ุฐูู.

ุจุดููู ุนุงูุ ุงูุฏุงูุฉ $ f $ ูุทูู ุนูููุง ุฏุงูุฉ ูุญุฏุจุฉ ููุท ุฅุฐุง ูุงูุช ุชููู ุดุฑูุท ุงููุชุจุงููุฉ ูุฌููุน ูุฏุฎูุงุชูุง $ a $ ู $ b $ ุ ููู $ t \in [0, 1] $: [๐][Inequality]

$$ tf(a) + (1-t)f(b) \geq f(ta + (1-t)b) $$

ุงููุชุจุงููุฉ ุชููู ุฅู ุฌููุน ุงูุฎุทูุท ุงูุชู ุชุฑุจุท ููุทุชูู ูู ุงูุฏุงูุฉ ูุฌุจ ุฃู ุชูุน ุนูู ุฃู ููู ุงูุฏุงูุฉ. ูุฏุงูุฉ ุงูุฎุณุงุฑุฉ ุงูุชู ุนุฑุถูุงูุง ูู ุจุฏุงูุฉ ูุฐุง ุงูุฌุฒุกุ ูููููุง ุฅูุฌุงุฏ ูุฐู ุงูุฎุท:

```python
pts = np.array([0])
plot_loss(pts, (-23, 25), quartic_loss)
plot_connected_thetas(pts, -12, 12, quartic_loss)
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_convexity_17_0.png'>
</p>

> ุงุณุชุฎุฏู ุงููุงุชุจ ุงูุฏุงูุฉ plot_connected_thetas ูุชุณุงุนุฏู ุนูู ุฑุณู ุงูุฎุท ุจูู ุงูููุทุชููุ ูุนุฑููุง ุงููุงุชุจ ูุงูุชุงูู:
>
> ```python
> def plot_connected_thetas(y_vals, theta_1, theta_2, loss_fn, **kwargs):
>     plot_theta_on_loss(y_vals, theta_1, loss_fn)
>     plot_theta_on_loss(y_vals, theta_2, loss_fn)
>     loss_1 = loss_fn(theta_1, y_vals)
>     loss_2 = loss_fn(theta_2, y_vals)
>     plt.plot([theta_1, theta_2], [loss_1, loss_2])
> ```
>

ูุจูุงุกูุง ุนูู ุงูุชุนุฑูู ุงูุณุงุจูุ ููุฐู ุงูุฏุงูุฉ ุบูุฑ ูุญุฏุจุฉ.

ููุฎุทุฃ ุงูุชุฑุจูุนู ุงููุชูุณุทุ ุฌููุน ุงูุฎุทูุท ุงูุชู ููุทุชูู ุชุธูุฑ ููู ุงูุฑุณู ุงูุจูุงูู. ูููููุง ุฑุณู ุฅุญุฏุงูุง ูุงูุชุงูู:

```python
pts = np.array([0])
plot_loss(pts, (-23, 25), mse)
plot_connected_thetas(pts, -12, 12, mse)
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_convexity_20_0.png'>
</p>

ุงูุชุนุฑูู ุงูุฑูุงุถู ููุชุญุฏุจ ูุนุทููุง ูุตู ุฏููู ูุชุญุฏูุฏ ูุง ุงุฐุง ูุงูุช ุงูุฏุงูู ูุญุฏุจู ุฃู ูุง. ูู ูุฐุง ุงููุชุงุจุ ุณูุชุฌุงูู ุงูุฌุฒุก ุงูุฑูุงุถู ูุฅุซุจุงุช ุงูุชุญุฏุจ ูููุชูู ุจุงูููู ูุง ุงุฐุง ูุงูุช ุฏุงูุฉ ูุญุฏุจู ุฃู ูุง.

### ููุฎุต ุงูุชุญุฏุจ

ููุฏุงูู ุงููุญุฏุจูุ ุฃู ูููู ุฏููุง ูุญููู ูู ููุณูุง ุนุงูู. ุฐูู ูุณูู ูููุฒูู ุงูุงุดุชูุงูู ุฅูุฌุงุฏ ุงูุถู ุงููุชุบูุฑุงุช ููููุงุฐุฌ ูุฃู ุฏุงูุฉ ุฎุณุงุฑุฉ. ุนูู ุงูุฑุบู ูู ุงูู ุจุฅููุงููุง ุงุณุชุฎุฏุงู ุงููุฒูู ุงูุงุดุชูุงูู ูุฏูุงู ุงูุฎุณุงุฑุฉ ุบูุฑ ุงููุญุฏุจู ูุฅูุฌุงุฏ ููู ุฏููุงุ ุชูู ุงูููู ุงูุฏููุง ุงููุญููู ุบูุฑ ูุถููู ุฃููุง ุฏุงุฆูุงู ูู ุงูููู ุงูุนุงูู ุงููุซุงููู.

## ุงููุฒูู ุงูุงุดุชูุงูู ุงูุนุดูุงุฆู

### ููุฏูุฉ 

ูู ูุฐุง ุงูุฌุฒุกุ ุณูุชุญุฏุซ ุนู ุชุนุฏูู ุนูู ุงููุฒูู ุงูุงุดุชูุงูู ูุฌุนูู ุฃูุซุฑ ูุงุฆุฏุฉ ููุจูุงูุงุช ุฐุงุช ุงูุญุฌู ุงููุจูุฑ. ูุฐุง ุงูุชุนุฏูู ูุทูู ุนููู ุฎูุงุฑุฒููุฉ **ุงููุฒูู ุงูุงุดุชูุงูู ุงูุนุดูุงุฆู Stochastic Gradient Descent**.

ุจุนุฏ ุฃู ุชุนูููุง ุทุฑููุฉ ุนูู ุงููุฒูู ุงูุงุดุชูุงูู ูุชุญุฏูุซู ููููุฉ $ \theta $ ุจุงุณุชุฎุฏุงู ุงูุงุดุชูุงู ูุฏุงูุฉ ุงูุฎุณุงุฑุฉ. ุจุงูุฐุงุช ุงุณุชุฎุฏููุง ุงููุนุงุฏูุฉ ุงูุชุงููุฉ:

$$ {\theta}^{(t+1)} = \theta^{(t)} - \alpha \cdot \nabla_{\theta} L(\theta^{(t)}, \textbf{y}) $$

ูู ูุฐู ุงููุนุงุฏูุฉ: 

- $ \theta^{(t)} $ ูู ุงูุชููุน ุงูุญุงูู ู $ \theta^{*} $ ูู ุงูุชูุฑุงุฑ $ t $.
- $ \alpha $ ูู ูุนุฏู ุงูุชุนููู Learning Rate.
- $ \nabla_\theta L(\theta^{(t)}, \textbf{y}) $ ูู ุงุดุชูุงู ุฏุงูุฉ ุงูุฎุณุงุฑุฉ.
- ููุญุณุจ ุงูุชููุน ุงูุชุงูู $ \theta^{(t+1)} $ ุนู ุทุฑูู ุทุฑุญ $ \alpha $ ู $ \nabla_\theta L(\theta, \textbf{y}) $ ูุงููุญุณูุจุฉ ูู $ \theta^{(t)} $.

##### ุญุฏูุฏ ุงููุฒูู ุงูุงุดุชูุงูู

ูู ุงููุนุงุฏูุฉ ุงูุณุงุจูุฉุ ูููุง ุจุญุณุงุจ $ \nabla_\theta L(\theta, \textbf{y}) $ ุจุงุณุชุฎุฏุงู ูุชูุณุท ุงูุงุดุชูุงู ูุฏุงูุฉ ุงูุฎุณุงุฑุฉ $ \ell(\theta, y_i) $ ูุฌููุน ุงูุจูุงูุงุช. ุจูุนูู ุขุฎุฑุ ูู ูู ูุฑุฉ ูุญุฏุซ ูููุฉ $ \theta $ ูุชุญูู ูู ุฌููุน ุงูููุงุท ุงูุฃุฎุฑู ูู ุจูุงูุงุชูุง. ููุฐุง ุงูุณุจุจุ ุงููุงููู ููุงุดุชูุงู ูู ุงููุนุงุฏูุฉ ุงูุณุงุจูุฉ ูุทูู ุนููู **ุงููุฒูู ุงูุงุดุชูุงูู ุงูููุฌููุน Batch Gradient Descent**.

ููุฃููุง ูุณูุก ุงูุญุธ ุนุงุฏุฉ ูุง ูุนูู ูุน ุจูุงูุงุช ูุจูุฑุฉ ุงูุญุฌูุ ูุฅู ุงููุฒูู ุงูุงุดุชูุงูู ุงูููุฌููุน ุณูุนูู ูุฅูุฌุงุฏ ุงููููุฉ ุงูููุงุณุจุฉ ู $ \theta $ ุจุนุฏ ุจุถุน ุชูุฑุงุฑุงุชุ ูููู ูู ุชูุฑุงุฑ ูุฏ ูุฃุฎุฐ ููุชุงู ุทููู ูุญุณุงุจ ุงููุชูุฌุฉ ููู ุฅุฐุง ูุงูุช ุงูููุงุท ูู ุจูุงูุงุชูุง ูุซูุฑู.

#### ุงููุฒูู ุงูุงุดุชูุงูู ุงูุนุดูุงุฆู

ูุญู ูุดููุฉ ุงูููุช ูู ุญุณุงุจ ุงูุงุดุชูุงู ูุฌููุน ุจูุงูุงุช ุงูุชุฏุฑูุจุ ูููู ุงููุฒูู ุงูุงุดุชูุงูู ุงูุนุดูุงุฆู ุจุชููุน ุงููููุฉ ุจุงุณุชุฎุฏุงู **ูููู ุนุดูุงุฆูุฉ ูุงุญุฏุฉ ูู ุงูุจูุงูุงุช**. ููุฃู ุงููููุฉ ุชู ุงุฎุชูุงุฑูุง ุจุดูู ุนุดูุงุฆูุ ูุชููุน ุฃู ุงูุงุดุชูุงู ููู ููุทุฉ ุณูุฃุฏู ุจุงูููุงูุฉ ุฅูู ููุณ ุงููุชูุฌุฉ ูููุฒูู ุงูุงุดุชูุงูู ุงูููุฌููุน.

ููุนูุฏ ูุฑุฉ ุฃุฎุฑู ููุนุงุฏูุฉ ุงููุฒูู ุงูุงุดุชูุงูู ุงูููุฌููุน:

$$ {\theta}^{(t+1)} = \theta^{(t)} - \alpha \cdot \nabla_{\theta} L(\theta^{(t)}, \textbf{y}) $$

ูู ูุฐู ุงููุนุงุฏูุฉุ ูุฏููุง ุงููุตุทูุญ $ \nabla_{\theta} L(\theta^{(t)}, \textbf{y}) $ุ ูุชูุณุท ุงูุงุดุชูุงู ูุฏุงูุฉ ุงูุฎุณุงุฑุฉ ุจูู ูู ุงูููุงุท ูู ุจูุงูุงุช ุงูุชุฏุฑูุจุ ููุนุงุฏูุชูุง:

$$ \begin{aligned}
\nabla_{\theta} L(\theta^{(t)}, \textbf{y}) &= \frac{1}{n} \sum_{i=1}^{n} \nabla_{\theta} \ell(\theta^{(t)}, y_i)
\end{aligned} $$

ููููุง $ \ell(\theta, y_i) $ ูู ุงูุฎุณุงุฑุฉ ูู ููุทุฉ ูุนููุฉ ูู ุจูุงูุงุช ุงูุชุฏุฑูุจ. ูุชุทุจูู ุงููุฒูู ุงูุงุดุชูุงูู ุงูุนุดูุงุฆูุ ุจุจุณุงุทู ูููู ุจุชุบูุฑ ูุชูุณุท ุงูุงุดุชูุงู ุจ ุงูุงุดุชูุงู ูู ููุทุฉ ูุนููุฉ. ุงููุนุงุฏูุฉ ุจุนุฏ ุงูุชุนุฏูู ูููุฒูู ุงูุงุดุชูุงูู ุงูุนุดูุงุฆู ูู:

$$ {\theta}^{(t+1)} = \theta^{(t)} - \alpha \cdot \nabla_{\theta} \ell(\theta^{(t)}, y_i) $$ 

ูู ูุฐู ุงููุนุงุฏูุฉุ $ y_i $ ูุชู ุงุฎุชูุงุฑูุง ุจุดูู ุนุดูุงุฆู ูู $ \textbf{y} $. ูุงุญุธ ุฃู ุงุฎุชูุงุฑ ุงูููุงุท ุจุดูู ุนุดูุงุฆู ููู ุฌุฏุงู ููุฌุงุญ ุงููุฒูู ุงูุงุดุชูุงูู ุงูุนุดูุงุฆู! ุฅุฐุง ูู ูุชู ุงุฎุชูุงุฑ ุงูููุงุท ุจุดูู ุนุดูุงุฆูุ ูุฏ ููุชุฌ ููุง ูุชุงุฆุฌ ุฃุณูุฃ ูู ูุชุงุฆุฌ ุงููุฒูู ุงูุงุดุชูุงูู ุงูููุฌููุน.

ูููู ุนุงุฏุฉู ุจุงุณุชุฎุฏุงู ุงููุฒูู ุงูุงุดุชูุงูู ุงูุนุดูุงุฆู ุนู ุทุฑูู ุฎูุท ุงูุจูุงูุงุช ูุงุณุชุฎุฏุงู ูู ููุทุฉ ุจุนุฏ ุงูุฎูุท ุญุชู ุชุชุฌุงูุฒ ุฃุญุฏ ุงูููุงุท ุจูุงูุงุช ุงูุชุฏุฑูุจ. ุฅุฐุง ูู ูุชู ุฐููุ ูุนูุฏ ุฎูุท ุงูููุงุท ูุงูููุงู ุจููุณ ุงูุฎุทูุงุช ุญุชู ุชุชุฌุงูุฒ ุงูุจูุงูุงุช. ูู ูู **ุชูุฑุงุฑ Iteration** ุงููุฒูู ุงูุงุดุชูุงูู ุงูุนุดูุงุฆู ูุชุญูู ูู ููุทุฉ ูุงุญุฏูุ ููู ุนูููุฉ ุชุฌุงูุฒ ุชุชู ุจูุฌุงุญ ูุทูู ุนูููุง **Epoch**.

#### ุงุณุชุฎุฏุงู ุฏุงูุฉ ุงูุฎุณุงุฑุฉ MSE

ููุซุงูุ ููุทุจู ุงููุฒูู ุงูุงุดุชูุงูู ุงูุนุดูุงุฆู ุนูู ุฏุงูุฉ ุงูุฎุณุงุฑุฉ ูMSE. ููุชุฐูุฑ ุชุนุฑูููุง:

$$ \begin{aligned}
L(\theta, \textbf{y})
&= \frac{1}{n} \sum_{i = 1}^{n}(y_i - \theta)^2
\end{aligned} $$

ุฃุฎุฐ ุงูุงุดุชูุงู $ \theta $ ูุตุจุญ ูุฏููุง:

$$ \begin{aligned}
\nabla_{\theta}  L(\theta, \textbf{y})
&= \frac{1}{n} \sum_{i = 1}^{n} -2(y_i - \theta)
\end{aligned} $$

ุจูุง ุฃู ุงููุนุงุฏูุฉ ุงูุณุงุจูุฉ ุชุนุทููุง ูุชูุณุท ุฎุณุงุฑุฉ ุงูุงุดุชูุงู ููู ุงูููุงุท ูู ุงูุจูุงูุงุชุ ูุฅู ุฎุณุงุฑุฉ ุงูุงุดุชูุงู ูููุทุฉ ูุนููุฉ ูู ุจุจุณุงุทู ุฌุฒุก ุงููุนุงุฏูุฉ ุงูุชู ุชู ุฃุฎุฐ ูุชูุณุทู:

$$ \begin{aligned}
\nabla_{\theta}  \ell(\theta, y_i)
&= -2(y_i - \theta)
\end{aligned} $$

ูุชุญุฏูุซูุง ููุงุดุชูุงู ุงูููุฌููุน ูุฏุงูุฉ ุงูุฎุณุงุฑุฉ MSE:

$$ \begin{aligned}
{\theta}^{(t+1)} = \theta^{(t)} - \alpha \cdot \left( \frac{1}{n} \sum_{i = 1}^{n} -2(y_i - \theta) \right)
\end{aligned} $$

ูุงููุฒูู ุงูุงุดุชูุงูู ุงูุนุดูุงุฆู ููุง ุณูููู ูุงูุชุงูู:

$$ \begin{aligned}
{\theta}^{(t+1)} = \theta^{(t)} - \alpha \cdot \left( -2(y_i - \theta) \right)
\end{aligned} $$

### ุณููู ุงููุฒูู ุงูุงุดุชูุงูู ุงูุนุดูุงุฆู

ุจูุง ุฃู ุงูุงุดุชูุงู ุงูุนุดูุงุฆู ููุท ูุชุญูู ูู ููุทุฉ ูุงุญุฏุฉ ูู ูุฑุฉุ ูุฅู ุชุญุฏูุซู ููููุฉ $ \theta $ ุณูููู ุฃูู ุฏูู ูู ุงูุชุญุฏูุซ ุฅุฐุง ุชู ูู ุงููุฒูู ุงูุนุดูุงุฆู ุงูููุฌููุน. ููููุ ุจูุง ุฃูู ุฃุณุฑุน ูู ุญุณุงุจ ุงููุชุงุฆุฌุ ูุฅู ุงููุฒูู ุงูุงุดุชูุงูู ุงูุนุดูุงุฆู ุจุฅููุงูู ุงูุชูุฏู ุจุดูู ูุจูุฑ ูููุตูู ูููููุฉ ุงูููุงุณุจุฉ ู $ \theta $ ูู ุญูู ุงููุฒูู ุงูุนุดูุงุฆู ุงูููุฌููุน ูู ููุชูู ููุชูุง ูู ุงูุชุญุฏูุซ ููุง ูุฑุฉ ูุงุญุฏุฉ.
ุงูุตูุฑุฉ ูู ุงูุฃุณูู ุชูุถุญ ุชุญุฏูุซุงุช ุชูุช ุจูุฌุงุญ ููููุฉ $ \theta $ ุจุงุณุชุฎุฏุงู ุงููุฒูู ุงูุงุดุชูุงูู ุงูููุฌููุน. ุงููุณุงุญุฉ ุบุงููุฉ ุงูููู ูู ุงูุตูุฑู ุชุนูู ุงููููุฉ ุงููุซุงููุฉ ู $ \theta $ ุนูู ุจูุงูุงุช ุงูุชุฏุฑูุจุ ููู $ \hat{\theta} $.
(ุงูุตูุฑุฉ ุชุธูุฑ ูููุฐุฌ ูุฏูู ูุชุบูุฑุงูุ ูููู ูู ุงูููู ููุงุญุธุฉ ุทุฑููุฉ ุงููุฒูู ุงูุงุดุชูุงูู ุงูููุฌููุน ุงูุชู ูุตู ูููุง ู $ \hat{\theta} $.)

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_stochastic_gd.png'>
</p>

ูู ุงูุฌุงูุจ ุงูุขุฎุฑุ ุงููุฒูู ุงูุงุดุชูุงูู ุงูุนุดูุงุฆูุ ุนุงุฏุฉ ูุง ูุฃุฎุฐ ุฎุทูู ุจุนูุฏุงู ุนู $ \hat{\theta} $ุ ูููู ูููู ูููู ุจุงูุชุญุฏูุซ ุจุดูู ูุชูุฑุฑุ ูุตู ุบุงูุจุงู ููููุทุฉ ุงููุซุงููุฉ ุจุดูู ุฃุณุฑุน ูู ุงูููุฌููุน.

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_stochastic_sgd.png'>
</p>

### ุชุนุฑูู ุฏุงูุฉ ูููุฒูู ุงูุงุดุชูุงูู ุงูุนุดูุงุฆู

ููุง ูุนููุง ุณุงุจูุงูุ ูููู ุจุชุนุฑูู ุฏุงูุฉ ุชุญุณุจ ููุง ุงููุฒูู ุงูุงุดุชูุงูู ุงูุนุดูุงุฆู ูุฏุงูุฉ ุฎุณุงุฑุฉ. ุณุชููู ูุดุงุจูุฉ ูุฏุงูุฉ `minimize` ุงูุชู ุณุจู ุชุนุฑูููุงุ ูููู ูุญุชุงุฌ ูุฅุถุงูุฉ ุงูุงุฎุชูุงุฑ ุงูุนุดูุงุฆู ููููู ูู ูู ุชูุฑุงุฑ:

```python
def minimize_sgd(loss_fn, grad_loss_fn, dataset, alpha=0.2):
    """
    ุชุณุชุฎุฏู ุงููุฒูู ุงูุงุดุชูุงูู ุงูุนุดูุงุฆู ููุชูููู ูู ุฏุงูุฉ ุงูุฎุณุงุฑุฉ loss_fn
    ุชููู ุงููุชูุฌุฉ ุงููููู ุงูุตุบุฑู ู ฮธ ุนูุฏูุง ูููู
    ุงููุฑู ุจูู ุงูุชูุฑุงุฑุงุช ุงูู ูู 0.001
    """
    NUM_OBS = len(dataset)
    theta = 0
    np.random.shuffle(dataset)
    while True:
        for i in range(0, NUM_OBS, 1):
            rand_obs = dataset[i]
            gradient = grad_loss_fn(theta, rand_obs)
            new_theta = theta - alpha * gradient
        
            if abs(new_theta - theta) < 0.001:
                return new_theta
        
            theta = new_theta
        np.random.shuffle(dataset)
```

### ูุฒูู ุงุดุชูุงูู ุจุฏูุนุงุช ุตุบูุฑุฉ

**ุงููุฒูู ุงูุงุดุชูุงูู ุจุฏูุนุงุช ุตุบูุฑุฉ Mini-batch Gradient Descent** ูุญุงูู ุฃู ููุงุฒู ุจูู ุงููุฒูู ุงูุงุดุชูุงูู ุงูุนุดูุงุฆู ู ุงูููุฌููุน ุนู ุทุฑููู ุฒูุงุฏุฉ ุนุฏุฏ ุงูุฃุฑูุงู ุงูุชู ูุชุทูุน ุนูููุง ูู ูู ุนูููุฉ ุชูุฑุงุฑ. ูู ุงููุฒูู ุงูุงุดุชูุงูู ุจุฏูุนุงุช ุตุบูุฑุฉุ ูุณุชุฎุฏู ุนุฏุฏ ูู ุงูููุงุท ูู ูู ุชุญุฏูุซ ุจุฏูุงู ูู ููุทุฉ ูุงุญุฏุฉ.
ูุณุชุฎุฏู ูุชูุณุท ุงูุงุดุชูุงู ูุฏูุงู ุงูุฎุณุงุฑุฉ ููููุงู ุจุชููุน ููููุฉ ุงูุงุดุชูุงู ุงูุตุญูุญุฉ ูุฎุณุงุฑุฉ ุงูุงูุชุฑูุจูุง ุงูุชูุงุทุนูุฉ Cross Entropy Loss. ุฅุฐุง ูุงูุช $ \mathcal{B} $ ูู ุงูุฏูุนุฉ ุงูุตุบูุฑุฉ ูู ุงูููุงุท ุงูุชู ูุฎุชุงุฑูุง ุจุดูู ุนุดูุงุฆู ูู $ n $ุ ูุงููุนุงุฏูุฉ ุงูุญุงููุฉ ูุงูุชุงูู:

$$ \nabla_\theta L(\theta, \textbf{y}) \approx \frac{1}{|\mathcal{B}|} \sum_{i\in\mathcal{B}}\nabla_{\theta}\ell(\theta, y_i) $$

ููุง ูู ุงููุฒูู ุงูุงุดุชูุงูู ุงูุนุดูุงุฆูุ ูููู ุจุงููุฒูู ุงูุงุดุชูุงูู ุจุฏูุนุงุช ุตุบูุฑุฉ ุนู ุทุฑูู ุฎูุท ุจูุงูุงุช ุงูุชุฏุฑูุจ ูุงุฎุชูุงุฑ ุฏูุนุงุช ุนู ุทุฑูู ุงูุชูุฑุงุฑ ุฏุงุฎู ุงูุจูุงูุงุช ุงููุฎููุทุฉ. ุจุนุฏ ูู Epochุ ูุนูุฏ ุฎูุท ุงูุจูุงูุงุช ูุงุฎุชูุงุฑ ุฏูุนู ุตุบูุฑุฉ ุฌุฏูุฏู.

ุนูู ุงูุฑุบู ูู ุฃููุง ูุฑููุง ุจูู ุงููุฒูู ุงูุงุดุชูุงูู ุงูุนุดูุงุฆู ูุจุฏูุนุงุช ุตุบูุฑุฉ ูู ูุฐู ุงููุชุงุจุ ูุณุชุฎุฏู ูุตุทูุญ ุงููุฒูู ุงูุงุดุชูุงูู ุงูุนุดูุงุฆู ุจุดูู ุนุงู ููุงุดุชูุงูุงุช ุจุฏูุนุงุช ุตุบูุฑุฉ ุจุฃู ุญุฌู.

#### ุงุฎุชูุงุฑ ุญุฌู ุงูุฏูุนุงุช ุงูุตุบูุฑุฉ

ูููู ุงููุฒูู ุงูุงุดุชูุงูู ุจุฏูุนุงุช ุตุบูุฑุฉ ูุซุงููุงู ุนูุฏูุง ูุนูู ุนูู ูุญุฏุฉ ุงููุนุงูุฌุฉ ุงูุฑุณูููู GPU (ูุฑุช ุงูุดุงุดุฉ ููุญุงุณุจ). ููู ุงูุนูููุงุช ูู ูุฐุง ุงูููุน ุชุฃุฎุฐ ููุชุงู ุทูููุ ุงุณุชุฎุฏุงู ุงูุฏูุนุงุช ุงูุตุบูุฑุฉ ูุฒูุฏ ูู ุฏูุฉ ุงูุงุดุชูุงู ุฏูู ุงูุฒูุงุฏุฉ ูู ุณุฑุนุฉ ุนูููุฉ ุงูุญุณุงุจ. ุจูุงุกูุง ุนูู ุฐุงูุฑุฉ ูุญุฏุฉ ุงููุนุงูุฌุฉ ุงูุฑุณูููุฉ ูู ุงูุฌูุงุฒุ ูุชู ุชุญุฏูุฏ ุญุฌู ุงูุฏูุนุงุช ุจูู 10 ู 100.

### ุชุนุฑูู ุฏุงูุฉ ูููุฒูู ุงูุงุดุชูุงูู ุจุฏูุนุงุช ุตุบูุฑุฉ

ุฏุงูุฉ ุงููุฒูู ุงูุงุดุชูุงูู ุจุฏูุนุงุช ุตุบูุฑุฉ ุชุญุชุงุฌ ูุฎูุงุฑ ูุชุญุฏูุฏ ุญุฌู ุงูุฏูุนุงุช. ุงูุฏุงูุฉ ุงูุชุงููุฉ ุชููุฑ ูุฐู ุงูุฎุงุตูุฉ:

```python
def minimize_mini_batch(loss_fn, grad_loss_fn, dataset, minibatch_size, alpha=0.2):
    """
    ุชุณุชุฎุฏู ุงููุฒูู ุงูุงุดุชูุงูู ุงูุนุดูุงุฆู ุจุฏูุนุงุช ุตุบูุฑู ููุชูููู ูู ุฏุงูุฉ ุงูุฎุณุงุฑุฉ loss_fn
    ุชููู ุงููุชูุฌุฉ ุงููููู ุงูุตุบุฑู ู ฮธ ุนูุฏูุง ูููู
    ุงููุฑู ุจูู ุงูุชูุฑุงุฑุงุช ุงูู ูู 0.001
    """
    NUM_OBS = len(dataset)
    assert minibatch_size < NUM_OBS
    
    theta = 0
    np.random.shuffle(dataset)
    while True:
        for i in range(0, NUM_OBS, minibatch_size):
            mini_batch = dataset[i:i+minibatch_size]
            gradient = grad_loss_fn(theta, mini_batch)
            new_theta = theta - alpha * gradient
            
            if abs(new_theta - theta) < 0.001:
                return new_theta
            
            theta = new_theta
        np.random.shuffle(dataset)
```

### ููุฎุต ุงููุฒูู ุงูุงุดุชูุงูู ุงูุนุดูุงุฆู

ูุณุชุฎุฏู ุงููุฒูู ุงูุงุดุชูุงูู ุจุฏูุนุงุช ูุชุญุณูู ุงููููุฐุฌ ุจุดูู ูุชูุฑุฑ ุญุชู ูุตู ุฅูู ุงููููุฉ ุงูุฏููุง ููุฎุณุงุฑุฉ. ุจูุง ุฃู ุงููุฒูู ุงูุงุดุชูุงูู ุจุฏูุนุงุช ูููู ุตุนุจุงู ููุญุณุงุจ ูู ุงูุจูุงูุงุช ุงููุจูุฑุฉุ ุนุงุฏุฉู ูุง ูุณุชุฎุฏู ุงููุฒูู ุงูุงุดุชูุงูู ุงูุนุดูุงุฆู ูุถุจุท ุชูู ุงูููุงุฐุฌ. ุนูุฏ ุงุณุชุฎุฏุงู GPUุ ุงููุฒูู ุงูุงุดุชูุงูู ุจุฏูุนุงุช ุจุณูุทู ููููู ุงูุญุณุงุจ ุจุดูู ุฃุณุฑุน ูู ุงูุนุดูุงุฆู ุจููุณ ุชูููุฉ ุงูุชุดุบูู. ููุจูุงูุงุช ุงููุจูุฑุฉุ ุงููุฒูู ุงูุงุดุชูุงูู ุงูุนุดูุงุฆู ูุจุฏูุนุงุช ุตุบูุฑุฉ ูู ุฃูุถู ููุงุณุชุฎุฏุงู ููููุง ุฃุณุฑุน ููุญุณุงุจ.

[GradientDescent]: https://machinelearningmastery.com/gradient-descent-for-machine-learning/
[Inequality]: https://www.mathsisfun.com/algebra/inequality.html