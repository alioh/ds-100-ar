---
title: ุงููุฒูู ุงูุฅุดุชูุงูู ูุชุญุณูู ุงููุชุงุฆุฌ ุงููููุฉ
show_title: true
chapter_number: 11
chapter_text: ุงููุตู ุงูุญุงุฏู ุนุดุฑ
chapter_lessons: [[0, 'ููุฏูุฉ'], [1, 'ุชูููู ุงูุฎุณุงุฑู ุจุฅุณุชุฎุฏุงู ุจุฑูุงูุฌ'], [2, 'ุงููุฒูู ุงูุฅุดุชูุงูู'], [3, 'ุงูุชุญุฏุจ'], [4, 'ุงููุฒูู ุงูุฅุดุชูุงูู ุงูุนุดูุงุฆู']]
chapter_sublessons: [
    [],
    ['ูุดุงูู simple_minimize'],
    ['ุงูููุฑู', 'ุชุญููู ุงููุฒูู ุงูุฅุดุชูุงูู', 'ุชุนุฑูู ุฏุงูุฉ minimize', 'ุชูููู ุฎุณุงุฑุฉ Huber', 'ููุฎุต ุงููุฒูู ุงูุฅุดุชูุงูู'],
    ['ุงูุชุดุงู ุงูุญุฏูุฏ ุงูุฏููุง ุจุงููุฒูู ุงูุฅุดุชูุงูู', 'ุชุนุฑูู ุงูุชุญุฏุจ', 'ููุฎุต ุงูุชุญุฏุจ'],
    [['ุงุณุชุฎุฏุงู ุฏุงูุฉ ุงูุฎุณุงุฑู MSE'], 'ุณููู ุงููุฒูู ุงูุฅุดุชูุงูู ุงูุนุดูุงุฆู', 'ุชุนุฑูู ุฏุงูุฉ ูููุฒูู ุงูุฅุดุชูุงูู ุงูุนุดูุงุฆู', 'ูุฒูู ุงุดุชูุงูู ุจุฏูุนุงุช ุตุบูุฑู', 'ุชุนุฑูู ุฏุงูุฉ ูููุฒูู ุงูุฅุดุชูุงูู ุจุฏูุนุงุช ุตุบูุฑู', ููุฎุต ุงููุฒูู ุงูุฅุดุชูุงูู ุงูุนุดูุงุฆู']
]
layout: default
---

## ููุฏูุฉ

ูุฅุณุชุฎุฏุงู ูุงุนุฏุฉ ุจูุงูุงุช ููุชูุจุค ูุงูุชููุนุ ูุฌุจ ุนูููุง ุชูููู ูููุฐุฌูุง ุจุดูู ุฏููู ูุฅุฎุชูุงุฑ ุฏุงูุฉ ุฎุณุงุฑู. ูุซูุงูุ ุจูุงูุงุช ุงูุฅูุฑุงููุงุชุ ูููุฐุฌูุง ุชููุน ุงู ูุณุจุฉ ุงูุฅูุฑุงููู ุซุงุจุชู ูุง ุชุชุบูุฑ. ุซู ูุฑุฑูุง ุฅุณุชุฎุฏุงู ุฏุงูุฉ ุงูุฎุทุฃ ุงูุชุฑุจูุนู ุงููุชูุณุท MSE ููุฌุฏูุง ุงููููุฐุฌ ุงูุฃูู ุฎุณุงุฑู.

ูุฌุฏูุง ุงูุถุงู ุงู ููุงู ูุตูุงู ุงุจุณุท ูุฏุงูุชู ุงูุฎุณุงุฑู ุงูุฎุทุฃ ุงูุชุฑุจูุนู ุงููุชูุณุท ู ูุชูุณุท ุงูุฎุทุฃ ุงูุญุชูู ููู: ุงููุชูุณุท ูุงููุณูุท. ููููุ ูููุง ูุงู ูููุฐุฌูุง ูุฏุงูุฉ ุงูุฎุณุงุฑู ุงูุซุฑ ุชุนููุฏุงู ูู ูุณุชุทูุน ุงูุฌุงุฏ ูุตูุงู ุฑูุงุถูุงู ููุงุณุจ. ูุซูุงูุ ุฏุงูุฉ Huber ูุฏููุง ุฎุตุงุฆุต ูููุฏู ูููู ุตุนุจ ุชูููุฒูุง.

ูููููุง ุงุณุชุฎุฏุงู ุงูููุจููุชุฑ ูุญู ูุฐู ุงููุดูู ุจูุงุณุทุฉ ุงููุฒูู ุงูุฅุดุชูุงููุ ุทุฑููู ุญุณุงุจูู ูุชูููู ุฏูุงู ุงูุฎุณุงุฑู.

## ุชูููู ุงูุฎุณุงุฑู ุจุฅุณุชุฎุฏุงู ุจุฑูุงูุฌ

ููุนูุฏ ูููููุฐุฌ ูู ุงููุตู ุงูุณุงุจู:

$$ \theta = C $$

ุณูุณุชุฎุฏู ุฏุงูุฉ ุงูุฎุณุงุฑู MSE:

$$ \begin{split}
\begin{aligned}
L(\theta, \textbf{y})
&= \frac{1}{n} \sum_{i = 1}^{n}(y_i - \theta)^2\\
\end{aligned}
\end{split} $$

ููุชุจุณูุทุ ุณูุณุชุฎุฏู ุงูุจูุงูุงุช ุงูุชุงููู: $ \textbf{y} = [ 12, 13, 15, 16, 17 ] $. ูุนูู ูู ุฎูุงู ุชุญููููุง ููุจูุงูุงุช ูู ุงููุตู ุงูุณุงุจู ุงู ูููุฉ $ \theta $ ูุฏุงูุฉ ุงูุฎุณุงุฑู MSE ูู ุงููุชูุณุท $ \text{mean}(\textbf{y}) = 14.6 $. ููุฑู ุงุฐุง ูุงู ุจุฅููุงููุง ุงูุญุตูู ุนูู ูุชูุฌู ุนูุฏ ูุชุงุจุชูุง ูุจุฑูุงูุฌ ููุฌุฏูุง.

ุงุฐุง ูููุง ุจูุชุงุจุฉ ุจุฑูุงูุฌ ุจุดูู ูุชููุ ูุจุฅููุงููุง ุงุณุชุฎุฏุงู ููุณ ุงูุจุฑูุงูุฌ ุนูู ุงู ุฏุงูุฉ ุฎุณุงุฑู ูุฅูุฌุงุฏ ุงูู ูููุฉ ู $ \theta $ุ ูุดูู ุฐูู ุฏุงูุฉ Huber ุงููุนูุฏู ุฑูุงุถูุงู:

$$ \begin{split}
L_\alpha(\theta, \textbf{y}) = \frac{1}{n} \sum_{i=1}^n \begin{cases}
    \frac{1}{2}(y_i - \theta)^2 &  | y_i - \theta | \le \alpha \\
    \alpha ( |y_i - \theta| - \frac{1}{2}\alpha ) & \text{otherwise}
\end{cases}
\end{split} $$

ุฃููุงูุ ูููู ุจุฑุณู ุชุฎุทูุทู ููุจูุงูุงุช. ุจุงูุฌุงูุจ ุงูุฃููู ูู ุงูุฑุณู ูููู ุจุฑุณู ุฏุงูุฉ ุงูุฎุณุงุฑู MSE ูููู ูุฎุชููู ู $ \theta $:

```python
pts = np.array([12, 13, 15, 16, 17])
points_and_loss(pts, (11, 18), mse)
```

> ูู ุงูููุฏ ุงูุจุฑูุฌู ุงูุณุงุจู ุงุณุชุฎุฏู ุงููุงุชุจ ุฏุงูุฉ ุนุฑููุง ูุณุจูุงู ุจุฃุณู `points_and_loss`ุ ุชูุจู ุงูุฏุงูุฉ ุซูุงุซ ูุชุบูุฑุงุชุ ุงูุฃููู ูู ุงูุจูุงูุงุช. ุงููุชุบูุฑ ุงูุซุงูู ูู ููุงุณุงุช ุงุจุนุงุฏ x-axis ุ ูุงููููู ุงูุฃุฎูุฑู ูู ููุน ุฏุงูุฉ ุงูุฎุณุงุฑูุ ูุงูุชู ูู ุนุจุงุฑู ุนู ุฏุงูู ุงุฎุฑู ุนุฑููุง ุงูุถุงู ุจุฃุณู `mse`. ุชุนุฑูู ููุง ุงูุฏุงูุชูู ูู ูุงูุชุงูู:
>
> ```python
> def mse(theta, y_vals):
>     return np.mean((y_vals - theta) ** 2)
> 
> def points_and_loss(y_vals, xlim, loss_fn):
>     thetas = np.arange(xlim[0], xlim[1] + 0.01, 0.05)
>     losses = [loss_fn(theta, y_vals) for theta in thetas]
>     
>     plt.figure(figsize=(9, 2))
>     
>     ax = plt.subplot(121)
>     sns.rugplot(y_vals, height=0.3, ax=ax)
>     plt.xlim(*xlim)
>     plt.title('Points')
>     plt.xlabel('Tip Percent')
>     
>     ax = plt.subplot(122)
>     plt.plot(thetas, losses)
>     plt.xlim(*xlim)
>     plt.title(loss_fn.__name__)
>     plt.xlabel(r'$ \theta $')
>     plt.ylabel('Loss')
>     plt.legend()
> ```
> 

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_basics_3_0.png'>
</p>

ููู ุจุฅููุงููุง ุจุฑูุฌุฉ ุจุฑูุงูุฌ ูููู ุจุฅูุฌุงุฏ ุงูู ูููุฉ ู $ \theta $ ุงูุชููุงุชูููุงูุ ุงูุทุฑููู ุงูุฃุณูู ูู ุจุญุณุงุจ ูููุฉ ุงูุฎุณุงุฑู ูุฃูุซุฑ ูู ูููู ู $ \theta $ุ ุซู ููุฌุฏ ูููุฉ $ \theta $ ุฐุง ุงูุฃูู ุฎุณุงุฑู.

ุนุฑููุง ุฏุงูู ุจุฃุณู `simple_minimize` ูุงูุชู ุชูุจู ูุชุบูุฑุงุช ูู ุฏุงูุฉ ุงูุฎุณุงุฑูุ ูุตูููุฉ ุงูุจูุงูุงุชุ ููุตูููู ุจููู $ \theta $:

```python
def simple_minimize(loss_fn, dataset, thetas):
    '''
    ุงููููู ุงูููุงุฆูู ููุฐู ุงูุฏุงูู ูู ูููุฉ ฮธ ูู ุจูู ุนุฏุฉ ููู ูู ฮธ ุฐุงุช ุงูุฃูู ุฎุณุงุฑู
    '''
    losses = [loss_fn(theta, dataset) for theta in thetas]
    return thetas[np.argmin(losses)]
```

ุซู ูุนุฑู ุฏุงูู ูุฅูุฌุงุฏ ูููุฉ MSE ูุงุณุชุฎุฏุงููุง ูู ุฏุงูุฉ `simple_minimize`:

```python
def mse(theta, dataset):
    return np.mean((dataset - theta) ** 2)

dataset = np.array([12, 13, 15, 16, 17])
thetas = np.arange(12, 18, 0.1)

simple_minimize(mse, dataset, thetas)
```

```ruby
14.599999999999991
```

ุงููุชูุฌู ูุฐู ูุฑูุจู ูููููู ุงููุชููุนู:

```python
# ุงูุฌุงุฏ ุงููููู ุจุฅุณุชุฎุฏุงู ุงููุชูุณุท
np.mean(dataset)
```

```ruby
14.6
```

ุงูุขู ูููููุง ูุชุงุจุฉ ุฏุงูู ูุญุณุงุจ ุฎุณุงุฑุฉ Huber ูุฑุณููุง:

```python
def huber_loss(theta, dataset, alpha = 1):
    d = np.abs(theta - dataset)
    return np.mean(
        np.where(d < alpha,
                 (theta - dataset)**2 / 2.0,
                 alpha * (d - alpha / 2.0))
    )

points_and_loss(pts, (11, 18), huber_loss)
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_basics_12_0.png'>
</p>

ุนูู ุงูุฑุบู ุงู ุงูููู ุงูุฏููุง ู $ \theta $ ูุฌุจ ุงู ุชููู ุฃูุฑุจ ุฅูู 15ุ ููุณ ูุฏููุง ุทุฑููู ูุชุญููู ูุฅูุฌุงุฏ ูููุฉ $ \theta $ ูุฏุงูุฉ ุงูุฎุณุงุฑู Huber. ุจุฏูุงูุ ูู ุฐููุ ุณูุณุชุฎุฏู ุงูุฏุงูู `simple_minimize`:

```python
simple_minimize(huber_loss, dataset, thetas)
```

```ruby
14.999999999999989
```

ุงูุขูุ ููุนูุฏ ูุจูุงูุงุช ูุณุจุฉ ุงูุฅูุฑุงููุงุช ูููุฌุฏ ุงูุถู ูููู ู $ \theta $ ุจุฅุณุชุฎุฏุงู Huber:

```python
tips = sns.load_dataset('tips')
tips['pcttip'] = tips['tip'] / tips['total_bill'] * 100
tips.head()
```

**pcttip**|**size**|**time**|**day**|**smoker**|**sex**|**tip**|**total\_bill**| 
:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:
5.944673|2|Dinner|Sun|No|Female|1.01|16.99|0
16.054159|3|Dinner|Sun|No|Male|1.66|10.34|1
16.658734|3|Dinner|Sun|No|Male|3.5|21.01|2
13.978041|2|Dinner|Sun|No|Male|3.31|23.68|3
14.680765|4|Dinner|Sun|No|Female|3.61|24.59|4

<br>
```python
points_and_loss(tips['pcttip'], (11, 20), huber_loss)
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_basics_17_0.png'>
</p>

```python
simple_minimize(huber_loss, tips['pcttip'], thetas)
```

```ruby
15.499999999999988
```

ููุงุญุธ ุงู ุนูุฏ ุงุณุชุฎุฏุงู ุฏุงูุฉ ุฎุณุงุฑุฉ Huber ูุงูุช ุงููุชูุฌู $ \hat{\theta} = 15.5 $ . ูููููุง ุงูุขู ููุงุฑูุฉ ูุฐู ุงููุชูุฌู ูุน MSE ู MAE:

```python
print(f"               MSE: theta_hat = {tips['pcttip'].mean():.2f}")
print(f"               MAE: theta_hat = {tips['pcttip'].median():.2f}")
print(f"        Huber loss: theta_hat = 15.50")
```

```ruby
            MSE: theta_hat = 16.08
            MAE: theta_hat = 15.48
    Huber loss: theta_hat = 15.50
```

ููุงุญุธ ุงู ุฏุงูุฉ Huber ุงูุฑุจ ุฅูู MAE ููููุง ูุง ุชุชุฃุซุฑ ุจุดูู ูุจูุฑ ุจุณุจุจ ุงูููู ุงูุดุงุฐู ุนูู ุงูุฌุงูุจ ุงูุฃููู ูู ุงูุฑุณู ุงูุจูุงูู ุงูุชุงูู ูุชูุฒูุน ุจูุงูุงุช ุงูุฅูุฑุงููุงุช:

```python
sns.distplot(tips['pcttip'], bins=50);
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_basics_22_0.png'>
</p>

### ูุดุงูู simple_minimize

ุนูู ุงูุฑุบู ูู ุงู ุฏุงูุฉ `simple_minimize` ุชุณุงุนุฏูุง ุนูู ุชูููู ุฏุงูุฉ ุงูุฎุณุงุฑูุ ุฅูุง ุงู ูุฏููุง ุจุนุถ ุงููุดุงูู ุงูุชู ุชุฌุนููุง ุบูุฑ ูููุฏู ููุฅุณุชุฎุฏุงู ุจุดูู ุนุงู. ูุดููุชูุง ุงูุฃูู ูู ุงููุง ุชุนูู ููุท ูุน ููู $ \theta $. ูุซูุงูุ ูู ุงูููุฏ ุงูุจุฑูุฌู ุงูุชุงููุ ุงูุฐู ุณุจู ุงู ุงุณุชุฎุฏููุงู ูู ุงูุฃุนููุ ุงุญุชุฌูุง ูุชุนุฑูู ููู $ \theta $ ูุฏููุงู ูู 12 ุฅูู 18:

```python
dataset = np.array([12, 13, 15, 16, 17])
thetas = np.arange(12, 18, 0.1)

simple_minimize(mse, dataset, thetas)
```

ููู ูุฌุฏูุง ุงูู ุนูููุง ุงูุชุญูู ูู ุงูููู ุจูู 12 ู 18ุ ุงุญุชุฌูุง ููุฑุงุฌุนุฉ ุงูุฑุณู ุงูุจูุงูู ูุฏุงูุฉ ุงูุฎุณุงุฑู ููุฌุฏูุง ุงู ุงูููู ุงูุฏููุง ุจูู ุชูู ุงููููุชูู. ูุฐู ุงูุทุฑููู ุบูุฑ ุนูููู ูุฃููุง ูููุง ุจุฅุถุงูุฉ ุฎุทูู ูุนูุฏู ุฌุฏูุฏู ูููุงุฐุฌูุง. ุจุงูุฅุถุงูู ูุฐููุ ูููุง ุจุชุนุฑูู ููู ุงูุฒูุงุฏู 0.1 ุจุดูู ูุฏูู. ููููุ ุงุฐุง ูุงูุช ุงููููู ุงููุซูู ู $ \theta $ ูู 12.043ุ ุณุชููู ุงูุฏุงูู `simple_minimize` ุจุชูุฑูุจ ุงููุชูุฌู ุฅูู 12.00 ููููุง ุงูุฃูุฑุจ ููุถุงุนูุงุช 0.1

ูููููุง ุญู ุชูู ุงููุดุงูู ุจุทุฑููู ูุงุญุฏู ุจุฅุณุชุฎุฏุงู ูุง ูุณูู ุจู *ุงููุฒูู ุงูุฅุดุชูุงูู Gradient Descent*.

## ุงููุฒูู ุงูุฅุดุชูุงูู

ูุญู ูููุชููู ูุจูุงุก ุฏุงูู ุชุณุชุทูุน ุงูุชูููู ูู ุฏุงูุฉ ุงูุฎุณุงุฑู ุจุฏูู ุชูููุฏ ุงููุณุชุฎุฏู ูุชุญุฏูุฏ ููู ูุณุจูู ู $ \theta $ ููุชุฌุฑุจู ุนูููุง. ุจูุนูู ุฃุตุญุ ุจูุง ุงู ุฏุงูุฉ `simple_minimize` ุดูููุง ูุงูุชุงูู:

```python
simple_minimize(loss_fn, dataset, thetas)
```

ูุฑูุฏ ุฏุงูุฉ ูุฏููุง ุงูุดูู ุงูุชุงูู

```python
minimize(loss_fn, dataset)
```

> ูุงุญุธ ูู ุงูุดูู ุงูุฐู ูุจุญุซ ุนููุ ูุง ูุญุชุงุฌ ุงููุณุชุฎุฏู ูุฅุถุงูุฉ ูููู ูุณุจูู ู $ \theta $ ูู ุงููุชุบูุฑุงุช ุงููุทููุจู ูุฏุงูุฉ ุชูููู ุงูุฎุณุงุฑู `minimize`.

ุชุญุชุงุฌ ูุฐู ุงูุฏุงูู ูุฅูุฌุงุฏ ููู $ \theta $ ุงูุฃูู ุฎุณุงุฑู ุงูุชููุงุชูููุงู ุงูุงู ูุงู ุญุฌููุง. ุณูุณุชุฎุฏู ุทุฑููุฉ ุชุณูู ุจุงููุฒูู ุงูุฅุดุชูุงูู ูุจูุงุก ุงูุฏุงูุฉ ุงูุฌุฏูุฏู ุงููุณูุงู `minimize`.

### ุงูููุฑู

ููุง ูู ุฏูุงู ุงูุฎุณุงุฑูุ ุณูุชุญุฏุซ ุนู ููุฑุฉ ุงููุฒูู ุงูุฅุดุชูุงูู ุฃููุงูุ ุซู ูุชุนุฑู ููููู ุงูุนูููู ุงูุฑูุงุถูู ูููุง.

ุจูุง ุงู ุงูุฏุงูู `minimize` ูุง ููุฏู ููุง ููู ู $ \theta $ ููุชุฌุฑุจู ุนูููุงุ ูููู ุจุฅุฎุชูุงุฑ ูููู ู $ \theta $ ุจุฃู ููุงูู. ุซูุ ูููู ุจุดูู ุชูุฑุงุฑู ุจุชุญุณูู ูุชุงุฆุฌ $ \theta $. ูููุชุญุณูู ูู ุงููุชุงุฆุฌุ ูููู ุจููุงุญุธุฉ ุงููููุงู Slope ูุชูู ุงููููู ูู $ \theta $ ุงูุชู ุงุฎุชุฑูุงูุง ูู ุงูุฑุณู ุงูุจูุงูู.

ูุซูุงูุ ุณูุณุชุฎุฏู MSE ุนูู ุงูุจูุงูุงุช ุงูุชุงููู $ \textbf{y} = [ 12.1, 12.8, 14.9, 16.3, 17.2 ] $ ููููุฉ $ \theta $ ุงูุชู ุงุฎุชุฑุงูุงูุง ูู 12:

```python
pts = np.array([12.1, 12.8, 14.9, 16.3, 17.2])
plot_loss(pts, (11, 18), mse)
plot_theta_on_loss(pts, 12, mse)
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_descent_define_5_0.png'>
</p>

> ุงุณุชุฎุฏู ุงููุงุชุจ ุฏุงูุชูู ููุง ูุนู ูุณุจูุงู ูุชุณุงุนุฏู ุนูู ุงูููุงู ุจุงูุนูููู ุงูุญุณุงุจูู ูุงูุฑุณู ุงูุจูุงูู ูููุง `plot_loss` ู `plot_theta_on_loss`. ูุงูููุฏ ุงูุจุฑูุฌู ูู ุงูุฃุณูู ูู ุชุนุฑูู ููุง ุงูุฏุงูุชูู:
>
> ```python
> def plot_loss(y_vals, xlim, loss_fn):
>    thetas = np.arange(xlim[0], xlim[1] + 0.01, 0.05)
>    losses = [loss_fn(theta, y_vals) for theta in thetas]
>
>    plt.figure(figsize=(5, 3))
>    plt.plot(thetas, losses, zorder=1)
>    plt.xlim(*xlim)
>    plt.title(loss_fn.__name__)
>    plt.xlabel(r'$ \theta $')
>    plt.ylabel('Loss')
>   
>   
> def plot_theta_on_loss(y_vals, theta, loss_fn, **kwargs):
>    loss = loss_fn(theta, y_vals)
>    default_args = dict(label=r'$ \theta $', zorder=2,
>                        s=200, c=sns.xkcd_rgb['green'])
>    plt.scatter([theta], [loss], **{**default_args, **kwargs})
> ```
> 

ูุฑูุฏ ุงุฎุชูุงุฑ ูููู ุฌุฏูุฏู ู $ \theta $ ูุชูููู ุงูุฎุณุงุฑู. ููุนูู ุฐููุ ููุง ุฐูุฑูุง ุณุงุจูุงูุ ููุงุญุธ ุงููููุงู ููููุฉ $ \theta= 12 $:

```python
pts = np.array([12.1, 12.8, 14.9, 16.3, 17.2])
plot_loss(pts, (11, 18), mse)
plot_tangent_on_loss(pts, 12, mse)
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_descent_define_7_0.png'>
</p>

ูููุฉ ุงููููุงู ุณูุจููุ ูุนูู ุฐูู ุงู ุฒูุงุฏุฉ ูููุฉ $ \theta $ ุณูููู ูู ุงูุฎุณุงุฑู.
ุงุฐุง ูุงูุช $ \theta= 16.5 $ุ ูุฃู ูููุฉ ุงููููุงู ุณุชููู ููุฌุจู:

```python
pts = np.array([12.1, 12.8, 14.9, 16.3, 17.2])
plot_loss(pts, (11, 18), mse)
plot_tangent_on_loss(pts, 16.5, mse)
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_descent_define_9_0.png'>
</p>

ุนูุฏูุง ุชููู ูุชูุฌุฉ ุงููููุงู ุฅูุฌุงุจููุ ูุฃู ุชูููู ูููุฉ $ \theta $ ุณูููู ุงูุฎุณุงุฑู.

ุงููููุงู ูู ุงูุฎุท ูุฎุจุฑูุง ุจุฅู ุงุชุฌุงู ูุฎุชุงุฑ $ \theta $ ูุชูููู ุงูุฎุณุงุฑู. ุงุฐุง ูุงู ุงููููุงู ูุชูุฌุชู ุณูุจููุ ููุญุชุงุฌ ูุชุญุฑู $ \theta $ ุฅูู ุงูุฌุงูุจ ุงูุฅูุฌุงุจู. ูุฅุฐุง ูุงู ุฅูุฌุงุจูุงูุ ูุนูููุง ุชุญุฑูู $ \theta $ ุฅูู ุงูุฌุงูุจ ุงูุณูุจู. ุฑูุงุถุงูุ ูููู ุงูุชุงูู:

$$ \theta^{(t+1)} = \theta^{(t)} - \frac{\partial}{\partial \theta} L(\theta^{(t)}, \textbf{y}) $$

ููููุง $ \theta^{(t)} $ ูู ุงููููู ุงูุญุงูููุ ู $ \theta^{(t+1)} $ ูู ุงููููู ุงูุชุงููู.

ุจุงููุณุจู ู MSEุ ูุณุชููู ูุงูุชุงูู:

$$ \begin{split}
\begin{aligned}
L(\theta, \textbf{y})
&= \frac{1}{n} \sum_{i = 1}^{n}(y_i - \theta)^2\\
\frac{\partial}{\partial \hat{\theta}} L(\theta, \textbf{y})
&= \frac{1}{n} \sum_{i = 1}^{n} -2(y_i - \theta) \\
&= -\frac{2}{n} \sum_{i = 1}^{n} (y_i - \theta) \\
\end{aligned}
\end{split} $$

ุนูุฏูุง ุชููู $ \theta^{(t)} = 12 $ุ ูุงููุชูุฌู ูู $ -\frac{2}{n} \sum_{i = 1}^{n} (y_i - \theta) = -5.32 $ุ ุซู ูุณุชุฎุฏููุง ุจุงููุนุงุฏูู ุงูุณุงุจูู: $ \theta^{(t+1)} = 12 - (-5.32) = 17.32 $

ุฑุณููุง ูู ุงูุฃุณูู ุงููููู ุงูุณุงุจูู ู $ \theta $ ุจุฏุงุฆุฑู ููุฑุบู ุจุญุฏูุฏ ุฎุถุฑุงุก ูุงููููู ุงูุฌุฏูุฏู ููุง ุจุฏุงุฆุฑู ุจุงูููู ุงูุฃุฎุถุฑ:

```python
pts = np.array([12.1, 12.8, 14.9, 16.3, 17.2])
plot_loss(pts, (11, 18), mse)
plot_theta_on_loss(pts, 12, mse, c='none',
                   edgecolor=sns.xkcd_rgb['green'], linewidth=2)
plot_theta_on_loss(pts, 17.32, mse)
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_descent_define_11_0.png'>
</p>

ุนูู ุงูุฑุบู ุงู $ \theta $ ุงูุชููุช ุฅูู ุงูุฌุงูุจ ุงูุฃูููุ ููููุง ูุง ุฒุงูุช ุจุนูุฏู ุฌุฏุงู ุนู ุงููููู ุงูุฏููุง. ูููููุง ุญู ุฐูู ุนู ุทุฑูู ุถุฑุจ ุงููููุงู ุจูููู ุตุบูุฑู ูุจู ุทุฑุญู ูู $ \theta $. ูุงูุนูููู ุงูุญุณุงุจูู ุงูููุงูุฉ ุณุชุจุฏู ูุงูุชุงูู:

$$ \theta^{(t+1)} = \theta^{(t)} - \alpha \cdot \frac{\partial}{\partial \theta} L(\theta^{(t)}, \textbf{y}) $$

ููููุง $ \alpha $ ูู ูููู ุซุงุจุชู ุตุบูุฑู. ูุซูุงูุ ุงุฐุง ุญุฏุฏูุง ูููุฉ $ \alpha = 0.3 $ุ ูุฃู ุงููููู ุงูุฌุฏูุฏู ู $ \theta^{(t+1)} $ ุณุชููู:

```python
plot_one_gd_iter(pts, 12, mse, grad_mse)
```

```ruby
old theta: 12
new theta: 13.596
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_descent_define_14_1.png'>
</p>


> ุนุฑู ุงููุงุชุจ ุฏุงูุฉ ุฌุฏูุฏู ุจุฃุณู `plot_one_gd_iter` ุชููู ุจูุชุงุจุฉ ูุฑุณู ุจูุงูู ููููู $ \theta $ ุงูุณุงุจูู ูุงูุฌุฏูุฏู ุจุนุฏ ุงูููุงู ุจุงูุนูููู ุงูุญุณุงุจูู ุงููุดุฑูุญู ูุณุจูุงูุ ุงุณุชุฎุฏู ุงููุงุชุจ ุงูุถุงู ุฏุงูุฉ ุจุฃุณู `grad_mse` ููู ุชุนุฑูู ูุฏุงูุฉ `mse` ุจุฅุณุชุฎุฏุงู ุงููุฒูู ุงูุฅุดุชูุงููุ ุงูููุฏ ุงูุจุฑูุฌู ูููุง ุงูุฏุงูุชูู:
>
> ```python
>   def plot_one_gd_iter(y_vals, theta, loss_fn, grad_loss, alpha=0.3):
>       new_theta = theta - alpha * grad_loss(theta, y_vals)
>       plot_loss(pts, (11, 18), loss_fn)
>       plot_theta_on_loss(pts, theta, loss_fn, c='none',
>                          edgecolor=sns.xkcd_rgb['green'], linewidth=2)
>       plot_theta_on_loss(pts, new_theta, loss_fn)
>       print(f'old theta: {theta}')
>       print(f'new theta: {new_theta}')
>
>   def grad_mse(theta, y_vals):
>       return -2 * np.mean(y_vals - theta)
> ```

ูู ุงูุฑุณู ุงูุชุงููุ ููู $ \theta $ ุจุนุฏ ุนุฏุฉ ุชูุฑุงุฑุงุช ุจููุณ ุงูุทุฑููู ุงูุณุงุจูู. ูุงุญุธ ุงู $ \theta $ ุชุชุบูุฑ ุจุดูู ุจุณูุท ูููุง ุงูุชุฑุจูุง ูู ุงููููู ุงูุฏููุง ูุฃู ุงููููุงู ุงูุถุงู ุงุตุจุญุช ูููุชู ุงูู:

```python
plot_one_gd_iter(pts, 13.60, mse, grad_mse)
```

```ruby
old theta: 13.6
new theta: 14.236
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_descent_define_16_1.png'>
</p>

```python
plot_one_gd_iter(pts, 14.24, mse, grad_mse)
```

```ruby
old theta: 14.24
new theta: 14.492
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_descent_define_17_1.png'>
</p>

```python
plot_one_gd_iter(pts, 14.49, mse, grad_mse)
```

```ruby
old theta: 14.49
new theta: 14.592
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_descent_define_18_1.png'>
</p>

### ุชุญููู ุงููุฒูู ุงูุฅุดุชูุงูู

ูุฏููุง ุงูุขู ููุฑุฉ ุนู ุทุฑููุฉ ุนูู ุฎูุงุฑุฒููุฉ ุงููุฒูู ุงูุฅุดุชูุงูู:
- ุงุฎุชูุงุฑ ูููู ุงูููู ู $ \theta $ ( ูู ุงูุนุงุฏู ุชููู 0 ).
- ุงุฌุฑุงุก ุงูุนูููู ุงูุญุณุงุจูู $ \theta - \alpha \cdot \frac{\partial}{\partial \theta} L(\theta, \textbf{y}) $ ุนูููุง ูุญูุธ ุงููุชูุฌู ููููู ุฌุฏูุฏู ู $ \theta $.
- ุชูุฑุงุฑ ุงูุนูููู ุญุชู ุชุชููู $ \theta $ ุนู ุงูุชุบูุฑ.

ุบุงูุจุงู ุณุชูุงุญุธ ุงุณุชุฎุฏุงู ุฑูุฒ ุงููุฒูู (ุงูุฅูุญุฏุงุฑ) $ \nabla_\theta $ ุจุฏูุงู ูู ุงูุฅุดุชูุงู ุงูุฌุฒุฆู $ \frac{\partial}{\partial \theta} $. 
ููุง ุงูุฑูุฒูู ูุชุดุงุจูุงูุ ูููู ุจูุง ุงูุง ุงุณุชุฎุฏุงู ุฑูุฒ ุงููุฒูู ุงูุซุฑ ุจุดูู ุนุงูุ ูุณูููู ุจุฅุณุชุฎุฏุงูู ูู ุงููุนุงุฏูู:

$$ \theta^{(t+1)} = \theta^{(t)} - \alpha \cdot \nabla_\theta L(\theta^{(t)}, \textbf{y}) $$

ููุฑุงุฌุนุฉ ุงูุฑููุฒ:

- $ \theta^{(t)} $ ูู ุงูุชููุน ุงูุญุงูู ู $ \theta^{*} $ ูู ุงูุชูุฑุงุฑ $ t $.
- $ \theta^{(t+1)} $ ุงููููู ุงูุชุงููู ู $ \theta $.
- $ \alpha $ ูุทูู ุนูููุง ูุนุฏู ุงูุชุนููู Learning Rateุ ูุนุงุฏุฉ ูุง ุชููู ุฑูู ุตุบูุฑ ุซุงุจุช. ูู ุจุนุถ ุงููุฑุงุช ูู ุงููููุฏ ุงู ุชุจุฏุฃ ุจุฑูู ุนุงูู ู $ \alpha $ ูุงูุชูููู ููู. ุงุฐุง ุชุบูุฑุช ูููุฉ $ \alpha $ ุจูู ุนูููุงุช ุงูุชูุฑุงุฑุ ูุณุชุฎุฏู ุงูุฑูุฒ $ \alpha^t $ ูุชูุถูุญ ุชุบูุฑ $ \alpha $ ูู $ t $.
- $ \nabla_\theta L(\theta^{(t)}, \textbf{y}) $ ูู ุงุดุชูุงู ุฌุฒุฆู ูุฏุงูุฉ ุงูุฎุณุงุฑู ูููุง ูููู ูุชููุนู ู $ \theta $ ูู ุงูุชูุฑุงุฑ $ t $.

ูููููุง ููุงุญุธู ุงูููุฉ ุงุณุชุฎุฏุงู ุฏุงูุฉ ุฎุณุงุฑู ูุงุจูู ููุชูุงุถู: $ \nabla_\theta L(\theta, \textbf{y}) $ ูู ุฌุฒุก ููู ูู ุฎูุงุฑุฒููุฉ ุงููุฒูู ุงูุฅุดุชูุงูู. (ุนูู ุงูุฑุบู ุงู ุจุงูุฅููุงู ุชููุน ูููุฉ ุงููุฒูู (ุงูุฅูุญุฏุงุฑ) ุจุญุณุงุจ ุงููุฑู ูู ุงูุฎุณุงุฑู ุจูู ูููุชูู $ \theta $ ููุณูุชูุง ุนูู ุงููุณุงูู ุจููููุงุ ููู ุฐูู ูุฒูุฏ ูู ูุฏุฉ ุฅูุฌุงุฏ ุงููุชูุฌู ูููุฒูู ุงูุฅุดุชูุงูู ุจุดูู ูุจูุฑ ููุง ูุฌุนููุง ุบูุฑ ูููุฏู ููุฅุณุชุฎุฏุงู).

ุฎูุงุฑุฒููุฉ ุงููุฒูู ุงูุฅุดุชูุงูู ุจุณูุทู ููููุฏู ุจุดูู ูุจูุฑ ูุฐูู ูุฅู ุจุฅููุงููุง ุฅุณุชุฎุฏุงููุง ูู ูุซูุฑ ูู ุงููุงุน ุงูููุงุฐุฌ ูุงููุซูุฑ ูู ุฏูุงู ุงูุฎุณุงุฑู. ูู ุงูุทุฑููู ุงูุญุณุงุจูู ุงูุฃูู ูุถุจุท ุงูููุงุฐุฌุ ุจูุง ูููุง ุงูุฅูุญุฏุงุฑ ุงูุฎุทู ุนูู ุจูุงูุงุช ุจุญุฌู ูุจูุฑ ูุงูุดุจูุงุช ุงูุนุตุจูู.

### ุชุนุฑูู ุฏุงูุฉ `minimize`

ุงูุขู ูุนูุฏ ููููุชูุง ุงูุฃุณุงุณูู: ุชุนุฑูู ุฏุงูุฉ `minimize`. ุณูุญุชุงุฌ ููุชุนุฏูู ููููุงู ูู ุชุนุฑูู ุงูุฏุงูู ููููุง ูุฑูุฏ ุฅูุฌุงุฏ ุงููุฒูู ุงูุฅุดุชูุงูู ูุฏุงูุฉ ุงูุฎุณุงุฑู:

```python
def minimize(loss_fn, grad_loss_fn, dataset, alpha=0.2, progress=True):
    '''
    ุชุณุชุฎุฏู ุงููุฒูู ุงูุฅุดุชูุงูู ููุชูููู ูู ุฏุงูุฉ ุงูุฎุณุงุฑู loss_fn.
    ุชูุชุฌ ููุง ุงูุฏุงูู ุงููููู ุงูุตุบุฑู ู theta_hat (ฮธ^) ุนูุฏูุง ูููู
    ุงูุชุบููุฑ ุงูู ูู 0.001 ุจูู ุงูุชูุฑุงุฑุงุช.
    '''
    theta = 0
    while True:
        if progress:
            print(f'theta: {theta:.2f} | loss: {loss_fn(theta, dataset):.2f}')
        gradient = grad_loss_fn(theta, dataset)
        new_theta = theta - alpha * gradient
        
        if abs(new_theta - theta) < 0.001:
            return new_theta
        
        theta = new_theta
```

ุซู ูููููุง ุชุนุฑูู ุฏูุงู ุชููู ุจุญุณุงุจ MSE ู ูุฒูููุง (ุงูุญุฏุงุฑูุง):

```python
def mse(theta, y_vals):
    return np.mean((y_vals - theta) ** 2)

def grad_mse(theta, y_vals):
    return -2 * np.mean(y_vals - theta)
```

ุงุฎูุฑุงูุ ูููููุง ุงุณุชุฎุฏุงู ุงูุฏุงูู `minimize` ูุญุณุงุจ ูููุฉ $ \theta $ ุงูุฃุฏูู ููุจูุงูุงุช ุงูุชุงููู $ \textbf{y} = [12.1, 12.8, 14.9, 16.3, 17.2] $ 

```python
%%time
theta = minimize(mse, grad_mse, np.array([12.1, 12.8, 14.9, 16.3, 17.2]))
print(f'Minimizing theta: {theta}')
print()
```

```ruby
theta: 0.00 | loss: 218.76
theta: 5.86 | loss: 81.21
theta: 9.38 | loss: 31.70
theta: 11.49 | loss: 13.87
theta: 12.76 | loss: 7.45
theta: 13.52 | loss: 5.14
theta: 13.98 | loss: 4.31
theta: 14.25 | loss: 4.01
theta: 14.41 | loss: 3.90
theta: 14.51 | loss: 3.86
theta: 14.57 | loss: 3.85
theta: 14.61 | loss: 3.85
theta: 14.63 | loss: 3.84
theta: 14.64 | loss: 3.84
theta: 14.65 | loss: 3.84
theta: 14.65 | loss: 3.84
theta: 14.66 | loss: 3.84
theta: 14.66 | loss: 3.84
Minimizing theta: 14.658511131035242

CPU times: user 7.88 ms, sys: 3.58 ms, total: 11.5 ms
Wall time: 8.54 ms
```

ููุงุญุธ ุงู ุงููุฒูู ุงูุฅุดุชูุงูู ูุงู ุจุฅูุฌุงุฏ ููุณ ุงููุชูุฌุฉ ุจุดูู ุณุฑูุน ู:

```python
np.mean([12.1, 12.8, 14.9, 16.3, 17.2])
```

```ruby
14.66
```

### ุชูููู ุฎุณุงุฑุฉ Huber

ุงูุขูุ ูููููุง ุชุทุจูู ุงููุฒูู ุงูุฅุดุชูุงูู ููุชูููู ูู ุฏุงูุฉ ุงูุฎุณุงุฑู Huber ุนูู ุจูุงูุงุช ุงูุฅูุฑุงููุงุช.

```python
tips = sns.load_dataset('tips')
tips['pcttip'] = tips['tip'] / tips['total_bill'] * 100
```

ุฏุงูุฉ ุงูุฎุณุงุฑู Huber ุชุนุฑู ูุงูุชุงูู:

$$ \begin{split}
L_\delta(\theta, \textbf{y}) = \frac{1}{n} \sum_{i=1}^n \begin{cases}
    \frac{1}{2}(y_i - \theta)^2 &  | y_i - \theta | \le \delta \\
     \delta (|y_i - \theta| - \frac{1}{2} \delta ) & \text{otherwise}
\end{cases}
\end{split} $$

ูุงููุฒูู ุงูุฅุดุชูุงูู ูุฏุงูุฉ Huber:

$$ \begin{split}
\nabla_{\theta} L_\delta(\theta, \textbf{y}) = \frac{1}{n} \sum_{i=1}^n \begin{cases}
    -(y_i - \theta) &  | y_i - \theta | \le \delta \\
    - \delta \cdot \text{sign} (y_i - \theta) & \text{otherwise}
\end{cases}
\end{split} $$

(ูุงุญุธ ุงููุง ูู ุงูุชุนุงุฑูู ุงูุณุงุจูู ูุฏุงูุฉ ุฎุณุงุฑุฉ Huber ุงุณุชุฎุฏููุง ุงููุชุบูุฑ $ \alpha $ ููุฅุดุงุฑู ูููุทุฉ ุงูุฅูุชูุงู. ููุฅุจุนุงุฏ ุงูุดู ุจูููุง ูุจูู $ \alpha $ ุงููุณุชุฎุฏูู ูู ุงููุฒูู ุงูุฅุดุชูุงููุ ูููุง ุจุชุบูุฑ ุฑูุฒ ููุทุฉ ุงูุฅูุชูุงู ูู ุฏุงูุฉ ุงูุฎุณุงุฑู Huber ุฅูู ุงูุฑูุฒ $ \delta $.)

```python
def huber_loss(theta, dataset, delta = 1):
    d = np.abs(theta - dataset)
    return np.mean(
        np.where(d <= delta,
                 (theta - dataset)**2 / 2.0,
                 delta * (d - delta / 2.0))
    )

def grad_huber_loss(theta, dataset, delta = 1):
    d = np.abs(theta - dataset)
    return np.mean(
        np.where(d <= delta,
                 -(dataset - theta),
                 -delta * np.sign(dataset - theta))
    )
```

ููููู ุจุงูุชูููู ูู ุฏุงูุฉ ุงูุฎุณุงุฑู Huber ูู ุจูุงูุงุช ุงูุฅูุฑุงููู:

```python
%%time
theta = minimize(huber_loss, grad_huber_loss, tips['pcttip'], progress=False)
print(f'Minimizing theta: {theta}')
print()
```

```ruby
Minimizing theta: 15.506849531471964

CPU times: user 194 ms, sys: 4.13 ms, total: 198 ms
Wall time: 208 ms
```

### ููุฎุต ุงููุฒูู ุงูุฅุดุชูุงูู

ูููุฑ ููุง ุงููุฒูู ุงูุฅุดุชูุงูู ุทุฑููู ุนุงููู ููุชูููู ูู ุฏุงูุฉ ุงูุฎุณุงุฑู ุนูุฏูุง ูุง ูุณุชุทูุน ุฅูุฌุงุฏ ุงููููู ุงูุฏููุง ู $ \theta $. ุนูุฏูุง ูููู ูููุฐุฌูุง ูุฏุงูุฉ ุงูุฎุณุงุฑู ุงูุซุฑ ุชุนููุฏุงูุ ูุณุชุฎุฏู ุงููุฒูู ุงูุฅุดุชูุงูู ููุณููู ูุถุจุท ุงูููุงุฐุฌ.

## ุงูุชุญุฏุจ

ูุณุงูู ุงููุฒูู ุงูุฅุดุชูุงูู ุจุดูู ุนุงู ุจุงูุชูููู ูู ุฏุงูุฉ ุงูุฎุณุงุฑู. ููุง ุงุธูุฑูุง ุฐูู ูู ุฏุงูุฉ Huber ููุฎุณุงุฑูุ ุชููู ูุงุฆุฏุฉ ุงููุฒูู ุงูุฅุดุชูุงูู ุนูุฏูุง ูููู ุตุนุจ ุนูููุง ุฅูุฌุงุฏ ุงููููุฉ ุงูุฏููุง.

### ุงูุชุดุงู ุงูุญุฏูุฏ ุงูุฏููุง ุจุงููุฒูู ุงูุฅุดุชูุงูู

ููุฃุณูุ ูู ุจุนุถ ุงูุฃุญูุงู ูุง ูููู ูููุฒูู ุงูุฅุดุชูุงูู ุฅูุฌุงุฏ ูููุฉ $ \theta $. ูููุชุฑุถ ุงูุชุงูู $ \theta = -21 $:

```python
pts = np.array([0])
plot_loss(pts, (-23, 25), quartic_loss)
plot_theta_on_loss(pts, -21, quartic_loss)
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_convexity_5_0.png'>
</p>

```python
plot_one_gd_iter(pts, -21, quartic_loss, grad_quartic_loss)
```

```ruby
old theta: -21
new theta: -9.944999999999999
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_convexity_6_1.png'>
</p>

```python
plot_one_gd_iter(pts, -9.9, quartic_loss, grad_quartic_loss)
```

```ruby
old theta: -9.9
new theta: -12.641412
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_convexity_7_1.png'>
</p>

```python
plot_one_gd_iter(pts, -12.6, quartic_loss, grad_quartic_loss)
```

```ruby
old theta: -12.6
new theta: -14.162808
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_convexity_8_1.png'>
</p>

```python
plot_one_gd_iter(pts, -14.2, quartic_loss, grad_quartic_loss)
```

```ruby
old theta: -14.2
new theta: -14.497463999999999
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_convexity_9_1.png'>
</p>



> ูู ุงูุฃูุซูู ุงูุณุงุจูู ุงุณุชุฎุฏู ุงููุงุชุจ ุงูุฏูุงู ุงูุชุงููู (ุฏุงูุฉ ุงูุฎุณุงุฑู ุงูุฑุจุงุนูุฉ ููุฒูููุง ุงูุฅุดุชูุงูู) ุงูุชู ูู ูุนุฑููุง ูุณุจูุงู:
> 
> ```python
> def quartic_loss(theta, y_vals):
>     return np.mean(1/5000 * (y_vals - theta + 12) * (y_vals - theta + 23)
>                   * (y_vals - theta - 14) * (y_vals - theta - 15) + 7)
> 
> def grad_quartic_loss(theta, y_vals):
>     return -1/2500 * (2 *(y_vals - theta)**3 + 9*(y_vals - theta)**2
>                      - 529*(y_vals - theta) - 327)
> ```
>

ูู ุงููุซุงู ุงูุณุงุจูุ ุฏุงูุฉ ุงูุฎุณุงุฑู ุงูุฑุจุงุนููุ ูุงูุช ูุชูุฌุฉ ุงููุฒูู ุงูุฅุดุชูุงูู ูุฑูุจู ุฅูู $ \theta = -14.5 $ุ ูููู ุงููููุฉ ุงูุฏููุง ุงูุนุงูู ูุฏุงูุฉ ุงูุฎุณุงุฑู ูู $ \theta = 18 $ุ ูู ูุฐุง ุงููุซุงู ูุฑู ุงู ุงููุฒูู ุงูุฅุดุชูุงูู ูุจุญุซ ุนู *ุงููููู ุงูุฏููุง ุงููุญููู Local Minimum* ูุงูุชู ููุณุช ุฏุงุฆูุงู ุชุณุงูู ูููุฉ ุงูุฎุณุงุฑู *ูููููู ุงูุฏููุง ุงูุนุงูู Global Minimum*.

ูุญุณู ุงูุญุธุ ุจุนุถ ุฏูุงู ุงูุฎุณุงุฑู ูุฏููุง ููุณ ุงูุฑูู ูููููู ุงูุฏููุง ุงููุญููู ูุงูุนุงูู. ููุฃุฎุฐ ูุซูุงู ุฏุงูุฉ ุงูุฎุทุฃ ุงูุชุฑุจูุนู ุงููุชูุณุท MSE:

```python
pts = np.array([-2, -1, 1])
plot_loss(pts, (-5, 5), mse)
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_convexity_11_0.png'>
</p>

ุชุทุจูู ุงููุฒูู ุงูุฅุดุชูุงูู ุนูู ูุฐู ุงูุฏุงูู ุณููุฌุฏ ููุง ุฏุงุฆูุงู ูููู ูุซุงููู ุนุงูู ู $ \theta $ ููู ุงููููู ุงูุฏููุง ุงููุญููู ุงููุญูุฏู ูู ููุณูุง ุงูุนุงูู.

ูุชูุณุท ุงูุฎุทุฃ ุงูุญุชูู ูุฏ ูุญุชูู ุนูู ุงูุซุฑ ูู ูููู ุฏููุง ูุญููู. ููููุ ูู ุงูููู ุงูุฏููุง ูู ููุณูุง ุงููููู ุงูุฏููุง ุงูุนุงูู.

```python
pts = np.array([-1, 1])
plot_loss(pts, (-5, 5), abs_loss)
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_convexity_13_0.png'>
</p>

> ุชุนุฑูู ุฏุงูุฉ ูุชูุณุท ุงูุฎุทุฃ ุงูุญุชูู
> 
> ```python
> def abs_loss(theta, y_vals):
>     return np.mean(np.abs(y_vals - theta))
> ```
>

ูู ูุฐุง ุงููุซุงูุ ุณุชููู ูููุฉ ุงููุฒูู ุงูุฅุดุชูุงูู ุงุญุฏู ุงูููู ุงููุญููู ุงูุฏููุง ุจูู $ [-1, 1] $ ููู ุงู ูู ุงูููู ูู ูุฐุง ุงููุทุงู ููู ุฏููุง ูุฏุงูุฉ ุงูุฎุณุงุฑู ูุฐูุ ุณููุชุฑุญ ุงููุฒูู ุงูุฅุดุชูุงูู ูููุฉ ุฏููุง ูุซุงููู ุจูู ูุฐู ุงูููุงุท ู $ \theta $.
### ุชุนุฑูู ุงูุชุญุฏุจ

ูู ุจุนุถ ุงูุฏูุงูุ ุงู ูููู ุฏููุง ูุญููู ูู ููุณูุงุงูุนุงูู. ูุฐู ุงูุฏูุงู ูุทูู ุนูููุง **ุฏูุงู ูุญุฏุจุฉ Convex function** ูุฃููุง ููุญููู ููุฃุนูู. ูุฐูู ุฏุงูุฉ Huber ููุฎุณุงุฑูุ ุงููููุฐุฌ ุงูุซุงุจุชุ MSE ู MAE ุฌููุนูุง ูุญุฏุจู.

ูุน ูุนุฏู ุชุนูู Learning Rate ููุงุณุจุ ุงููุฒูู ุงูุฅุดุชูุงูู ููุฌุฏ $ \theta $ ุงูุนุงูู ุงููุซุงููู ูุฏุงูุฉ ุงูุฎุณุงุฑู ุงููุญุฏุจู. ูุจุณุจุจ ุฐููุ ููุถู ุถุจุท ููุงุฐุฌูุง ุจุฅุณุชุฎุฏุงู ุงูุฏูุงู ุงููุญุฏุจู ุฅูุง ุงุฐุง ูุงู ูุฏููุง ุณุจุจ ููุงุณุจ ุบูุฑ ุฐูู.

ุจุดููู ุนุงูุ ุงูุฏุงูู $ f $ ูุทูู ุนูููุง ุฏุงูู ูุญุฏุจู ููุท ุงุฐุง ูุงูุช ุชููู ุดุฑูุท ุงููุชุจุงููู ูุฌููุน ูุฏุฎูุงุชูุง $ a $ ู $ b $ ุ ููู $ t \in [0, 1] $: [๐][Inequality]

$$ tf(a) + (1-t)f(b) \geq f(ta + (1-t)b) $$

ุงููุชุจุงููู ุชููู ุงู ุฌููุน ุงูุฎุทูุท ุงูุชู ุชุฑุจุท ููุทุชูู ูู ุงูุฏุงูู ูุฌุจ ุงู ุชูุน ุนูู ุงู ููู ุงูุฏุงูู. ูุฏุงูุฉ ุงูุฎุณุงุฑู ุงูุชู ุนุฑุถูุงูุง ูู ุจุฏุงูุฉ ูุฐุง ุงูุฌุฒุกุ ูููููุง ุงูุฌุงุฏ ูุฐู ุงูุฎุท:

```python
pts = np.array([0])
plot_loss(pts, (-23, 25), quartic_loss)
plot_connected_thetas(pts, -12, 12, quartic_loss)
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_convexity_17_0.png'>
</p>

> ุงุณุชุฎุฏู ุงููุงุชุจ ุงูุฏุงูู plot_connected_thetas ูุชุณุงุนุฏู ุนูู ุฑุณู ุงูุฎุท ุจูู ุงูููุทุชููุ ูุนุฑููุง ุงููุงุชุจ ูุงูุชุงูู:
>
> ```python
> def plot_connected_thetas(y_vals, theta_1, theta_2, loss_fn, **kwargs):
>     plot_theta_on_loss(y_vals, theta_1, loss_fn)
>     plot_theta_on_loss(y_vals, theta_2, loss_fn)
>     loss_1 = loss_fn(theta_1, y_vals)
>     loss_2 = loss_fn(theta_2, y_vals)
>     plt.plot([theta_1, theta_2], [loss_1, loss_2])
> ```
>

ูุจูุงุกูุง ุนูู ุงูุชุนุฑูู ุงูุณุงุจูุ ููุฐู ุงูุฏุงูู ุบูุฑ ูุญุฏุจู.

ููุฎุทุฃ ุงูุชุฑุจูุนู ุงููุชูุณุทุ ุฌููุน ุงูุฎุทูุท ุงูุชู ููุทุชูู ุชุธูุฑ ููู ุงูุฑุณู ุงูุจูุงูู. ูููููุง ุฑุณู ุงุญุฏุงูุง ูุงูุชุงูู:

```python
pts = np.array([0])
plot_loss(pts, (-23, 25), mse)
plot_connected_thetas(pts, -12, 12, mse)
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_convexity_20_0.png'>
</p>

ุงูุชุนุฑูู ุงูุฑูุงุถู ููุชุญุฏุจ ูุนุทููุง ูุตู ุฏููู ูุชุญุฏูุฏ ูุง ุงุฐุง ูุงูุช ุงูุฏุงูู ูุญุฏุจู ุฃู ูุง. ูู ูุฐุง ุงููุชุงุจุ ุณูุชุฌุงูู ุงูุฌุฒุก ุงูุฑูุงุถู ูุฅุซุจุงุช ุงูุชุญุฏุจ ูููุชูู ุจุงูููู ูุง ุงุฐุง ูุงูุช ุฏุงูู ูุญุฏุจู ุฃู ูุง.

### ููุฎุต ุงูุชุญุฏุจ

ููุฏุงูู ุงููุญุฏุจูุ ุงู ูููู ุฏููุง ูุญููู ูู ููุณูุง ุนุงูู. ุฐูู ูุณูู ูููุฒูู ุงูุฅุดุชูุงูู ุฅูุฌุงุฏ ุงูุถู ุงููุชุบูุฑุงุช ููููุงุฐุฌ ูุฃู ุฏุงูุฉ ุฎุณุงุฑู. ุนูู ุงูุฑุบู ูู ุงูู ุจุฅููุงููุง ุฅุณุชุฎุฏุงู ุงููุฒูู ุงูุฅุดุชูุงูู ูุฏูุงู ุงูุฎุณุงุฑู ุบูุฑ ุงููุญุฏุจู ูุฅูุฌุงุฏ ููู ุฏููุงุ ุชูู ุงูููู ุงูุฏููุง ุงููุญููู ุบูุฑ ูุถููู ุฃููุง ุฏุงุฆูุงู ูู ุงูููู ุงูุนุงูู ุงููุซุงููู.

## ุงููุฒูู ุงูุฅุดุชูุงูู ุงูุนุดูุงุฆู

### ููุฏูุฉ 

ูู ูุฐุง ุงูุฌุฒุกุ ุณูุชุญุฏุซ ุนู ุชุนุฏูู ุนูู ุงููุฒูู ุงูุฅุดุชูุงูู ูุฌุนูู ุฃูุซุฑ ูุงุฆุฏู ููุจูุงูุงุช ุฐุงุช ุงูุญุฌู ุงููุจูุฑ. ูุฐุง ุงูุชุนุฏูู ูุทูู ุนููู ุฎูุงุฑุฒููุฉ **ุงููุฒูู ุงูุฅุดุชูุงูู ุงูุนุดูุงุฆู Stochastic Gradient Descent**.

ุจุนุฏ ุงู ุชุนูููุง ุทุฑููุฉ ุนูู ุงููุฒูู ุงูุฅุดุชูุงูู ูุชุญุฏูุซู ููููุฉ $ \theta $ ุจุฅุณุชุฎุฏุงู ุงูุฅุดุชูุงู ูุฏุงูุฉ ุงูุฎุณุงุฑู. ุจุงูุฐุงุช ุฅุณุชุฎุฏููุง ุงููุนุงุฏูู ุงูุชุงููู:

$$ {\theta}^{(t+1)} = \theta^{(t)} - \alpha \cdot \nabla_{\theta} L(\theta^{(t)}, \textbf{y}) $$

ูู ูุฐู ุงููุนุงุฏูู: 

- $ \theta^{(t)} $ ูู ุงูุชููุน ุงูุญุงูู ู $ \theta^{*} $ ูู ุงูุชูุฑุงุฑ $ t $.
- $ \alpha $ ูู ูุนุฏู ุงูุชุนููู Learning Rate.
- $ \nabla_\theta L(\theta^{(t)}, \textbf{y}) $ ูู ุงุดุชูุงู ุฏุงูุฉ ุงูุฎุณุงุฑู.
- ููุญุณุจ ุงูุชููุน ุงูุชุงูู $ \theta^{(t+1)} $ ุนู ุทุฑูู ุทุฑุญ $ \alpha $ ู $ \nabla_\theta L(\theta, \textbf{y}) $ ูุงููุญุณูุจู ูู $ \theta^{(t)} $.

##### ุญุฏูุฏ ุงููุฒูู ุงูุฅุดุชูุงูู

ูู ุงููุนุงุฏูู ุงูุณุงุจููุ ูููุง ุจุญุณุงุจ $ \nabla_\theta L(\theta, \textbf{y}) $ ุจุฅุณุชุฎุฏุงู ูุชูุณุท ุงูุฅุดุชูุงู ูุฏุงูุฉ ุงูุฎุณุงุฑู $ \ell(\theta, y_i) $ ูุฌููุน ุงูุจูุงูุงุช. ุจูุนูู ุขุฎุฑุ ูู ูู ูุฑู ูุญุฏุซ ูููุฉ $ \theta $ ูุชุญูู ูู ุฌููุน ุงูููุงุท ุงูุฃุฎุฑู ูู ุจูุงูุงุชูุง. ููุฐุง ุงูุณุจุจุ ุงููุงููู ููุฅุดุชูุงู ูู ุงููุนุงุฏูู ุงูุณุงุจูู ูุทูู ุนููู **ุงููุฒูู ุงูุฅุดุชูุงูู ุงูููุฌููุน Batch Gradient Descent**.

ููุฃููุง ูุณูุก ุงูุญุธ ุนุงุฏุฉ ูุง ูุนูู ูุน ุจูุงูุงุช ูุจูุฑุฉ ุงูุญุฌูุ ูุฃู ุงููุฒูู ุงูุฅุดุชูุงูู ุงูููุฌููุน ุณูุนูู ูุฅูุฌุงุฏ ุงููููู ุงูููุงุณุจู ู $ \theta $ ุจุนุฏ ุจุถุน ุชูุฑุงุฑุงุชุ ูููู ูู ุชูุฑุงุฑ ูุฏ ูุงุฎุฐ ููุชุงู ุทููู ูุญุณุงุจ ุงููุชูุฌู ููู ุฅุฐุง ูุงูุช ุงูููุงุท ูู ุจูุงูุงุชูุง ูุซูุฑู.

#### ุงููุฒูู ุงูุฅุดุชูุงูู ุงูุนุดูุงุฆู

ูุญู ูุดููุฉ ุงูููุช ูู ุญุณุงุจ ุงูุฅุดุชูุงู ูุฌููุน ุจูุงูุงุช ุงูุชุฏุฑูุจุ ูููู ุงููุฒูู ุงูุฅุดุชูุงูู ุงูุนุดูุงุฆู ุจุชููุน ุงููููู ุจุฅุณุชุฎุฏุงู **ูููู ุนุดูุงุฆูู ูุงุญุฏู ูู ุงูุจูุงูุงุช**. ููุฃู ุงููููู ุชู ุฅุฎุชูุงุฑูุง ุจุดูู ุนุดูุงุฆูุ ูุชููุน ุงู ุงูุฅุดุชูุงู ููู ููุทู ุณูุฃุฏู ุจุงูููุงูู ุฅูู ููุณ ุงููุชูุฌู ูููุฒูู ุงูุฅุดุชูุงูู ุงูููุฌููุน.

ููุนูุฏ ูุฑู ุฃุฎุฑู ููุนุนุงุฏุฉ ุงููุฒูู ุงูุฅุดุชูุงูู ุงูููุฌููุน:

$$ {\theta}^{(t+1)} = \theta^{(t)} - \alpha \cdot \nabla_{\theta} L(\theta^{(t)}, \textbf{y}) $$

ูู ูุฐู ุงููุนุงุฏููุ ูุฏููุง ุงููุตุทูุญ $ \nabla_{\theta} L(\theta^{(t)}, \textbf{y}) $ุ ูุชูุณุท ุงูุฅุดุชูุงู ูุฏุงูุฉ ุงูุฎุณุงุฑู ุจูู ูู ุงูููุงุท ูู ุจูุงูุงุช ุงูุชุฏุฑูุจุ ููุนุงุฏูุชูุง:

$$ \begin{aligned}
\nabla_{\theta} L(\theta^{(t)}, \textbf{y}) &= \frac{1}{n} \sum_{i=1}^{n} \nabla_{\theta} \ell(\theta^{(t)}, y_i)
\end{aligned} $$

ููููุง $ \ell(\theta, y_i) $ ูู ุงูุฎุณุงุฑู ูู ููุทู ูุนููู ูู ุจูุงูุงุช ุงูุชุฏุฑูุจ. ูุชุทุจูู ุงููุฒูู ุงูุฅุดุชูุงูู ุงูุนุดูุงุฆูุ ุจุจุณุงุทู ูููู ุจุชุบูุฑ ูุชูุณุท ุงูุฅุดุชูุงู ุจ ุงูุฅุดุชูุงู ูู ููุทู ูุนููู. ุงููุนุงุฏูู ุงููุนุฏูู ูููุฒูู ุงูุฅุดุชูุงูู ุงูุนุดูุงุฆู ูู:

$$ {\theta}^{(t+1)} = \theta^{(t)} - \alpha \cdot \nabla_{\theta} \ell(\theta^{(t)}, y_i) $$ 

ูู ูุฐู ุงููุนุงุฏููุ $ y_i $ ูุชู ุฅุฎุชูุงุฑูุง ุจุดูู ุนุดูุงุฆู ูู $ \textbf{y} $. ูุงุญุธ ุฃู ุงุฎุชูุงุฑ ุงูููุงุท ุจุดูู ุนุดูุงุฆู ููู ุฌุฏุงู ููุฌุงุญ ุงููุฒูู ุงูุฅุดุชูุงูู ุงูุนุดูุงุฆู! ุงุฐุง ูู ูุชู ุฅุฎุชูุงุฑ ุงูููุงุท ุจุดูู ุนุดูุงุฆูุ ูุฏ ููุชุฌ ููุง ูุชุงุฆุฌ ุฃุณูุก ูู ูุชุงุฆุฌ ุงููุฒูู ุงูุฅุดุชูุงูู ุงูููุฌููุน.

ูููู ุนุงุฏุฉู ุจุฅุณุชุฎุฏุงู ุงููุฒูู ุงูุฅุดุชูุงูู ุงูุนุดูุงุฆู ุนู ุทุฑูู ุฎูุท ุงูุจูุงูุงุช ูุงุณุชุฎุฏุงู ูู ููุทู ุจุนุฏ ุงูุฎูุท ุญุชู ุชุชุฌุงูุฒ ุงุญุฏ ุงูููุงุท ุจูุงูุงุช ุงูุชุฏุฑูุจ. ุงุฐุง ูู ูุชู ุฐููุ ูุนูุฏ ุฎูุท ุงูููุงุท ูุงูููุงู ุจููุณ ุงูุฎุทูุงุช ุญุชู ุชุชุฌุงูุฒ ุงูุจูุงูุงุช. ูู ูู **ุชูุฑุงุฑ Iteration** ุงููุฒูู ุงูุฅุดุชูุงูู ุงูุนุดูุงุฆู ูุชุญูู ูู ููุทู ูุงุญุฏูุ ููู ุนูููุฉ ุชุฌุงูุฒ ุชุชู ุจูุฌุงุญ ูุทูู ุนูููุง **Epoch**.

#### ุงุณุชุฎุฏุงู ุฏุงูุฉ ุงูุฎุณุงุฑู MSE

ููุซุงูุ ููุทุจู ุงููุฒูู ุงูุฅุดุชูุงูู ุงูุนุดูุงุฆู ุนูู ุฏุงูุฉ ุงูุฎุณุงุฑุฉ ูMSE. ููุชุฐูุฑ ุชุนุฑูููุง:

$$ \begin{aligned}
L(\theta, \textbf{y})
&= \frac{1}{n} \sum_{i = 1}^{n}(y_i - \theta)^2
\end{aligned} $$

ุงุฎุฐ ุงูุฅุดุชูุงู $ \theta $ ูุตุจุญ ูุฏููุง:

$$ \begin{aligned}
\nabla_{\theta}  L(\theta, \textbf{y})
&= \frac{1}{n} \sum_{i = 1}^{n} -2(y_i - \theta)
\end{aligned} $$

ุจูุง ุงู ุงููุนุงุฏูู ุงูุณุงุจูู ุชุนุทููุง ูุชูุณุท ุฎุณุงุฑุฉ ุงูุฅุดุชูุงู ููู ุงูููุงุท ูู ุงูุจูุงูุงุชุ ูุฃู ุฎุณุงุฑุฉ ุงูุฅุดุชูุงู ูููุทู ูุนููู ูู ุจุจุณุงุทู ุฌุฒุก ุงููุนุงุฏูู ุงูุชู ุชู ุฃุฎุฐ ูุชูุณุทู:

$$ \begin{aligned}
\nabla_{\theta}  \ell(\theta, y_i)
&= -2(y_i - \theta)
\end{aligned} $$

ูุชุญุฏูุซูุง ููุฅุดุชูุงู ุงูููุฌููุน ูุฏุงูุฉ ุงูุฎุณุงุฑุฉ MSE:

$$ \begin{aligned}
{\theta}^{(t+1)} = \theta^{(t)} - \alpha \cdot \left( \frac{1}{n} \sum_{i = 1}^{n} -2(y_i - \theta) \right)
\end{aligned} $$

ูุงููุฒูู ุงูุฅุดุชูุงูู ุงูุนุดูุงุฆู ููุง ุณูููู ูุงูุชุงูู:

$$ \begin{aligned}
{\theta}^{(t+1)} = \theta^{(t)} - \alpha \cdot \left( -2(y_i - \theta) \right)
\end{aligned} $$

### ุณููู ุงููุฒูู ุงูุฅุดุชูุงูู ุงูุนุดูุงุฆู

ุจูุง ุงู ุงูุฅุดุชูุงู ุงูุนุดูุงุฆู ููุท ูุชุญูู ูู ููุทู ูุงุญุฏู ูู ูุฑูุ ูุฃู ุชุญุฏูุซู ููููุฉ $ \theta $ ุณูููู ุฃูู ุฏูู ูู ุงูุชุญุฏูุซ ุงุฐุง ุชู ูู ุงููุฒูู ุงูุนุดูุงุฆู ุงูููุฌููุน. ููููุ ุจูุง ุฃูู ุฃุณุฑุน ูู ุญุณุงุจ ุงููุชุงุฆุฌุ ูุฃู ุงููุฒูู ุงูุฅุดุชูุงูู ุงูุนุดูุงุฆู ุจุฅููุงูู ุงูุชูุฏู ุจุดูู ูุจูุฑ ูููุตูู ูููููู ุงูููุงุณุจู ู $ \theta $ ูู ุญูู ุงููุฒูู ุงูุนุดูุงุฆู ุงูููุฌููุน ูู ููุชูู ููุชูุง ูู ุงูุชุญุฏูุซ ููุง ูุฑู ูุงุญุฏู.
ุงูุตูุฑู ูู ุงูุฃุณูู ุชูุถุญ ุชุญุฏูุซุงุช ุชูุช ุจูุฌุงุญ ููููุฉ $ \theta $ ุจุฅุณุชุฎุฏุงู ุงููุฒูู ุงูุฅุดุชูุงูู ุงูููุฌููุน. ุงููุณุงุญู ุบุงููุฉ ุงูููู ูู ุงูุตุตูุฑู ุชุนูู ุงููููุฉ ุงููุซุงููู ู $ \theta $ ุนูู ุจูุงูุงุช ุงูุชุฏุฑูุจุ ููู $ \hat{\theta} $.
(ุงูุตูุฑู ุชุธูุฑ ูููุฐุฌ ูุฏูู ูุชุบูุฑุงูุ ูููู ูู ุงูููู ููุงุญุธุฉ ุทุฑููุฉ ุงููุฒูู ุงูุฅุดุชูุงูู ุงูููุฌููุน ุงูุชู ูุตู ูููุง ู $ \hat{\theta} $.)

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_stochastic_gd.png'>
</p>

ูู ุงูุฌุงูุจ ุงูุขุฎุฑุ ุงููุฒูู ุงูุฅุดุชูุงูู ุงูุนุดูุงุฆูุ ุนุงุฏุฉ ูุง ูุฃุฎุฐ ุฎุทูู ุจุนูุฏุงู ุนู $ \hat{\theta} $ุ ูููู ูููู ูููู ุจุงูุชุญุฏูุซ ุจุดูู ูุชูุฑุฑุ ุนุงุฏูุฉ ูุง ูุตู ููููุทู ุงููุซุงููู ุจุดูู ุงุณุฑุน ูู ุงูููุฌููุน.

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_stochastic_sgd.png'>
</p>

### ุชุนุฑูู ุฏุงูุฉ ูููุฒูู ุงูุฅุดุชูุงูู ุงูุนุดูุงุฆู

ููุง ูุนููุง ุณุงุจูุงูุ ูููู ุจุชุนุฑูู ุฏุงูุฉ ุชุญุณุจ ููุง ุงููุฒูู ุงูุฅุดุชูุงูู ุงูุนุดูุงุฆู ูุฏุงูุฉ ุฎุณุงุฑู. ุณุชููู ูุดุงุจูู ูุฏุงูุฉ `minimize` ุงูุชู ุณุจู ุชุนุฑูููุงุ ูููู ูุญุชุงุฌ ูุฅุถุงูุฉ ุงูุฅุฎุชูุงุฑ ุงูุนุดูุงุฆู ููููู ูู ูู ุชูุฑุงุฑ:

```python
def minimize_sgd(loss_fn, grad_loss_fn, dataset, alpha=0.2):
    """
    ุชุณุชุฎุฏู ุงููุฒูู ุงูุฅุดุชูุงูู ุงูุนุดูุงุฆู ููุชูููู ูู ุฏุงูุฉ ุงูุฎุณุงุฑู loss_fn
    ุชููู ุงููุชูุฌุฉ ุงููููู ุงูุตุบุฑู ู ฮธ ุนูุฏูุง ูููู
    ุงููุฑู ุจูู ุงูุชูุฑุงุฑุงุช ุงูู ูู 0.001
    """
    NUM_OBS = len(dataset)
    theta = 0
    np.random.shuffle(dataset)
    while True:
        for i in range(0, NUM_OBS, 1):
            rand_obs = dataset[i]
            gradient = grad_loss_fn(theta, rand_obs)
            new_theta = theta - alpha * gradient
        
            if abs(new_theta - theta) < 0.001:
                return new_theta
        
            theta = new_theta
        np.random.shuffle(dataset)
```

### ูุฒูู ุงุดุชูุงูู ุจุฏูุนุงุช ุตุบูุฑู

**ุงููุฒูู ุงูุฅุดุชูุงูู ุจุฏูุนุงุช ุตุบูุฑู Mini-batch Gradient Descent** ูุญุงูู ุงู ููุงุฒู ุจูู ุงููุฒูู ุงูุฅุดุชูุงูู ุงูุนุดูุงุฆู ู ุงูููุฌููุน ุนู ุทุฑููู ุฒูุงุฏุฉ ุนุฏุฏ ุงูุฃุฑูุงู ุงูุชู ูุชุทูุน ุนูููุง ูู ูู ุนูููุฉ ุชูุฑุงุฑ. ูู ุงููุฒูู ุงูุฅุดุชูุงูู ุจุฏูุนุงุช ุตุบูุฑูุ ูุณุชุฎุฏู ุนุฏุฏ ูู ุงูููุงุท ูู ูู ุชุญุฏูุซ ุจุฏูุงู ูู ููุทู ูุงุญุฏู.
ูุณุชุฎุฏู ูุชูุณุท ุงูุฅุดุชูุงู ูุฏูุงู ุงูุฎุณุงุฑู ููููุงู ุจุชููุน ููููุฉ ุงูุฅุดุชูุงู ุงูุตุญูุญู ูุฎุณุงุฑุฉ ุงูุงูุชุฑูุจูุง ุงูุชูุงุทุนูุฉ Cross Entropy Loss. ุงุฐุง ูุงูุช $ \mathcal{B} $ ูู ุงูุฏูุนู ุงูุตุบูุฑู ูู ุงูููุงุท ุงูุชู ูุฎุชุงุฑูุง ุจุดูู ุนุดูุงุฆู ูู $ n $ุ ูุงููุนุงุฏูู ุงูุญุงููู ูุงูุชุงูู:

$$ \nabla_\theta L(\theta, \textbf{y}) \approx \frac{1}{|\mathcal{B}|} \sum_{i\in\mathcal{B}}\nabla_{\theta}\ell(\theta, y_i) $$

ููุง ูู ุงููุฒูู ุงูุฅุดุชูุงูู ุงูุนุดูุงุฆูุ ูููู ุจุงููุฒูู ุงูุฅุดุชูุงูู ุจุฏูุนุงุช ุตุบูุฑู ุนู ุทุฑูู ุฎูุท ุจูุงูุงุช ุงูุชุฏุฑูุจ ูุงุฎุชูุงุฑ ุฏูุนุงุช ุนู ุทุฑูู ุงูุชูุฑุงุฑ ุฏุงุฎู ุงูุจูุงูุงุช ุงููุฎููุทู. ุจุนุฏ ูู Epochุ ูุนูุฏ ุฎูุท ุงูุจูุงูุงุช ูุงุฎุชูุงุฑ ุฏูุนู ุตุบูุฑู ุฌุฏูุฏู.

ุนูู ุงูุฑุบู ูู ุงููุง ูุฑููุง ุจูู ุงููุฒูู ุงูุฅุดุชูุงูู ุงูุนุดูุงุฆู ูุจุฏูุนุงุช ุตุบูุฑู ูู ูุฐู ุงููุชุงุจุ ูุณุชุฎุฏู ูุตุทูุญ ุงููุฒูู ุงูุฅุดุชูุงูู ุงูุนุดูุงุฆู ุจุดูู ุนุงู ููุฅุดุชูุงูุงุช ุจุฏูุนุงุช ุตุบูุฑู ุจุฃู ุญุฌู.

#### ุฅุฎุชูุงุฑ ุญุฌู ุงูุฏูุนุงุช ุงูุตุบูุฑู

ูููู ุงููุฒูู ุงูุฅุดุชูุงูู ุจุฏูุนุงุช ุตุบูุฑู ูุซุงููุงู ุนูุฏูุง ูุนูู ุนูู ูุญุฏุฉ ุงููุนุงูุฌู ุงูุฑุณูููู GPU (ูุฑุช ุงูุดุงุดู ููุญุงุณุจ). ููู ุงูุนูููุงุช ูู ูุฐุง ุงูููุน ุชุฃุฎุฐ ููุชุงู ุทูููุ ุงุณุชุฎุฏุงู ุงูุฏูุนุงุช ุงูุตุบูุฑู ูุฒูุฏ ูู ุฏูุฉ ุงูุฅุดุชูุงู ุฏูู ุงูุฒูุงุฏู ูู ุณุฑุนู ุนูููุฉ ุงูุญุณุงุจ. ุจูุงุกูุง ุนูู ุฐุงูุฑุฉ ูุญุฏุฉ ุงููุนุงูุฉ ุงูุฑุณูููุฉ ูู ุงูุฌูุงุฒุ ูุชู ุชุญุฏูุฏ ุญุฌู ุงูุฏูุนุงุช ุจูู 10 ู 100.

### ุชุนุฑูู ุฏุงูุฉ ูููุฒูู ุงูุฅุดุชูุงูู ุจุฏูุนุงุช ุตุบูุฑู

ุฏุงูุฉ ุงููุฒูู ุงูุฅุดุชูุงูู ุจุฏูุนุงุช ุตุบูุฑู ุชุญุชุงุฌ ูุฎูุงุฑ ูุชุญุฏูุฏ ุญุฌู ุงูุฏูุนุงุช. ุงูุฏุงูู ุงูุชุงููู ุชููุฑ ูุฐู ุงูุฎุงุตูู:

```python
def minimize_mini_batch(loss_fn, grad_loss_fn, dataset, minibatch_size, alpha=0.2):
    """
    ุชุณุชุฎุฏู ุงููุฒูู ุงูุฅุดุชูุงูู ุงูุนุดูุงุฆู ุจุฏูุนุงุช ุตุบูุฑู ููุชูููู ูู ุฏุงูุฉ ุงูุฎุณุงุฑู loss_fn
    ุชููู ุงููุชูุฌุฉ ุงููููู ุงูุตุบุฑู ู ฮธ ุนูุฏูุง ูููู
    ุงููุฑู ุจูู ุงูุชูุฑุงุฑุงุช ุงูู ูู 0.001
    """
    NUM_OBS = len(dataset)
    assert minibatch_size < NUM_OBS
    
    theta = 0
    np.random.shuffle(dataset)
    while True:
        for i in range(0, NUM_OBS, minibatch_size):
            mini_batch = dataset[i:i+minibatch_size]
            gradient = grad_loss_fn(theta, mini_batch)
            new_theta = theta - alpha * gradient
            
            if abs(new_theta - theta) < 0.001:
                return new_theta
            
            theta = new_theta
        np.random.shuffle(dataset)
```

### ููุฎุต ุงููุฒูู ุงูุฅุดุชูุงูู ุงูุนุดูุงุฆู

ูุณุชุฎุฏู ุงููุฒูู ุงูุฅุดุชูุงูู ุจุฏูุนุงุช ูุชุญุณูู ุงููููุฐุฌ ุจุดูู ูุชูุฑุฑ ุญุชู ูุตู ุฅูู ุงููููุฉ ุงูุฏููุง ููุฎุณุงุฑู. ุจูุง ุงู ุงููุฒูู ุงูุฅุดุชูุงูู ุจุฏูุนุงุช ูููู ุตุนุจุงู ููุญุณุงุจ ูู ุงูุจูุงูุงุช ุงููุจูุฑูุ ุนุงุฏุฉู ูุง ูุณุชุฎุฏู ุงููุฒูู ุงูุฅุดุชูุงูู ุงูุนุดูุงุฆู ูุถุจุท ุชูู ุงูููุงุฐุฌ. ุนูุฏ ุงุณุชุฎุฏุงู GPUุ ุงููุฒูู ุงูุฅุดุชูุงูู ุจุฏูุนุงุช ุจุณูุทู ููููู ุงูุญุณุงุจ ุจุดูู ุงุณุฑุน ูู ุงูุนุดูุงุฆู ุจููุณ ุชูููุฉ ุงูุชุดุบูู. ููุจูุงูุงุช ุงููุจูุฑูุ ุงููุฒูู ุงูุฅุดุชูุงูู ุงูุนุดูุงุฆู ูุจุฏูุนุงุช ุตุบูุฑู ูู ุงูุถู ููุฅุณุชุฎุฏุงู ุจูุง ุงููุง ุงุณุฑุน ููุญุณุงุจ.

[Inequality]: https://www.mathsisfun.com/algebra/inequality.html