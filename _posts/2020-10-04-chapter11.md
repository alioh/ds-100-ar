---
title: النزول الإشتقاقي وتحسين النتائج الكمية
show_title: true
chapter_number: 11
chapter_text: الفصل الحادي عشر
chapter_lessons: [[0, 'مقدمة'], [1, 'تقليل الخساره بإستخدام برنامج'], [2, 'النزول الإشتقاقي'], [3, 'التحدب'], [4, 'النزول الإشتقاقي العشوائي']]
chapter_sublessons: [
    [],
    ['مشاكل simple_minimize'],
    ['الفكره', 'تحليل النزول الإشتقاقي', 'تعريف دالة minimize', 'تقليل خسارة Huber', 'ملخص النزول الإشتقاقي'],
    ['اكتشاف الحدود الدنيا بالنزول الإشتقاقي', 'تعريف التحدب', 'ملخص التحدب'],
    [['استخدام دالة الخساره MSE'], 'سلوك النزول الإشتقاقي العشوائي', 'تعريف دالة للنزول الإشتقاقي العشوائي', 'نزول اشتقاقي بدفعات صغيره', 'تعريف دالة للنزول الإشتقاقي بدفعات صغيره', ملخص النزول الإشتقاقي العشوائي']
]
layout: default
---

## مقدمة

لإستخدام قاعدة بيانات للتنبؤ والتوقع، يجب علينا تكوين نموذجنا بشكل دقيق وإختيار دالة خساره. مثلاً، بيانات الإكراميات، نموذجنا توقع ان نسبة الإكراميه ثابته لا تتغير. ثم قررنا إستخدام دالة الخطأ التربيعي المتوسط MSE ووجدنا النموذج الأقل خساره.

وجدنا ايضاً ان هناك وصفاً ابسط لدالتي الخساره الخطأ التربيعي المتوسط و متوسط الخطأ الحتمي وهي: المتوسط والوسيط. ولكن، كلما كان نموذجنا ودالة الخساره اكثر تعقيداً لن نستطيع ايجاد وصفاً رياضياً مناسب. مثلاً، دالة Huber لديها خصائص مفيده ولكن صعب تمييزها.

يمكننا استخدام الكمبيوتر لحل هذه المشكل بواسطة النزول الإشتقاقي، طريقه حسابيه لتقليل دوال الخساره.

## تقليل الخساره بإستخدام برنامج

لنعود للنموذج من الفصل السابق:

$$ \theta = C $$

سنستخدم دالة الخساره MSE:

$$ \begin{split}
\begin{aligned}
L(\theta, \textbf{y})
&= \frac{1}{n} \sum_{i = 1}^{n}(y_i - \theta)^2\\
\end{aligned}
\end{split} $$

للتبسيط، سنستخدم البيانات التاليه: $ \textbf{y} = [ 12, 13, 15, 16, 17 ] $. نعلم من خلال تحليلنا للبيانات في الفصل السابق ان قيمة $ \theta $ لدالة الخساره MSE هي المتوسط $ \text{mean}(\textbf{y}) = 14.6 $. لنرى اذا كان بإمكاننا الحصول على نتيجه عند كتابتنا لبرنامج يوجدها.

اذا قمنا بكتابة برنامج بشكل متقن، فبإمكاننا استخدام نفس البرنامج على اي دالة خساره لإيجاد اقل قيمة ل $ \theta $، يشمل ذلك دالة Huber المعقده رياضياً:

$$ \begin{split}
L_\alpha(\theta, \textbf{y}) = \frac{1}{n} \sum_{i=1}^n \begin{cases}
    \frac{1}{2}(y_i - \theta)^2 &  | y_i - \theta | \le \alpha \\
    \alpha ( |y_i - \theta| - \frac{1}{2}\alpha ) & \text{otherwise}
\end{cases}
\end{split} $$

أولاً، نقوم برسم تخطيطي للبيانات. بالجانب الأيمن من الرسم نقوم برسم دالة الخساره MSE لقيم مختلفه ل $ \theta $:

```python
pts = np.array([12, 13, 15, 16, 17])
points_and_loss(pts, (11, 18), mse)
```

> في الكود البرمجي السابق استخدم الكاتب دالة عرفها مسبقاً بأسم `points_and_loss`، تقبل الدالة ثلاث متغيرات، الأولى هي البيانات. المتغير الثاني هي مقاسات ابعاد x-axis ، والقيمه الأخيره هي نوع دالة الخساره، والتي هي عباره عن داله اخرى عرفها ايضاً بأسم `mse`. تعريف كلا الدالتين هو كالتالي:
>
> ```python
> def mse(theta, y_vals):
>     return np.mean((y_vals - theta) ** 2)
> 
> def points_and_loss(y_vals, xlim, loss_fn):
>     thetas = np.arange(xlim[0], xlim[1] + 0.01, 0.05)
>     losses = [loss_fn(theta, y_vals) for theta in thetas]
>     
>     plt.figure(figsize=(9, 2))
>     
>     ax = plt.subplot(121)
>     sns.rugplot(y_vals, height=0.3, ax=ax)
>     plt.xlim(*xlim)
>     plt.title('Points')
>     plt.xlabel('Tip Percent')
>     
>     ax = plt.subplot(122)
>     plt.plot(thetas, losses)
>     plt.xlim(*xlim)
>     plt.title(loss_fn.__name__)
>     plt.xlabel(r'$ \theta $')
>     plt.ylabel('Loss')
>     plt.legend()
> ```
> 

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_basics_3_0.png'>
</p>

كيف بإمكاننا برمجة برنامج يقوم بإيجاد اقل قيمة ل $ \theta $ اوتوماتيكياً؟ الطريقه الأسهل هي بحساب قيمة الخساره لأكثر من قيمه ل $ \theta $، ثم نوجد قيمة $ \theta $ ذا الأقل خساره.

عرفنا داله بأسم `simple_minimize` والتي تقبل متغيرات هي دالة الخساره، مصفوفة البيانات، ومصفوفه بقيم $ \theta $:

```python
def simple_minimize(loss_fn, dataset, thetas):
    '''
    القيمه النهائيه لهذه الداله هي قيمة θ من بين عدة قيم من θ ذات الأقل خساره
    '''
    losses = [loss_fn(theta, dataset) for theta in thetas]
    return thetas[np.argmin(losses)]
```

ثم نعرف داله لإيجاد قيمة MSE واستخدامها في دالة `simple_minimize`:

```python
def mse(theta, dataset):
    return np.mean((dataset - theta) ** 2)

dataset = np.array([12, 13, 15, 16, 17])
thetas = np.arange(12, 18, 0.1)

simple_minimize(mse, dataset, thetas)
```

```ruby
14.599999999999991
```

النتيجه هذه قريبه للقيمه المتوقعه:

```python
# ايجاد القيمه بإستخدام المتوسط
np.mean(dataset)
```

```ruby
14.6
```

الآن يمكننا كتابة داله لحساب خسارة Huber ورسمها:

```python
def huber_loss(theta, dataset, alpha = 1):
    d = np.abs(theta - dataset)
    return np.mean(
        np.where(d < alpha,
                 (theta - dataset)**2 / 2.0,
                 alpha * (d - alpha / 2.0))
    )

points_and_loss(pts, (11, 18), huber_loss)
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_basics_12_0.png'>
</p>

على الرغم ان القيم الدنيا ل $ \theta $ يجب ان تكون أقرب إلى 15، ليس لدينا طريقه لتحليل وإيجاد قيمة $ \theta $ لدالة الخساره Huber. بدلاً، من ذلك، سنستخدم الداله `simple_minimize`:

```python
simple_minimize(huber_loss, dataset, thetas)
```

```ruby
14.999999999999989
```

الآن، لنعود لبيانات نسبة الإكراميات ونوجد افضل قيمه ل $ \theta $ بإستخدام Huber:

```python
tips = sns.load_dataset('tips')
tips['pcttip'] = tips['tip'] / tips['total_bill'] * 100
tips.head()
```

**pcttip**|**size**|**time**|**day**|**smoker**|**sex**|**tip**|**total\_bill**| 
:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:
5.944673|2|Dinner|Sun|No|Female|1.01|16.99|0
16.054159|3|Dinner|Sun|No|Male|1.66|10.34|1
16.658734|3|Dinner|Sun|No|Male|3.5|21.01|2
13.978041|2|Dinner|Sun|No|Male|3.31|23.68|3
14.680765|4|Dinner|Sun|No|Female|3.61|24.59|4

<br>
```python
points_and_loss(tips['pcttip'], (11, 20), huber_loss)
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_basics_17_0.png'>
</p>

```python
simple_minimize(huber_loss, tips['pcttip'], thetas)
```

```ruby
15.499999999999988
```

نلاحظ ان عند استخدام دالة خسارة Huber كانت النتيجه $ \hat{\theta} = 15.5 $ . يمكننا الآن مقارنة هذه النتيجه مع MSE و MAE:

```python
print(f"               MSE: theta_hat = {tips['pcttip'].mean():.2f}")
print(f"               MAE: theta_hat = {tips['pcttip'].median():.2f}")
print(f"        Huber loss: theta_hat = 15.50")
```

```ruby
            MSE: theta_hat = 16.08
            MAE: theta_hat = 15.48
    Huber loss: theta_hat = 15.50
```

نلاحظ ان دالة Huber اقرب إلى MAE كونها لا تتأثر بشكل كبير بسبب القيم الشاذه على الجانب الأيمن في الرسم البياني التالي لتوزيع بيانات الإكراميات:

```python
sns.distplot(tips['pcttip'], bins=50);
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_basics_22_0.png'>
</p>

### مشاكل simple_minimize

على الرغم من ان دالة `simple_minimize` تساعدنا على تقليل دالة الخساره، إلا ان لديها بعض المشاكل التي تجعلها غير مفيده للإستخدام بشكل عام. مشكلتها الأهم هي انها تعمل فقط مع قيم $ \theta $. مثلاً، في الكود البرمجي التالي، الذي سبق ان استخدمناه في الأعلى، احتجنا لتعريف قيم $ \theta $ يدوياً من 12 إلى 18:

```python
dataset = np.array([12, 13, 15, 16, 17])
thetas = np.arange(12, 18, 0.1)

simple_minimize(mse, dataset, thetas)
```

كيف وجدنا انه علينا التحقق من القيم بين 12 و 18؟ احتجنا لمراجعة الرسم البياني لدالة الخساره ووجدنا ان القيم الدنيا بين تلك القيمتين. هذه الطريقه غير عمليه لأننا قمنا بإضافة خطوه معقده جديده لنماذجنا. بالإضافه لذلك، قمنا بتعريف قيم الزياده 0.1 بشكل يدوي. ولكن، اذا كانت القيمه المثلى ل $ \theta $ هي 12.043، ستقوم الداله `simple_minimize` بتقريب النتيجه إلى 12.00 كونها الأقرب لمضاعفات 0.1

يمكننا حل تلك المشاكل بطريقه واحده بإستخدام ما يسمى بـ *النزول الإشتقاقي Gradient Descent*.

## النزول الإشتقاقي

نحن مُهتمون لبناء داله تستطيع التقليل من دالة الخساره بدون تقييد المستخدم لتحديد قيم مسبقه ل $ \theta $ للتجربه عليها. بمعنى أصح، بما ان دالة `simple_minimize` شكلها كالتالي:

```python
simple_minimize(loss_fn, dataset, thetas)
```

نريد دالة لديها الشكل التالي

```python
minimize(loss_fn, dataset)
```

> لاحظ في الشكل الذي نبحث عنه، لا يحتاج المستخدم لإضافة لقيم مسبقه ل $ \theta $ في المتغيرات المطلوبه لدالة تقليل الخساره `minimize`.

تحتاج هذه الداله لإيجاد قيم $ \theta $ الأقل خساره اوتوماتيكياً اياً كان حجمها. سنستخدم طريقة تسمى بالنزول الإشتقاقي لبناء الدالة الجديده المسماه `minimize`.

### الفكره

كما في دوال الخساره، سنتحدث عن فكرة النزول الإشتقاقي أولاً، ثم نتعرف ونفهم العمليه الرياضيه فيها.

بما ان الداله `minimize` لا يقدم لها قيم ل $ \theta $ للتجربه عليها، نقوم بإختيار قيمه ل $ \theta $ بأي مكانٍ. ثم، نقوم بشكل تكراري بتحسين نتائج $ \theta $. وللتحسين من النتائج، نقوم بملاحظة الميلان Slope لتلك القيمه من $ \theta $ التي اخترناها في الرسم البياني.

مثلاً، سنستخدم MSE على البيانات التاليه $ \textbf{y} = [ 12.1, 12.8, 14.9, 16.3, 17.2 ] $ وقيمة $ \theta $ التي اختراناها هي 12:

```python
pts = np.array([12.1, 12.8, 14.9, 16.3, 17.2])
plot_loss(pts, (11, 18), mse)
plot_theta_on_loss(pts, 12, mse)
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_descent_define_5_0.png'>
</p>

> استخدم الكاتب دالتين كما فعل مسبقاً لتساعده على القيام بالعمليه الحسابيه والرسم البياني وهما `plot_loss` و `plot_theta_on_loss`. والكود البرمجي في الأسفل هو تعريف كلا الدالتين:
>
> ```python
> def plot_loss(y_vals, xlim, loss_fn):
>    thetas = np.arange(xlim[0], xlim[1] + 0.01, 0.05)
>    losses = [loss_fn(theta, y_vals) for theta in thetas]
>
>    plt.figure(figsize=(5, 3))
>    plt.plot(thetas, losses, zorder=1)
>    plt.xlim(*xlim)
>    plt.title(loss_fn.__name__)
>    plt.xlabel(r'$ \theta $')
>    plt.ylabel('Loss')
>   
>   
> def plot_theta_on_loss(y_vals, theta, loss_fn, **kwargs):
>    loss = loss_fn(theta, y_vals)
>    default_args = dict(label=r'$ \theta $', zorder=2,
>                        s=200, c=sns.xkcd_rgb['green'])
>    plt.scatter([theta], [loss], **{**default_args, **kwargs})
> ```
> 

نريد اختيار قيمه جديده ل $ \theta $ لتقليل الخساره. ولعمل ذلك، كما ذكرنا سابقاً، نلاحظ الميلان لقيمة $ \theta= 12 $:

```python
pts = np.array([12.1, 12.8, 14.9, 16.3, 17.2])
plot_loss(pts, (11, 18), mse)
plot_tangent_on_loss(pts, 12, mse)
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_descent_define_7_0.png'>
</p>

قيمة الميلان سلبيه، يعني ذلك ان زيادة قيمة $ \theta $ سيقلل من الخساره.
اذا كانت $ \theta= 16.5 $، فأن قيمة الميلان ستكون موجبه:

```python
pts = np.array([12.1, 12.8, 14.9, 16.3, 17.2])
plot_loss(pts, (11, 18), mse)
plot_tangent_on_loss(pts, 16.5, mse)
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_descent_define_9_0.png'>
</p>

عندما تكون نتيجة الميلان إيجابيه، فأن تقليل قيمة $ \theta $ سيقلل الخساره.

الميلان في الخط يخبرنا بإي اتجاه نختار $ \theta $ لتقليل الخساره. اذا كان الميلان نتيجته سلبيه، فنحتاج لتحرك $ \theta $ إلى الجانب الإيجابي. وإذا كان إيجابياً، فعلينا تحريك $ \theta $ إلى الجانب السلبي. رياضاً، نقول التالي:

$$ \theta^{(t+1)} = \theta^{(t)} - \frac{\partial}{\partial \theta} L(\theta^{(t)}, \textbf{y}) $$

وفيها $ \theta^{(t)} $ هي القيمه الحاليه، و $ \theta^{(t+1)} $ هي القيمه التاليه.

بالنسبه ل MSE، فستكون كالتالي:

$$ \begin{split}
\begin{aligned}
L(\theta, \textbf{y})
&= \frac{1}{n} \sum_{i = 1}^{n}(y_i - \theta)^2\\
\frac{\partial}{\partial \hat{\theta}} L(\theta, \textbf{y})
&= \frac{1}{n} \sum_{i = 1}^{n} -2(y_i - \theta) \\
&= -\frac{2}{n} \sum_{i = 1}^{n} (y_i - \theta) \\
\end{aligned}
\end{split} $$

عندما تكون $ \theta^{(t)} = 12 $، فالنتيجه هي $ -\frac{2}{n} \sum_{i = 1}^{n} (y_i - \theta) = -5.32 $، ثم نستخدمها بالمعادله السابقه: $ \theta^{(t+1)} = 12 - (-5.32) = 17.32 $

رسمنا في الأسفل القيمه السابقه ل $ \theta $ بدائره مفرغه بحدود خضراء والقيمه الجديده لها بدائره باللون الأخضر:

```python
pts = np.array([12.1, 12.8, 14.9, 16.3, 17.2])
plot_loss(pts, (11, 18), mse)
plot_theta_on_loss(pts, 12, mse, c='none',
                   edgecolor=sns.xkcd_rgb['green'], linewidth=2)
plot_theta_on_loss(pts, 17.32, mse)
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_descent_define_11_0.png'>
</p>

على الرغم ان $ \theta $ انتقلت إلى الجانب الأيمن، لكنها لا زالت بعيده جداً عن القيمه الدنيا. يمكننا حل ذلك عن طريق ضرب الميلان بقيمه صغيره قبل طرحه من $ \theta $. والعمليه الحسابيه النهاية ستبدو كالتالي:

$$ \theta^{(t+1)} = \theta^{(t)} - \alpha \cdot \frac{\partial}{\partial \theta} L(\theta^{(t)}, \textbf{y}) $$

وفيها $ \alpha $ هي قيمه ثابته صغيره. مثلاً، اذا حددنا قيمة $ \alpha = 0.3 $، فأن القيمه الجديده ل $ \theta^{(t+1)} $ ستكون:

```python
plot_one_gd_iter(pts, 12, mse, grad_mse)
```

```ruby
old theta: 12
new theta: 13.596
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_descent_define_14_1.png'>
</p>


> عرف الكاتب دالة جديده بأسم `plot_one_gd_iter` تقوم بكتابة ورسم بياني لقيمه $ \theta $ السابقه والجديده بعد القيام بالعمليه الحسابيه المشروحه مسبقاً، استخدم الكاتب ايضاً دالة بأسم `grad_mse` وهي تعريف لدالة `mse` بإستخدام النزول الإشتقاقي، الكود البرمجي لكلا الدالتين:
>
> ```python
>   def plot_one_gd_iter(y_vals, theta, loss_fn, grad_loss, alpha=0.3):
>       new_theta = theta - alpha * grad_loss(theta, y_vals)
>       plot_loss(pts, (11, 18), loss_fn)
>       plot_theta_on_loss(pts, theta, loss_fn, c='none',
>                          edgecolor=sns.xkcd_rgb['green'], linewidth=2)
>       plot_theta_on_loss(pts, new_theta, loss_fn)
>       print(f'old theta: {theta}')
>       print(f'new theta: {new_theta}')
>
>   def grad_mse(theta, y_vals):
>       return -2 * np.mean(y_vals - theta)
> ```

في الرسم التالي، قيم $ \theta $ بعد عدة تكرارات بنفس الطريقه السابقه. لاحظ ان $ \theta $ تتغير بشكل بسيط كلما اقتربنا من القيمه الدنيا لأن الميلان ايضاً اصبحت قيمته اقل:

```python
plot_one_gd_iter(pts, 13.60, mse, grad_mse)
```

```ruby
old theta: 13.6
new theta: 14.236
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_descent_define_16_1.png'>
</p>

```python
plot_one_gd_iter(pts, 14.24, mse, grad_mse)
```

```ruby
old theta: 14.24
new theta: 14.492
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_descent_define_17_1.png'>
</p>

```python
plot_one_gd_iter(pts, 14.49, mse, grad_mse)
```

```ruby
old theta: 14.49
new theta: 14.592
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_descent_define_18_1.png'>
</p>

### تحليل النزول الإشتقاقي

لدينا الآن فكرة عن طريقة عمل خوارزمية النزول الإشتقاقي:
- اختيار قيمه اوليه ل $ \theta $ ( في العاده تكون 0 ).
- اجراء العمليه الحسابيه $ \theta - \alpha \cdot \frac{\partial}{\partial \theta} L(\theta, \textbf{y}) $ عليها وحفظ النتيجه كقيمه جديده ل $ \theta $.
- تكرار العمليه حتى تتوقف $ \theta $ عن التغير.

غالباً ستلاحظ استخدام رمز النزول (الإنحدار) $ \nabla_\theta $ بدلاً من الإشتقاق الجزئي $ \frac{\partial}{\partial \theta} $. 
كلا الرمزين متشابهان، ولكن بما انا استخدام رمز النزول اكثر بشكل عام، فسنقوم بإستخدامه في المعادله:

$$ \theta^{(t+1)} = \theta^{(t)} - \alpha \cdot \nabla_\theta L(\theta^{(t)}, \textbf{y}) $$

لمراجعة الرموز:

- $ \theta^{(t)} $ هي التوقع الحالي ل $ \theta^{*} $ في التكرار $ t $.
- $ \theta^{(t+1)} $ القيمه التاليه ل $ \theta $.
- $ \alpha $ يطلق عليها معدل التعلّم Learning Rate، وعادة ما تكون رقم صغير ثابت. في بعض المرات من المفيد ان تبدأ برقم عالي ل $ \alpha $ والتقليل منه. اذا تغيرت قيمة $ \alpha $ بين عمليات التكرار، نستخدم الرمز $ \alpha^t $ لتوضيح تغير $ \alpha $ في $ t $.
- $ \nabla_\theta L(\theta^{(t)}, \textbf{y}) $ هي اشتقاق جزئي لدالة الخساره فيها قيمه متوقعه ل $ \theta $ في التكرار $ t $.

يمكننا ملاحظه اهمية استخدام دالة خساره قابله للتفاضل: $ \nabla_\theta L(\theta, \textbf{y}) $ هي جزء مهم من خوارزمية النزول الإشتقاقي. (على الرغم ان بالإمكان توقع قيمة النزول (الإنحدار) بحساب الفرق في الخساره بين قيمتين $ \theta $ وقسمتها على المسافه بينهما، لكن ذلك يزيد من مدة إيجاد النتيجه للنزول الإشتقاقي بشكل كبير مما يجعلها غير مفيده للإستخدام).

خوارزمية النزول الإشتقاقي بسيطه ومفيده بشكل كبير وذلك لإن بإمكاننا إستخدامها في كثير من انواع النماذج والكثير من دوال الخساره. هي الطريقه الحسابيه الأهم لضبط النماذج، بما فيها الإنحدار الخطي على بيانات بحجم كبير والشبكات العصبيه.

### تعريف دالة `minimize`

الآن نعود لمهمتنا الأساسيه: تعريف دالة `minimize`. سنحتاج للتعديل قليلاً من تعريف الداله كوننا نريد إيجاد النزول الإشتقاقي لدالة الخساره:

```python
def minimize(loss_fn, grad_loss_fn, dataset, alpha=0.2, progress=True):
    '''
    تستخدم النزول الإشتقاقي للتقليل من دالة الخساره loss_fn.
    تنتج لنا الداله القيمه الصغرى ل theta_hat (θ^) عندما يكون
    التغيير اقل من 0.001 بين التكرارات.
    '''
    theta = 0
    while True:
        if progress:
            print(f'theta: {theta:.2f} | loss: {loss_fn(theta, dataset):.2f}')
        gradient = grad_loss_fn(theta, dataset)
        new_theta = theta - alpha * gradient
        
        if abs(new_theta - theta) < 0.001:
            return new_theta
        
        theta = new_theta
```

ثم يمكننا تعريف دوال تقوم بحساب MSE و نزولها (انحدارها):

```python
def mse(theta, y_vals):
    return np.mean((y_vals - theta) ** 2)

def grad_mse(theta, y_vals):
    return -2 * np.mean(y_vals - theta)
```

اخيراً، يمكننا استخدام الداله `minimize` لحساب قيمة $ \theta $ الأدنى للبيانات التاليه $ \textbf{y} = [12.1, 12.8, 14.9, 16.3, 17.2] $ 

```python
%%time
theta = minimize(mse, grad_mse, np.array([12.1, 12.8, 14.9, 16.3, 17.2]))
print(f'Minimizing theta: {theta}')
print()
```

```ruby
theta: 0.00 | loss: 218.76
theta: 5.86 | loss: 81.21
theta: 9.38 | loss: 31.70
theta: 11.49 | loss: 13.87
theta: 12.76 | loss: 7.45
theta: 13.52 | loss: 5.14
theta: 13.98 | loss: 4.31
theta: 14.25 | loss: 4.01
theta: 14.41 | loss: 3.90
theta: 14.51 | loss: 3.86
theta: 14.57 | loss: 3.85
theta: 14.61 | loss: 3.85
theta: 14.63 | loss: 3.84
theta: 14.64 | loss: 3.84
theta: 14.65 | loss: 3.84
theta: 14.65 | loss: 3.84
theta: 14.66 | loss: 3.84
theta: 14.66 | loss: 3.84
Minimizing theta: 14.658511131035242

CPU times: user 7.88 ms, sys: 3.58 ms, total: 11.5 ms
Wall time: 8.54 ms
```

نلاحظ ان النزول الإشتقاقي قام بإيجاد نفس النتيجة بشكل سريع ل:

```python
np.mean([12.1, 12.8, 14.9, 16.3, 17.2])
```

```ruby
14.66
```

### تقليل خسارة Huber

الآن، يمكننا تطبيق النزول الإشتقاقي للتقليل من دالة الخساره Huber على بيانات الإكراميات.

```python
tips = sns.load_dataset('tips')
tips['pcttip'] = tips['tip'] / tips['total_bill'] * 100
```

دالة الخساره Huber تعرف كالتالي:

$$ \begin{split}
L_\delta(\theta, \textbf{y}) = \frac{1}{n} \sum_{i=1}^n \begin{cases}
    \frac{1}{2}(y_i - \theta)^2 &  | y_i - \theta | \le \delta \\
     \delta (|y_i - \theta| - \frac{1}{2} \delta ) & \text{otherwise}
\end{cases}
\end{split} $$

والنزول الإشتقاقي لدالة Huber:

$$ \begin{split}
\nabla_{\theta} L_\delta(\theta, \textbf{y}) = \frac{1}{n} \sum_{i=1}^n \begin{cases}
    -(y_i - \theta) &  | y_i - \theta | \le \delta \\
    - \delta \cdot \text{sign} (y_i - \theta) & \text{otherwise}
\end{cases}
\end{split} $$

(لاحظ اننا في التعاريف السابقه لدالة خسارة Huber استخدمنا المتغير $ \alpha $ للإشاره لنقطة الإنتقال. ولإبعاد الشك بينها وبين $ \alpha $ المستخدمه في النزول الإشتقاقي، قمنا بتغير رمز نقطة الإنتقال في دالة الخساره Huber إلى الرمز $ \delta $.)

```python
def huber_loss(theta, dataset, delta = 1):
    d = np.abs(theta - dataset)
    return np.mean(
        np.where(d <= delta,
                 (theta - dataset)**2 / 2.0,
                 delta * (d - delta / 2.0))
    )

def grad_huber_loss(theta, dataset, delta = 1):
    d = np.abs(theta - dataset)
    return np.mean(
        np.where(d <= delta,
                 -(dataset - theta),
                 -delta * np.sign(dataset - theta))
    )
```

لنقوم بالتقليل من دالة الخساره Huber في بيانات الإكراميه:

```python
%%time
theta = minimize(huber_loss, grad_huber_loss, tips['pcttip'], progress=False)
print(f'Minimizing theta: {theta}')
print()
```

```ruby
Minimizing theta: 15.506849531471964

CPU times: user 194 ms, sys: 4.13 ms, total: 198 ms
Wall time: 208 ms
```

### ملخص النزول الإشتقاقي

يوفر لنا النزول الإشتقاقي طريقه عامله للتقليل من دالة الخساره عندما لا نستطيع إيجاد القيمه الدنيا ل $ \theta $. عندما يكون نموذجنا ودالة الخساره اكثر تعقيداً، نستخدم النزول الإشتقاقي كوسيله لضبط النماذج.

## التحدب

يساهم النزول الإشتقاقي بشكل عام بالتقليل من دالة الخساره. كما اظهرنا ذلك في دالة Huber للخساره، تكمن فائدة النزول الإشتقاقي عندما يكون صعب علينا إيجاد القيمة الدنيا.

### اكتشاف الحدود الدنيا بالنزول الإشتقاقي

للأسف، في بعض الأحيان لا يمكن للنزول الإشتقاقي إيجاد قيمة $ \theta $. لنفترض التالي $ \theta = -21 $:

```python
pts = np.array([0])
plot_loss(pts, (-23, 25), quartic_loss)
plot_theta_on_loss(pts, -21, quartic_loss)
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_convexity_5_0.png'>
</p>

```python
plot_one_gd_iter(pts, -21, quartic_loss, grad_quartic_loss)
```

```ruby
old theta: -21
new theta: -9.944999999999999
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_convexity_6_1.png'>
</p>

```python
plot_one_gd_iter(pts, -9.9, quartic_loss, grad_quartic_loss)
```

```ruby
old theta: -9.9
new theta: -12.641412
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_convexity_7_1.png'>
</p>

```python
plot_one_gd_iter(pts, -12.6, quartic_loss, grad_quartic_loss)
```

```ruby
old theta: -12.6
new theta: -14.162808
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_convexity_8_1.png'>
</p>

```python
plot_one_gd_iter(pts, -14.2, quartic_loss, grad_quartic_loss)
```

```ruby
old theta: -14.2
new theta: -14.497463999999999
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_convexity_9_1.png'>
</p>



> في الأمثله السابقه استخدم الكاتب الدوال التاليه (دالة الخساره الرباعية ونزولها الإشتقاقي) التي لم يعرفها مسبقاً:
> 
> ```python
> def quartic_loss(theta, y_vals):
>     return np.mean(1/5000 * (y_vals - theta + 12) * (y_vals - theta + 23)
>                   * (y_vals - theta - 14) * (y_vals - theta - 15) + 7)
> 
> def grad_quartic_loss(theta, y_vals):
>     return -1/2500 * (2 *(y_vals - theta)**3 + 9*(y_vals - theta)**2
>                      - 529*(y_vals - theta) - 327)
> ```
>

في المثال السابق، دالة الخساره الرباعيه، كانت نتيجة النزول الإشتقاقي قريبه إلى $ \theta = -14.5 $، ولكن القيمة الدنيا العامه لدالة الخساره هي $ \theta = 18 $، في هذا المثال نرى ان النزول الإشتقاقي يبحث عن *القيمه الدنيا المحليه Local Minimum* والتي ليست دائماً تساوي قيمة الخساره *للقيمه الدنيا العامه Global Minimum*.

لحسن الحظ، بعض دوال الخساره لديها نفس الرقم للقيمه الدنيا المحليه والعامه. لنأخذ مثلاً دالة الخطأ التربيعي المتوسط MSE:

```python
pts = np.array([-2, -1, 1])
plot_loss(pts, (-5, 5), mse)
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_convexity_11_0.png'>
</p>

تطبيق النزول الإشتقاقي على هذه الداله سيوجد لنا دائماً قيمه مثاليه عامه ل $ \theta $ كون القيمه الدنيا المحليه الوحيده هي نفسها العامه.

متوسط الخطأ الحتمي قد يحتوي على اكثر من قيمه دنيا محليه. ولكن، كل القيم الدنيا هي نفسها القيمه الدنيا العامه.

```python
pts = np.array([-1, 1])
plot_loss(pts, (-5, 5), abs_loss)
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_convexity_13_0.png'>
</p>

> تعريف دالة متوسط الخطأ الحتمي
> 
> ```python
> def abs_loss(theta, y_vals):
>     return np.mean(np.abs(y_vals - theta))
> ```
>

في هذا المثال، ستكون قيمة النزول الإشتقاقي احدى القيم المحليه الدنيا بين $ [-1, 1] $ كون ان كل القيم في هذا النطاق قيم دنيا لدالة الخساره هذه، سيقترح النزول الإشتقاقي قيمة دنيا مثاليه بين هذه النقاط ل $ \theta $.
### تعريف التحدب

في بعض الدوال، اي قيمه دنيا محليه هي نفسهاالعامه. هذه الدوال يطلق عليها **دوال محدبة Convex function** لأنها منحنيه للأعلى. كذلك دالة Huber للخساره، النموذج الثابت، MSE و MAE جميعها محدبه.

مع معدل تعلم Learning Rate مناسب، النزول الإشتقاقي يوجد $ \theta $ العامه المثاليه لدالة الخساره المحدبه. وبسبب ذلك، نفضل ضبط نماذجنا بإستخدام الدوال المحدبه إلا اذا كان لدينا سبب مناسب غير ذلك.

بشكلٍ عام، الداله $ f $ يطلق عليها داله محدبه فقط اذا كانت توفي شروط المتباينه لجميع مدخلاتها $ a $ و $ b $ ، لكل $ t \in [0, 1] $: [📝][Inequality]

$$ tf(a) + (1-t)f(b) \geq f(ta + (1-t)b) $$

المتباينه تقول ان جميع الخطوط التي تربط نقطتين في الداله يجب ان تقع على او فوق الداله. لدالة الخساره التي عرضناها في بداية هذا الجزء، يمكننا ايجاد هذه الخط:

```python
pts = np.array([0])
plot_loss(pts, (-23, 25), quartic_loss)
plot_connected_thetas(pts, -12, 12, quartic_loss)
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_convexity_17_0.png'>
</p>

> استخدم الكاتب الداله plot_connected_thetas لتساعده على رسم الخط بين النقطتين، وعرفها الكاتب كالتالي:
>
> ```python
> def plot_connected_thetas(y_vals, theta_1, theta_2, loss_fn, **kwargs):
>     plot_theta_on_loss(y_vals, theta_1, loss_fn)
>     plot_theta_on_loss(y_vals, theta_2, loss_fn)
>     loss_1 = loss_fn(theta_1, y_vals)
>     loss_2 = loss_fn(theta_2, y_vals)
>     plt.plot([theta_1, theta_2], [loss_1, loss_2])
> ```
>

وبناءًا على التعريف السابق، فهذه الداله غير محدبه.

للخطأ التربيعي المتوسط، جميع الخطوط التي نقطتين تظهر فوق الرسم البياني. يمكننا رسم احداها كالتالي:

```python
pts = np.array([0])
plot_loss(pts, (-23, 25), mse)
plot_connected_thetas(pts, -12, 12, mse)
```

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_convexity_20_0.png'>
</p>

التعريف الرياضي للتحدب يعطينا وصف دقيق لتحديد ما اذا كانت الداله محدبه أو لا. في هذا الكتاب، سنتجاهل الجزء الرياضي لإثبات التحدب ونكتفي بالقول ما اذا كانت داله محدبه أو لا.

### ملخص التحدب

للداله المحدبه، اي قيمه دنيا محليه هي نفسها عامه. ذلك يسهل للنزول الإشتقاقي إيجاد افضل المتغيرات للنماذج لأي دالة خساره. على الرغم من انه بإمكاننا إستخدام النزول الإشتقاقي لدوال الخساره غير المحدبه لإيجاد قيم دنيا، تلك القيم الدنيا المحليه غير مضمون أنها دائماً هي القيم العامه المثاليه.

## النزول الإشتقاقي العشوائي

### مقدمة 

في هذا الجزء، سنتحدث عن تعديل على النزول الإشتقاقي يجعله أكثر فائده للبيانات ذات الحجم الكبير. هذا التعديل يطلق عليه خوارزمية **النزول الإشتقاقي العشوائي Stochastic Gradient Descent**.

بعد ان تعلمنا طريقة عمل النزول الإشتقاقي وتحديثه لقيمة $ \theta $ بإستخدام الإشتقاق لدالة الخساره. بالذات إستخدمنا المعادله التاليه:

$$ {\theta}^{(t+1)} = \theta^{(t)} - \alpha \cdot \nabla_{\theta} L(\theta^{(t)}, \textbf{y}) $$

في هذه المعادله: 

- $ \theta^{(t)} $ هي التوقع الحالي ل $ \theta^{*} $ في التكرار $ t $.
- $ \alpha $ هي معدل التعلّم Learning Rate.
- $ \nabla_\theta L(\theta^{(t)}, \textbf{y}) $ هي اشتقاق دالة الخساره.
- ونحسب التوقع التالي $ \theta^{(t+1)} $ عن طريق طرح $ \alpha $ و $ \nabla_\theta L(\theta, \textbf{y}) $ والمحسوبه في $ \theta^{(t)} $.

##### حدود النزول الإشتقاقي

في المعادله السابقه، قمنا بحساب $ \nabla_\theta L(\theta, \textbf{y}) $ بإستخدام متوسط الإشتقاق لدالة الخساره $ \ell(\theta, y_i) $ لجميع البيانات. بمعنى آخر، في كل مره نحدث قيمة $ \theta $ نتحقق من جميع النقاط الأخرى في بياناتنا. لهذا السبب، القانون للإشتقاق في المعادله السابقه يطلق عليه **النزول الإشتقاقي المُجَمع Batch Gradient Descent**.

ولأننا لسوء الحظ عادة ما نعمل مع بيانات كبيرة الحجم، فأن النزول الإشتقاقي المُجَمع سيعمل لإيجاد القيمه المناسبه ل $ \theta $ بعد بضع تكرارات، ولكن كل تكرار قد ياخذ وقتاً طويل لحساب النتيجه فيه إذا كانت النقاط في بياناتنا كثيره.

#### النزول الإشتقاقي العشوائي

لحل مشكلة الوقت في حساب الإشتقاق لجميع بيانات التدريب، يقوم النزول الإشتقاقي العشوائي بتوقع القيمه بإستخدام **قيمه عشوائيه واحده من البيانات**. ولأن القيمه تم إختيارها بشكل عشوائي، نتوقع ان الإشتقاق لكل نقطه سيأدي بالنهايه إلى نفس النتيجه للنزول الإشتقاقي المُجَمع.

لنعود مره أخرى لمععادة النزول الإشتقاقي المُجَمع:

$$ {\theta}^{(t+1)} = \theta^{(t)} - \alpha \cdot \nabla_{\theta} L(\theta^{(t)}, \textbf{y}) $$

في هذه المعادله، لدينا المصطلح $ \nabla_{\theta} L(\theta^{(t)}, \textbf{y}) $، متوسط الإشتقاق لدالة الخساره بين كل النقاط في بيانات التدريب، ومعادلتها:

$$ \begin{aligned}
\nabla_{\theta} L(\theta^{(t)}, \textbf{y}) &= \frac{1}{n} \sum_{i=1}^{n} \nabla_{\theta} \ell(\theta^{(t)}, y_i)
\end{aligned} $$

وفيها $ \ell(\theta, y_i) $ هي الخساره في نقطه معينه في بيانات التدريب. لتطبيق النزول الإشتقاقي العشوائي، ببساطه نقوم بتغير متوسط الإشتقاق ب الإشتقاق في نقطه معينه. المعادله المعدله للنزول الإشتقاقي العشوائي هي:

$$ {\theta}^{(t+1)} = \theta^{(t)} - \alpha \cdot \nabla_{\theta} \ell(\theta^{(t)}, y_i) $$ 

في هذه المعادله، $ y_i $ يتم إختيارها بشكل عشوائي من $ \textbf{y} $. لاحظ أن اختيار النقاط بشكل عشوائي مهم جداً لنجاح النزول الإشتقاقي العشوائي! اذا لم يتم إختيار النقاط بشكل عشوائي، قد ينتج لنا نتائج أسوء من نتائج النزول الإشتقاقي المُجَمع.

نقوم عادةً بإستخدام النزول الإشتقاقي العشوائي عن طريق خلط البيانات واستخدام كل نقطه بعد الخلط حتى تتجاوز احد النقاط بيانات التدريب. اذا لم يتم ذلك، نعيد خلط النقاط والقيام بنفس الخطوات حتى تتجاوز البيانات. في كل **تكرار Iteration** النزول الإشتقاقي العشوائي يتحقق من نقطه واحده؛ وكل عملية تجاوز تتم بنجاح يطلق عليها **Epoch**.

#### استخدام دالة الخساره MSE

كمثال، لنطبق النزول الإشتقاقي العشوائي على دالة الخسارة لMSE. لنتذكر تعريفها:

$$ \begin{aligned}
L(\theta, \textbf{y})
&= \frac{1}{n} \sum_{i = 1}^{n}(y_i - \theta)^2
\end{aligned} $$

اخذ الإشتقاق $ \theta $ يصبح لدينا:

$$ \begin{aligned}
\nabla_{\theta}  L(\theta, \textbf{y})
&= \frac{1}{n} \sum_{i = 1}^{n} -2(y_i - \theta)
\end{aligned} $$

بما ان المعادله السابقه تعطينا متوسط خسارة الإشتقاق لكل النقاط في البيانات، فأن خسارة الإشتقاق لنقطه معينه هي ببساطه جزء المعادله التي تم أخذ متوسطه:

$$ \begin{aligned}
\nabla_{\theta}  \ell(\theta, y_i)
&= -2(y_i - \theta)
\end{aligned} $$

لتحديثها للإشتقاق المُجَمع لدالة الخسارة MSE:

$$ \begin{aligned}
{\theta}^{(t+1)} = \theta^{(t)} - \alpha \cdot \left( \frac{1}{n} \sum_{i = 1}^{n} -2(y_i - \theta) \right)
\end{aligned} $$

والنزول الإشتقاقي العشوائي لها سيكون كالتالي:

$$ \begin{aligned}
{\theta}^{(t+1)} = \theta^{(t)} - \alpha \cdot \left( -2(y_i - \theta) \right)
\end{aligned} $$

### سلوك النزول الإشتقاقي العشوائي

بما ان الإشتقاق العشوائي فقط يتحقق من نقطه واحده كل مره، فأن تحديثه لقيمة $ \theta $ سيكون أقل دقه من التحديث اذا تم من النزول العشوائي المُجَمع. ولكن، بما أنه أسرع في حساب النتائج، فأن النزول الإشتقاقي العشوائي بإمكانه التقدم بشكل كبير للوصول للقيمه المناسبه ل $ \theta $ في حين النزول العشوائي المُجَمع لم ينتهي وقتها من التحديث ولا مره واحده.
الصوره في الأسفل توضح تحديثات تمت بنجاح لقيمة $ \theta $ بإستخدام النزول الإشتقاقي المُجَمع. المساحه غامقة اللون في الصصوره تعني القيمة المثاليه ل $ \theta $ على بيانات التدريب، وهي $ \hat{\theta} $.
(الصوره تظهر نموذج لديه متغيران، ولكن من المهم ملاحظة طريقة النزول الإشتقاقي المُجَمع التي يصل فيها ل $ \hat{\theta} $.)

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_stochastic_gd.png'>
</p>

في الجانب الآخر، النزول الإشتقاقي العشوائي، عادة ما يأخذ خطوه بعيداً عن $ \hat{\theta} $، ولكن كونه يقوم بالتحديث بشكل متكرر، عادلة ما يصل للنقطه المثاليه بشكل اسرع من المُجَمع.

<p align='center'>
<img src='{{ site.baseurl }}/img/chapter11/gradient_stochastic_sgd.png'>
</p>

### تعريف دالة للنزول الإشتقاقي العشوائي

كما فعلنا سابقاً، نقوم بتعريف دالة تحسب لنا النزول الإشتقاقي العشوائي لدالة خساره. ستكون مشابهه لدالة `minimize` التي سبق تعريفها، ولكن نحتاج لإضافة الإختيار العشوائي للقيم في كل تكرار:

```python
def minimize_sgd(loss_fn, grad_loss_fn, dataset, alpha=0.2):
    """
    تستخدم النزول الإشتقاقي العشوائي للتقليل من دالة الخساره loss_fn
    تكون النتيجة القيمه الصغرى ل θ عندما يكون
    الفرق بين التكرارات اقل من 0.001
    """
    NUM_OBS = len(dataset)
    theta = 0
    np.random.shuffle(dataset)
    while True:
        for i in range(0, NUM_OBS, 1):
            rand_obs = dataset[i]
            gradient = grad_loss_fn(theta, rand_obs)
            new_theta = theta - alpha * gradient
        
            if abs(new_theta - theta) < 0.001:
                return new_theta
        
            theta = new_theta
        np.random.shuffle(dataset)
```

### نزول اشتقاقي بدفعات صغيره

**النزول الإشتقاقي بدفعات صغيره Mini-batch Gradient Descent** يحاول ان يوازن بين النزول الإشتقاقي العشوائي و المُجَمع عن طريقه زيادة عدد الأرقام التي يتطلع عليها في كل عملية تكرار. في النزول الإشتقاقي بدفعات صغيره، نستخدم عدد من النقاط في كل تحديث بدلاً من نقطه واحده.
نستخدم متوسط الإشتقاق لدوال الخساره للقيام بتوقع لقيمة الإشتقاق الصحيحه لخسارة الانتروبيا التقاطعية Cross Entropy Loss. اذا كانت $ \mathcal{B} $ هي الدفعه الصغيره من النقاط التي نختارها بشكل عشوائي من $ n $، فالمعادله الحاليه كالتالي:

$$ \nabla_\theta L(\theta, \textbf{y}) \approx \frac{1}{|\mathcal{B}|} \sum_{i\in\mathcal{B}}\nabla_{\theta}\ell(\theta, y_i) $$

كما في النزول الإشتقاقي العشوائي، نقوم بالنزول الإشتقاقي بدفعات صغيره عن طريق خلط بيانات التدريب واختيار دفعات عن طريق التكرار داخل البيانات المخلوطه. بعد كل Epoch، نعيد خلط البيانات واختيار دفعه صغيره جديده.

على الرغم من اننا فرقنا بين النزول الإشتقاقي العشوائي وبدفعات صغيره في هذه الكتاب، يستخدم مصطلح النزول الإشتقاقي العشوائي بشكل عام للإشتقاقات بدفعات صغيره بأي حجم.

#### إختيار حجم الدفعات الصغيره

يكون النزول الإشتقاقي بدفعات صغيره مثالياً عندما يعمل على وحدة المعالجه الرسوميه GPU (كرت الشاشه للحاسب). كون العمليات من هذا النوع تأخذ وقتاً طويل، استخدام الدفعات الصغيره يزيد من دقة الإشتقاق دون الزياده من سرعه عملية الحساب. بناءًا على ذاكرة وحدة المعالة الرسومية في الجهاز، يتم تحديد حجم الدفعات بين 10 و 100.

### تعريف دالة للنزول الإشتقاقي بدفعات صغيره

دالة النزول الإشتقاقي بدفعات صغيره تحتاج لخيار لتحديد حجم الدفعات. الداله التاليه توفر هذه الخاصيه:

```python
def minimize_mini_batch(loss_fn, grad_loss_fn, dataset, minibatch_size, alpha=0.2):
    """
    تستخدم النزول الإشتقاقي العشوائي بدفعات صغيره للتقليل من دالة الخساره loss_fn
    تكون النتيجة القيمه الصغرى ل θ عندما يكون
    الفرق بين التكرارات اقل من 0.001
    """
    NUM_OBS = len(dataset)
    assert minibatch_size < NUM_OBS
    
    theta = 0
    np.random.shuffle(dataset)
    while True:
        for i in range(0, NUM_OBS, minibatch_size):
            mini_batch = dataset[i:i+minibatch_size]
            gradient = grad_loss_fn(theta, mini_batch)
            new_theta = theta - alpha * gradient
            
            if abs(new_theta - theta) < 0.001:
                return new_theta
            
            theta = new_theta
        np.random.shuffle(dataset)
```

### ملخص النزول الإشتقاقي العشوائي

نستخدم النزول الإشتقاقي بدفعات لتحسين النموذج بشكل متكرر حتى نصل إلى القيمة الدنيا للخساره. بما ان النزول الإشتقاقي بدفعات يكون صعباً للحساب في البيانات الكبيره، عادةً ما نستخدم النزول الإشتقاقي العشوائي لضبط تلك النماذج. عند استخدام GPU، النزول الإشتقاقي بدفعات بسيطه يمكنه الحساب بشكل اسرع من العشوائي بنفس تكلفة التشغيل. للبيانات الكبيره، النزول الإشتقاقي العشوائي وبدفعات صغيره هي افضل للإستخدام بما انها اسرع للحساب.

[Inequality]: https://www.mathsisfun.com/algebra/inequality.html